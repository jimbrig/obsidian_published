{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Obsidian Notes \u00b6 Publish your public notes with MkDocs Hello World! \u00b6 The index.md in the /docs folder is the homepage you see here.","title":"Home"},{"location":"#obsidian-notes","text":"Publish your public notes with MkDocs","title":"Obsidian Notes"},{"location":"#hello-world","text":"The index.md in the /docs folder is the homepage you see here.","title":"Hello World!"},{"location":"Cloud/","text":"","title":"Index"},{"location":"Cloud/Google%20Cloud%20Setup%20Notes/","text":"Google Cloud Setup Notes \u00b6 Environment Setup and Configuration \u00b6 In the Google Cloud Console , select or create a GCP project. Project Selector Page Ensure Billing is Enabled for the project. Enable the Cloud Run API . Install and Initialize gcloud SDK on local machine. On windows run, cinst gcloud if using Chocolatey. Update components via gcloud components update Authenticate GCP (two methods): Using a dedicated service account Links: Source:","title":"Google Cloud Setup Notes"},{"location":"Cloud/Google%20Cloud%20Setup%20Notes/#google-cloud-setup-notes","text":"","title":"Google Cloud Setup Notes"},{"location":"Cloud/Google%20Cloud%20Setup%20Notes/#environment-setup-and-configuration","text":"In the Google Cloud Console , select or create a GCP project. Project Selector Page Ensure Billing is Enabled for the project. Enable the Cloud Run API . Install and Initialize gcloud SDK on local machine. On windows run, cinst gcloud if using Chocolatey. Update components via gcloud components update Authenticate GCP (two methods): Using a dedicated service account Links: Source:","title":"Environment Setup and Configuration"},{"location":"Databases/Databases/","text":"Databases - MOC \u00b6 Database Engines \u00b6 PostgreSQL Database Design \u00b6 Views vs. Materialized Views Tools \u00b6 PostgreSQL Tools Backlinks: Sources:","title":"Databases - MOC"},{"location":"Databases/Databases/#databases-moc","text":"","title":"Databases - MOC"},{"location":"Databases/Databases/#database-engines","text":"PostgreSQL","title":"Database Engines"},{"location":"Databases/Databases/#database-design","text":"Views vs. Materialized Views","title":"Database Design"},{"location":"Databases/Databases/#tools","text":"PostgreSQL Tools Backlinks: Sources:","title":"Tools"},{"location":"Databases/PostgreSQL%20Tools/","text":"PostgreSQL Tools \u00b6 Core \u00b6 PostgreSQL Engine \u00b6 PostgreSQL: The world's most advanced open source database PostgreSQL: About PostgreSQL: Documentation: 13: PostgreSQL 13.2 Documentation PostgreSQL: Downloads PostgreSQL: Software Catalogue - Administration/development tools CLI \u00b6 psql pgcli pg_dump pg_restore GUI \u00b6 pgAdmin4 Dbeaver Beekeeper Studio Valentina Studio DBTarzan Developer Tools \u00b6 pgHero pgSync postgres-ai/database-lab DBML, dbdocs.io, dbdiagram.io PostgREST postGIS WAL-G: wal-g/wal-g: Archival and Restoration for Postgres (github.com) Docker \u00b6 postgres (docker.com) postgres/Dockerfile postgresai/extended-postgres (docker.com) docker pull postgresai/extended-postgres postgresai/sync-instance (docker.com) docker pull postgresai / sync-instances docker run \\ - -name sync_instance \\ - -env PGDATA =/ var / lib / postgresql / pgdata \\ - -env WALG_GS_PREFIX = \"gs://{BUCKET}/{SCOPE}\" \\ - -env GOOGLE_APPLICATION_CREDENTIALS = \"/etc/sa/credentials.json\" \\ - -volume { PATH_TO_CREDENTIALS } : / etc / sa / credentials . json \\ - -volume / var / lib / dblab / data : / var / lib / postgresql / pgdata : rshared \\ - -detach \\ postgresai / sync-instance : 13 postgrest/postgrest (docker.com) postgrestoauth/api (docker.com) Extensions \u00b6 - Table of Contents \u2014 pgRouting Manual (3.1) \u00b6 Links: Source:","title":"PostgreSQL Tools"},{"location":"Databases/PostgreSQL%20Tools/#postgresql-tools","text":"","title":"PostgreSQL Tools"},{"location":"Databases/PostgreSQL%20Tools/#core","text":"","title":"Core"},{"location":"Databases/PostgreSQL%20Tools/#postgresql-engine","text":"PostgreSQL: The world's most advanced open source database PostgreSQL: About PostgreSQL: Documentation: 13: PostgreSQL 13.2 Documentation PostgreSQL: Downloads PostgreSQL: Software Catalogue - Administration/development tools","title":"PostgreSQL Engine"},{"location":"Databases/PostgreSQL%20Tools/#cli","text":"psql pgcli pg_dump pg_restore","title":"CLI"},{"location":"Databases/PostgreSQL%20Tools/#gui","text":"pgAdmin4 Dbeaver Beekeeper Studio Valentina Studio DBTarzan","title":"GUI"},{"location":"Databases/PostgreSQL%20Tools/#developer-tools","text":"pgHero pgSync postgres-ai/database-lab DBML, dbdocs.io, dbdiagram.io PostgREST postGIS WAL-G: wal-g/wal-g: Archival and Restoration for Postgres (github.com)","title":"Developer Tools"},{"location":"Databases/PostgreSQL%20Tools/#docker","text":"postgres (docker.com) postgres/Dockerfile postgresai/extended-postgres (docker.com) docker pull postgresai/extended-postgres postgresai/sync-instance (docker.com) docker pull postgresai / sync-instances docker run \\ - -name sync_instance \\ - -env PGDATA =/ var / lib / postgresql / pgdata \\ - -env WALG_GS_PREFIX = \"gs://{BUCKET}/{SCOPE}\" \\ - -env GOOGLE_APPLICATION_CREDENTIALS = \"/etc/sa/credentials.json\" \\ - -volume { PATH_TO_CREDENTIALS } : / etc / sa / credentials . json \\ - -volume / var / lib / dblab / data : / var / lib / postgresql / pgdata : rshared \\ - -detach \\ postgresai / sync-instance : 13 postgrest/postgrest (docker.com) postgrestoauth/api (docker.com)","title":"Docker"},{"location":"Databases/PostgreSQL%20Tools/#extensions","text":"","title":"Extensions"},{"location":"Databases/PostgreSQL%20Tools/#-table-of-contents-pgrouting-manual-31","text":"Links: Source:","title":"- Table of Contents \u2014 pgRouting Manual (3.1)"},{"location":"Databases/PostgreSQL/","text":"PostgreSQL \u00b6 Links: Source:","title":"PostgreSQL"},{"location":"Databases/PostgreSQL/#postgresql","text":"Links: Source:","title":"PostgreSQL"},{"location":"Databases/Views%20vs.%20Materialized%20Views/","text":"Views vs. Materialized Views \u00b6 What are the major differences between a view and materialized view, and why should you use one over the other? Views \u00b6 Normal views have their own advantages and disadvantages in comparison to materialized views . A view is created with the Create View SQL command and contains all data obtained from the supplied view query expression. A primary advantage of views is that you can query them in the same manner as you would any normal database schema's table and retrieve the latest updated calculated results. The key here is that views always compute every time they are queried or accessed in any way; which may be seen as both a pro and con. Another side-note is that in the circumstance you make any type of update to the content in a View, it will always be \u201cpushed back\u201d and updated in the original table. Likewise, the reverse is also true: any changes that are made to the original base table are instantly reflected in the View. What this means, however, is that the performance of a View will always be slower than that of a materialized view. The major advantage is that a View doesn\u2019t actually require storage space. You can also have total control over which users can or cannot view sensitive information within the database itself. A materialized view, on the other hand, is a physical copy of those original base tables. Think of it more like a photograph of the original base table. The key difference is that a materialized view will not be updated every time it is interacted with. Links: Databases | PostgreSQL | System Design | [[Web Development]] Source: What are Materialized Views? A 5 Minute Introduction (educative.io)","title":"Views vs. Materialized Views"},{"location":"Databases/Views%20vs.%20Materialized%20Views/#views-vs-materialized-views","text":"What are the major differences between a view and materialized view, and why should you use one over the other?","title":"Views vs. Materialized Views"},{"location":"Databases/Views%20vs.%20Materialized%20Views/#views","text":"Normal views have their own advantages and disadvantages in comparison to materialized views . A view is created with the Create View SQL command and contains all data obtained from the supplied view query expression. A primary advantage of views is that you can query them in the same manner as you would any normal database schema's table and retrieve the latest updated calculated results. The key here is that views always compute every time they are queried or accessed in any way; which may be seen as both a pro and con. Another side-note is that in the circumstance you make any type of update to the content in a View, it will always be \u201cpushed back\u201d and updated in the original table. Likewise, the reverse is also true: any changes that are made to the original base table are instantly reflected in the View. What this means, however, is that the performance of a View will always be slower than that of a materialized view. The major advantage is that a View doesn\u2019t actually require storage space. You can also have total control over which users can or cannot view sensitive information within the database itself. A materialized view, on the other hand, is a physical copy of those original base tables. Think of it more like a photograph of the original base table. The key difference is that a materialized view will not be updated every time it is interacted with. Links: Databases | PostgreSQL | System Design | [[Web Development]] Source: What are Materialized Views? A 5 Minute Introduction (educative.io)","title":"Views"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/","text":"Firefox Developer Edition \u00b6 Really, just use Firefox for fuck sake. Features \u00b6 The key killer feature of Firefox is containers . Settings \u00b6 General: Enable Ctrl + Tab to cycle through tabs in recently used order: - Themes \u00b6 Currently using the Matte Black (Red) Theme Extensions \u00b6 Keeper Password Manager & Digital Vault Momentum OneTab Raindrop.io Drak Reader Non-Essential Extensions: Evernote Web Clipper Instapaper TamperMonkey Developer Tools \u00b6 See 30 Tips Tricks with the Firefox Developer Tools Medium article You can save a snapshot of the network requests in your Network Monitor. It saves them as HAR or HTTP Archive format. You can also import HAR files and have them display in the Network Monitor so you can debug them. Links: Sources: - 30 Tips Tricks with the Firefox Developer Tools - Mozilla Github Organization Account Home Page - Calling all web developers: here\u2019s why you should be using Firefox - Firefox is the best browser for web-developers","title":"Firefox Developer Edition"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/#firefox-developer-edition","text":"Really, just use Firefox for fuck sake.","title":"Firefox Developer Edition"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/#features","text":"The key killer feature of Firefox is containers .","title":"Features"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/#settings","text":"General: Enable Ctrl + Tab to cycle through tabs in recently used order: -","title":"Settings"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/#themes","text":"Currently using the Matte Black (Red) Theme","title":"Themes"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/#extensions","text":"Keeper Password Manager & Digital Vault Momentum OneTab Raindrop.io Drak Reader Non-Essential Extensions: Evernote Web Clipper Instapaper TamperMonkey","title":"Extensions"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/#developer-tools","text":"See 30 Tips Tricks with the Firefox Developer Tools Medium article You can save a snapshot of the network requests in your Network Monitor. It saves them as HAR or HTTP Archive format. You can also import HAR files and have them display in the Network Monitor so you can debug them. Links: Sources: - 30 Tips Tricks with the Firefox Developer Tools - Mozilla Github Organization Account Home Page - Calling all web developers: here\u2019s why you should be using Firefox - Firefox is the best browser for web-developers","title":"Developer Tools"},{"location":"Developer%20Tools/Web%20Browsers/Web%20Browsers/","text":"Web Browsers \u00b6 Currently my primary web browsers are: - [Microsoft Edge Canary] - [Mozilla Firefox Developer Edition] Links: Source:","title":"Web Browsers"},{"location":"Developer%20Tools/Web%20Browsers/Web%20Browsers/#web-browsers","text":"Currently my primary web browsers are: - [Microsoft Edge Canary] - [Mozilla Firefox Developer Edition] Links: Source:","title":"Web Browsers"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/","text":"Docker Compose Reference Docs \u00b6 Overview of docker-compose CLI \u00b6 Estimated reading time: 5 minutes This page provides the usage information for the docker-compose Command. Command options overview and help \u00b6 You can also see this information by running docker-compose --help from the command line. Define and run multi - container applications with Docker . Usage : docker - compose [ -f <arg>... ] [ --profile <name>... ] [ options ] [ COMMAND ] [ ARGS... ] docker - compose - h |-- help Options : - f , -- file FILE Specify an alternate compose file ( default : docker - compose . yml ) - p , -- project - name NAME Specify an alternate project name ( default : directory name ) -- profile NAME Specify a profile to enable -- verbose Show more output -- log - level LEVEL Set log level ( DEBUG , INFO , WARNING , ERROR , CRITICAL ) -- no - ansi Do not print ANSI control characters - v , -- version Print version and exit - H , -- host HOST Daemon socket to connect to -- tls Use TLS ; implied by -- tlsverify -- tlscacert CA_PATH Trust certs signed only by this CA -- tlscert CLIENT_CERT_PATH Path to TLS certificate file -- tlskey TLS_KEY_PATH Path to TLS key file -- tlsverify Use TLS and verify the remote -- skip - hostname - check Don 't check the daemon' s hostname against the name specified in the client certificate -- project - directory PATH Specify an alternate working directory ( default : the path of the Compose file ) -- compatibility If set , Compose will attempt to convert deploy keys in v3 files to their non - Swarm equivalent Commands : build Build or rebuild services bundle Generate a Docker bundle from the Compose file config Validate and view the Compose file create Create services down Stop and remove containers , networks , images , and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one - off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker - Compose version information You can use Docker Compose binary, docker-compose [-f <arg>...] [options] [COMMAND] [ARGS...] , to build and manage multiple services in Docker containers. Use -f to specify name and path of one or more Compose files \u00b6 Use the -f flag to specify the location of a Compose configuration file. Specifying multiple Compose files \u00b6 You can supply multiple -f configuration files. When you supply multiple files, Compose combines them into a single configuration. Compose builds the configuration in the order you supply the files. Subsequent files override and add to their predecessors. For example, consider this command line: $ docker-compose -f docker-compose.yml -f docker-compose.admin.yml run backup_db The docker-compose.yml file might specify a webapp service. webapp : image : examples / web ports : - \"8000:8000\" volumes : - \"/data\" If the docker-compose.admin.yml also specifies this same service, any matching fields override the previous file. New values, add to the webapp service configuration. webapp : build : . environment : - DEBUG = 1 When you use multiple Compose files, all paths in the files are relative to the first configuration file specified with -f . You can use the --project-directory option to override this base path. Use a -f with - (dash) as the filename to read the configuration from stdin . When stdin is used all paths in the configuration are relative to the current working directory. The -f flag is optional. If you don\u2019t provide this flag on the command line, Compose traverses the working directory and its parent directories looking for a docker-compose.yml and a docker-compose.override.yml file. You must supply at least the docker-compose.yml file. If both files are present on the same directory level, Compose combines the two files into a single configuration. The configuration in the docker-compose.override.yml file is applied over and in addition to the values in the docker-compose.yml file. Specifying a path to a single Compose file \u00b6 You can use the -f flag to specify a path to a Compose file that is not located in the current directory, either from the command line or by setting up a COMPOSE_FILE environment variable in your shell or in an environment file. For an example of using the -f option at the command line, suppose you are running the Compose Rails sample , and have a docker-compose.yml file in a directory called sandbox/rails . You can use a command like docker-compose pull to get the postgres image for the db service from anywhere by using the -f flag as follows: docker-compose -f ~/sandbox/rails/docker-compose.yml pull db Here\u2019s the full example: $ docker - compose - f ~/ sandbox / rails / docker - compose . yml pull db Pulling db ( postgres : latest ) ... latest : Pulling from library / postgres ef0380f84d05 : Pull complete 50 cf91dc1db8 : Pull complete d3add4cd115c : Pull complete 467830 d8a616 : Pull complete 089 b9db7dc57 : Pull complete 6 fba0a36935c : Pull complete 81 ef0e73c953 : Pull complete 338 a6c4894dc : Pull complete 15853 f32f67c : Pull complete 044 c83d92898 : Pull complete 17301519 f133 : Pull complete dcca70822752 : Pull complete cecf11b8ccf3 : Pull complete Digest : sha256 : 1364924 c753d5ff7e2260cd34dc4ba05ebd40ee8193391220be0f9901d4e1651 Status : Downloaded newer image for postgres : latest Use -p to specify a project name \u00b6 Each configuration has a project name. If you supply a -p flag, you can specify a project name. If you don\u2019t specify the flag, Compose uses the current directory name. See also the COMPOSE_PROJECT_NAME environment variable . Use --profile to specify one or more active profiles \u00b6 Calling docker-compose --profile frontend up will start the services with the profile frontend and services without specified profiles. You can also enable multiple profiles, e.g. with docker-compose --profile frontend --profile debug up the profiles frontend and debug will be enabled. See also Using profiles with Compose and the COMPOSE_PROFILES environment variable . Set up environment variables \u00b6 You can set environment variables for various docker-compose options, including the -f and -p flags. For example, the COMPOSE_FILE environment variable relates to the -f flag, and COMPOSE_PROJECT_NAME environment variable relates to the -p flag. Also, you can set some of these variables in an environment file . Where to go next \u00b6 CLI environment variables Declare default environment variables in file fig , composition , compose , docker , orchestration , cli , reference , docker-compose Links: Source:","title":"Docker Compose Reference Docs"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#docker-compose-reference-docs","text":"","title":"Docker Compose Reference Docs"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#overview-of-docker-compose-cli","text":"Estimated reading time: 5 minutes This page provides the usage information for the docker-compose Command.","title":"Overview of docker-compose CLI"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#command-options-overview-and-help","text":"You can also see this information by running docker-compose --help from the command line. Define and run multi - container applications with Docker . Usage : docker - compose [ -f <arg>... ] [ --profile <name>... ] [ options ] [ COMMAND ] [ ARGS... ] docker - compose - h |-- help Options : - f , -- file FILE Specify an alternate compose file ( default : docker - compose . yml ) - p , -- project - name NAME Specify an alternate project name ( default : directory name ) -- profile NAME Specify a profile to enable -- verbose Show more output -- log - level LEVEL Set log level ( DEBUG , INFO , WARNING , ERROR , CRITICAL ) -- no - ansi Do not print ANSI control characters - v , -- version Print version and exit - H , -- host HOST Daemon socket to connect to -- tls Use TLS ; implied by -- tlsverify -- tlscacert CA_PATH Trust certs signed only by this CA -- tlscert CLIENT_CERT_PATH Path to TLS certificate file -- tlskey TLS_KEY_PATH Path to TLS key file -- tlsverify Use TLS and verify the remote -- skip - hostname - check Don 't check the daemon' s hostname against the name specified in the client certificate -- project - directory PATH Specify an alternate working directory ( default : the path of the Compose file ) -- compatibility If set , Compose will attempt to convert deploy keys in v3 files to their non - Swarm equivalent Commands : build Build or rebuild services bundle Generate a Docker bundle from the Compose file config Validate and view the Compose file create Create services down Stop and remove containers , networks , images , and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one - off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker - Compose version information You can use Docker Compose binary, docker-compose [-f <arg>...] [options] [COMMAND] [ARGS...] , to build and manage multiple services in Docker containers.","title":"Command options overview and help"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#use-f-to-specify-name-and-path-of-one-or-more-compose-files","text":"Use the -f flag to specify the location of a Compose configuration file.","title":"Use -f to specify name and path of one or more Compose files"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#specifying-multiple-compose-files","text":"You can supply multiple -f configuration files. When you supply multiple files, Compose combines them into a single configuration. Compose builds the configuration in the order you supply the files. Subsequent files override and add to their predecessors. For example, consider this command line: $ docker-compose -f docker-compose.yml -f docker-compose.admin.yml run backup_db The docker-compose.yml file might specify a webapp service. webapp : image : examples / web ports : - \"8000:8000\" volumes : - \"/data\" If the docker-compose.admin.yml also specifies this same service, any matching fields override the previous file. New values, add to the webapp service configuration. webapp : build : . environment : - DEBUG = 1 When you use multiple Compose files, all paths in the files are relative to the first configuration file specified with -f . You can use the --project-directory option to override this base path. Use a -f with - (dash) as the filename to read the configuration from stdin . When stdin is used all paths in the configuration are relative to the current working directory. The -f flag is optional. If you don\u2019t provide this flag on the command line, Compose traverses the working directory and its parent directories looking for a docker-compose.yml and a docker-compose.override.yml file. You must supply at least the docker-compose.yml file. If both files are present on the same directory level, Compose combines the two files into a single configuration. The configuration in the docker-compose.override.yml file is applied over and in addition to the values in the docker-compose.yml file.","title":"Specifying multiple Compose files"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#specifying-a-path-to-a-single-compose-file","text":"You can use the -f flag to specify a path to a Compose file that is not located in the current directory, either from the command line or by setting up a COMPOSE_FILE environment variable in your shell or in an environment file. For an example of using the -f option at the command line, suppose you are running the Compose Rails sample , and have a docker-compose.yml file in a directory called sandbox/rails . You can use a command like docker-compose pull to get the postgres image for the db service from anywhere by using the -f flag as follows: docker-compose -f ~/sandbox/rails/docker-compose.yml pull db Here\u2019s the full example: $ docker - compose - f ~/ sandbox / rails / docker - compose . yml pull db Pulling db ( postgres : latest ) ... latest : Pulling from library / postgres ef0380f84d05 : Pull complete 50 cf91dc1db8 : Pull complete d3add4cd115c : Pull complete 467830 d8a616 : Pull complete 089 b9db7dc57 : Pull complete 6 fba0a36935c : Pull complete 81 ef0e73c953 : Pull complete 338 a6c4894dc : Pull complete 15853 f32f67c : Pull complete 044 c83d92898 : Pull complete 17301519 f133 : Pull complete dcca70822752 : Pull complete cecf11b8ccf3 : Pull complete Digest : sha256 : 1364924 c753d5ff7e2260cd34dc4ba05ebd40ee8193391220be0f9901d4e1651 Status : Downloaded newer image for postgres : latest","title":"Specifying a path to a single Compose file"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#use-p-to-specify-a-project-name","text":"Each configuration has a project name. If you supply a -p flag, you can specify a project name. If you don\u2019t specify the flag, Compose uses the current directory name. See also the COMPOSE_PROJECT_NAME environment variable .","title":"Use -p to specify a project name"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#use-profile-to-specify-one-or-more-active-profiles","text":"Calling docker-compose --profile frontend up will start the services with the profile frontend and services without specified profiles. You can also enable multiple profiles, e.g. with docker-compose --profile frontend --profile debug up the profiles frontend and debug will be enabled. See also Using profiles with Compose and the COMPOSE_PROFILES environment variable .","title":"Use --profile to specify one or more active profiles"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#set-up-environment-variables","text":"You can set environment variables for various docker-compose options, including the -f and -p flags. For example, the COMPOSE_FILE environment variable relates to the -f flag, and COMPOSE_PROJECT_NAME environment variable relates to the -p flag. Also, you can set some of these variables in an environment file .","title":"Set up environment variables"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#where-to-go-next","text":"CLI environment variables Declare default environment variables in file fig , composition , compose , docker , orchestration , cli , reference , docker-compose Links: Source:","title":"Where to go next"},{"location":"Docker/Docker/","text":"Docker Best Practices \u00b6 Utilize multistage builds and set DOCKER_BUILDKIT=1 environment variable to allow them to build in parallel . You can also copy files between stages in multistage builds from previous layers Split long RUN commands into multiple lines per statement and alphabetize the order of the arguments apt-get notes: don't run apt-get upgrade or dist-upgrade since that will be the job of the base image keep apt-get update and apt-get install together RUN apt-get update && apt-get install -y \\ package-bar \\ package-baz \\ package-foo Build an Image from a Github Repo (without a dockerfile) docker build -t myimage : latest -f - https : // github . com / docker-library / hello-world . git << EOF FROM busybox COPY hello . c . EOF Use ENV to update the path: ENV PATH /usr/local/nginx/bin:$PATH Use ENTRYPOINT for the main executable, with default flags provided by CMD ENTRYPOINT [\"s3cmd\"] CMD [\"--help\"] so this will show help menu: docker run s3cmd and this will do whatever the params say: docker run s3cmd ls s3://mybucket it's very common to create a script docker-entrypoint.sh as the ENTRYPOINT COPY ./docker-entrypoint.sh / ENTRYPOINT [\"/docker-entrypoint.sh\"] CMD [\"postgres\"] docker-entrypoint.sh: #!/bin/bash # exit if any commands return non-zero set -e # if first param is 'postgres' if [ \" $1 \" = 'postgres' ] ; then # assign postgres to the PGDATA directory chown -R postgres \" $PGDATA \" # if string is empty if [ -z \" $( ls -A \" $PGDATA \" ) \" ] ; then # gosu is like sudo without certain annoying TTY features gosu postgres initdb fi # $@ is all of the parameters passed through (e.g. $1 $2 ...) # exec will replace the currently executing process with a new one exec gosu postgres \" $@ \" fi exec \" $@ \" example invocations: docker run postgres : runs postres docker run postgres postgres --help : run Postgres and pass parameters to the server docker run --rm -it postgres bash : start a totally different tool, such as Bash use VOLUME for any mutable or user-servicable parts of your image avoid using sudo - gosu is a better option if you can run a service without root, use the USER command to change to the user You can create a user and set group with something like: RUN groupadd -r postgres && useradd --no-log-init -r -g postgres postgres. use absolute paths for your WORKDIR Caching only RUN, COPY, and ADD statments are cached COPY and ADD will perform a checksum on the corresponding file contents RUN will only attempt to match by the command name Once 1 cache layer is invalidated, everything dockerfile statement after it is run dynamically (not cached) So if you copy your source code in and it has changed, everything else after that is rebuilt Some people inject a specifically cache-busting layer: docker build --build-arg CACHE_BUST=$(date +%s) . ARG CACHE_BUST RUN echo \"command with external dependencies\" Dockerfile vs Docker-Compose From somebody on Docker Team Dockerfiles are the recipe for building images and should add all the binaries/other files you need to make your service work. There are a couple of exceptions to this: secrets (i.e.: credentials), configs (i.e.: configuration files), and application state data (e.g.: your database data). Note that secrets and configs are read only. Compose files are used to describe how a set of services are deployed and interact. The Compose format is used not only for a single engine (i.e.: docker-compose) but also for orchestrated environments like Swarm and Kubernetes. The goal of the Compose format is to make it easy to write an application and test it locally, then deploy it to an orchestrated environment with little or no changes. This goal limits what we can change in the format because of fundamental differences like how each environemtn handles volumes and data storage. Links: Source: Best practices for writing Dockerfiles | Docker Documentation","title":"Docker Best Practices"},{"location":"Docker/Docker/#docker-best-practices","text":"Utilize multistage builds and set DOCKER_BUILDKIT=1 environment variable to allow them to build in parallel . You can also copy files between stages in multistage builds from previous layers Split long RUN commands into multiple lines per statement and alphabetize the order of the arguments apt-get notes: don't run apt-get upgrade or dist-upgrade since that will be the job of the base image keep apt-get update and apt-get install together RUN apt-get update && apt-get install -y \\ package-bar \\ package-baz \\ package-foo Build an Image from a Github Repo (without a dockerfile) docker build -t myimage : latest -f - https : // github . com / docker-library / hello-world . git << EOF FROM busybox COPY hello . c . EOF Use ENV to update the path: ENV PATH /usr/local/nginx/bin:$PATH Use ENTRYPOINT for the main executable, with default flags provided by CMD ENTRYPOINT [\"s3cmd\"] CMD [\"--help\"] so this will show help menu: docker run s3cmd and this will do whatever the params say: docker run s3cmd ls s3://mybucket it's very common to create a script docker-entrypoint.sh as the ENTRYPOINT COPY ./docker-entrypoint.sh / ENTRYPOINT [\"/docker-entrypoint.sh\"] CMD [\"postgres\"] docker-entrypoint.sh: #!/bin/bash # exit if any commands return non-zero set -e # if first param is 'postgres' if [ \" $1 \" = 'postgres' ] ; then # assign postgres to the PGDATA directory chown -R postgres \" $PGDATA \" # if string is empty if [ -z \" $( ls -A \" $PGDATA \" ) \" ] ; then # gosu is like sudo without certain annoying TTY features gosu postgres initdb fi # $@ is all of the parameters passed through (e.g. $1 $2 ...) # exec will replace the currently executing process with a new one exec gosu postgres \" $@ \" fi exec \" $@ \" example invocations: docker run postgres : runs postres docker run postgres postgres --help : run Postgres and pass parameters to the server docker run --rm -it postgres bash : start a totally different tool, such as Bash use VOLUME for any mutable or user-servicable parts of your image avoid using sudo - gosu is a better option if you can run a service without root, use the USER command to change to the user You can create a user and set group with something like: RUN groupadd -r postgres && useradd --no-log-init -r -g postgres postgres. use absolute paths for your WORKDIR Caching only RUN, COPY, and ADD statments are cached COPY and ADD will perform a checksum on the corresponding file contents RUN will only attempt to match by the command name Once 1 cache layer is invalidated, everything dockerfile statement after it is run dynamically (not cached) So if you copy your source code in and it has changed, everything else after that is rebuilt Some people inject a specifically cache-busting layer: docker build --build-arg CACHE_BUST=$(date +%s) . ARG CACHE_BUST RUN echo \"command with external dependencies\" Dockerfile vs Docker-Compose From somebody on Docker Team Dockerfiles are the recipe for building images and should add all the binaries/other files you need to make your service work. There are a couple of exceptions to this: secrets (i.e.: credentials), configs (i.e.: configuration files), and application state data (e.g.: your database data). Note that secrets and configs are read only. Compose files are used to describe how a set of services are deployed and interact. The Compose format is used not only for a single engine (i.e.: docker-compose) but also for orchestrated environments like Swarm and Kubernetes. The goal of the Compose format is to make it easy to write an application and test it locally, then deploy it to an orchestrated environment with little or no changes. This goal limits what we can change in the format because of fundamental differences like how each environemtn handles volumes and data storage. Links: Source: Best practices for writing Dockerfiles | Docker Documentation","title":"Docker Best Practices"},{"location":"Obsidian/Obsidian%20Git%20Plugin%20Notes/","text":"Obsidian Git Plugin Notes \u00b6 Initialize with Git \u00b6 git init gh repo create --private touch .gitignore notepad .gitignore git add * git commit -m \"init\" git push --set-upstream origin master Links: Obsidian Source: denolehov/obsidian-git: Backup your Obsidian.md vault with git (github.com) obsidian-git-tut-windows/README.md at main \u00b7 gitobsidiantutorial/obsidian-git-tut-windows (github.com) ssh agent - How can I run ssh-add automatically, without a password prompt? - Unix & Linux Stack Exchange Adding an existing project to GitHub using the command line - GitHub Docs","title":"Obsidian Git Plugin Notes"},{"location":"Obsidian/Obsidian%20Git%20Plugin%20Notes/#obsidian-git-plugin-notes","text":"","title":"Obsidian Git Plugin Notes"},{"location":"Obsidian/Obsidian%20Git%20Plugin%20Notes/#initialize-with-git","text":"git init gh repo create --private touch .gitignore notepad .gitignore git add * git commit -m \"init\" git push --set-upstream origin master Links: Obsidian Source: denolehov/obsidian-git: Backup your Obsidian.md vault with git (github.com) obsidian-git-tut-windows/README.md at main \u00b7 gitobsidiantutorial/obsidian-git-tut-windows (github.com) ssh agent - How can I run ssh-add automatically, without a password prompt? - Unix & Linux Stack Exchange Adding an existing project to GitHub using the command line - GitHub Docs","title":"Initialize with Git"},{"location":"Obsidian/Obsidian/","text":"Obsidian \u00b6 Contents \u00b6 Templater Obsidian Git Github \u00b6 \u2b50= Recommended obsidian-md \u00b7 GitHub Topics Plugins & Tools \u2692\ufe0f \u00b6 denolehov/obsidian-git: Backup your Obsidian.md vault with git (github.com) \u2b50 argenos/zotero-mdnotes: A Zotero plugin to export item metadata and notes as markdown files (github.com) liamcain/obsidian-calendar-plugin: Simple calendar widget for Obsidian. (github.com) \u2b50 tgrosinger/advanced-tables-obsidian: Improved table navigation, formatting, and manipulation in Obsidian.md (github.com) \u2b50 deathau/sliding-panes-obsidian: Andy Matuschak Mode as a plugin (github.com) SilentVoid13/Templater: A template plugin for obsidian (github.com) \u2b50 jamiebrynes7/obsidian-todoist-plugin: Materialize Todoist tasks in Obsidian notes (github.com) \u2b50 tgrosinger/slated-obsidian: Task management in Obsidian.md (github.com) mgmeyers/obsidian-kanban (github.com) \u2b50 argenos/nldates-obsidian: Work with dates in natural language in Obsidian (github.com) phibr0/obsidian-charts: Charts - Obsidian Plugin | Create editable, interactive and animated Charts in Obsidian (github.com) st3v3nmw/obsidian-spaced-repetition: Fight the forgetting curve & note aging by reviewing flashcards & notes using spaced repetition on Obsidian.md (github.com) mrjackphil/obsidian-text-expand: A simple text expand plugin for Obsidian.md (github.com) \u2b50 jplattel/obsidian-query-language: An Obsidian plugin allowing you to query your notes (github.com) liamcain/obsidian-periodic-notes: Create/manage your daily, weekly, and monthly notes in Obsidian (github.com) denolehov/obsidian-url-into-selection: Paste URLs into selected text \"notion style\" (github.com) \u2b50 darlal/obsidian-switcher-plus: Enhanced Quick Switcher plugin for Obsidian.md (github.com) ryanjamurphy/review-obsidian: Add the current note to a future daily note to remember to review it. (github.com) FHachez/obsidian-convert-url-to-iframe: Plugin for Obsidian.md to convert a selected URL to an iframe. (github.com) \u2b50 visini/obsidian-icons-plugin: Add icons to your Obsidian notes \u2013 Experimental Obsidian Plugin (github.com) \u2b50 Yeboster/autocomplete-obsidian: Obsidian plugin to provide text autocomplete (github.com) \u2b50 akosbalasko/yarle: Yarle - The ultimate converter of Evernote notes to Markdown (github.com) HEmile/juggl: An interactive, stylable and expandable graph view for Obsidian. Juggl is designed as an advanced 'local' graph view, where you can juggle all your thoughts with ease. (github.com) lynchjames/note-refactor-obsidian: Allows for text selections to be copied (refactored) into new notes and notes to be split into other notes. (github.com) zoni/obsidian-export: Rust library and CLI to export an Obsidian vault to regular Markdown (github.com) \u2b50 Liamballin/ObsidianBookmark: Chrome extension and nodejs server to allow web clipping to Obsidian. (github.com) \u2b50 obsidian-userland/publish: Open source Obsidian Publish alternative (github.com) \u2b50 djsudduth/keep-it-markdown: Convert Google Keep notes dynamically to markdown for Obsidian and Notion using the unofficial Keep API (github.com) \u2b50 pjeby/tag-wrangler: Rename, merge, toggle, and search tags from the Obsidian tag pane (github.com) argenos/hotkeysplus-obsidian: Adds hotkeys to toggle todos, ordered/unordered lists and blockquotes in Obsidian (github.com) pjeby/hot-reload: Automatically reload Obsidian plugins in development when their files are changed (github.com) kepano/obsidian-minimal-settings: Settings plugin to control colors and fonts in Minimal Theme (github.com) kepano/obsidian-hider: Hide Obsidian UI elements such as tooltips, status, titlebar and more (github.com) Vinzent03/find-unlinked-files: Find files, which are nowhere linked, so they are maybe lost in your vault. (github.com) tallguyjenks/Obsidian-For-Business: Using Obsidian.... For Business! (github.com) StefanoCecere/markdown_pandoc_book_template: a template to create pdf/ePub/html/docx books by Markdown via Pandoc (github.com) akaalias/obsidian-extract-pdf-highlights: Extract highlights, underlines and annotations from your PDFs into Obsidian (github.com) ryanjamurphy/workbench-obsidian: A plugin to help you collect working materials. (github.com) mgmeyers/obsidian-style-settings: Dynamically creates a user interface for adjusting theme, plugin, and snippet CSS variables (github.com) pyrochlore/obsidian-tracker: Track everything in daily notes (github.com) deathau/cm-show-whitespace-obsidian: A plugin for [Obsidian](https://obsidian.md) which shows whitespace in the editor. (github.com) DahaWong/obsidian-footlinks: Obsidian plugin that extracts urls from the main text to footer, offering a better reading/editing experience. (github.com) lukeleppan/better-word-count: Counts the words of selected text in the editor. (github.com) renehernandez/obsidian-readwise: Sync Readwise highlights into your obsidian vault (github.com) zephraph/obsidian-tools: An unofficial collection of tools that helps you build plugins for obsidian.md (github.com) avr/obsidian-reading-time (github.com) avirut/obsidian-metatemplates: Take advantage of YAML front-matter in generating notes from templates (for obsidian.md) (github.com) mrjackphil/obsidian-jump-to-link: Quick jump between links using hotkeys (github.com) mrjackphil/obsidian-crosslink-between-notes: This plugin adds a command which allows to add a link to the current note at the bottom of selected notes (github.com) danymat/Obsidian-Markdown-Parser: This repository will give you tools to parse and fetch useful informations of your notes in your Obsidian vault. (github.com) avirut/obsidian-query2table: Represent files returned by a query as a table of their YAML frontmatter (for obsidian.md) (github.com) aviskase/obsidian-link-indexer (github.com) hadynz/obsidian-kindle-plugin: Sync your Kindle notes and highlights directly into your Obsidian vault (github.com) erichalldev/obsidian-smart-random-note: A smart random note plugin for Obsidian (github.com) pjeby/pane-relief: Obsidian plugin for per-pane history, pane movement/navigation hotkeys, and more (github.com) HEmile/obsidian-search-on-internet: Add context menu items in Obsidian to search the internet. (github.com) meld-cp/obsidian-encrypt: Hide secrets in your Obsidian.md vault (github.com) THeK3nger/obsidian-plugin-template: Template for Obsidian.md Plugins (github.com) tgrosinger/recent-files-obsidian: Display a list of most recently opened files (github.com) joethei/obsidian-plantuml: Generate PlantUML Diagrams inside Obsidian.md (github.com) ryanjamurphy/vantage-obsidian: Vantage helps you build complex queries using Obsidian's native search tools. (github.com) trashhalo/obsidian-extract-url: Plugin to extract markdown out of urls (github.com) gavvvr/obsidian-imgur-plugin: Pastes images right to imgur.com (github.com) lukeleppan/obsidian-discordrpc: Update your Discord Status to show your friends what you are working on in Obsidian. With Discord Rich Presence. (github.com) dhruvik7/obsidian-daily-stats: Plugin to view your daily word count across all notes in your Obsidian.md vault. (github.com) ze-kel/DayOne-JSON-to-MD: Converts jsons from Day One app to Markdown. Intended for transferring from DayOne to Obsidian but should work with everything else. (github.com) whateverforever/zettelwarmer: CLI Tool for Zettlr/Obsidian to help you browse random notes. The older the note, the more likely it will be shown. (github.com) liamcain/obsidian-things-logbook: Sync your Things 3 Logbook with Obsidian (github.com) akaalias/obsidian-shuffle: Create custom and randomized writing prompts (github.com) kepano/obsidian-advanced-appearance: Change Obsidian colors, fonts and other cosmetic settings (github.com) phibr0/cycle-through-panes: Cycle through Panes - Obsidian Plugin (github.com) liamcain/obsidian-creases: Mark headings to be collapsed by default (github.com) pjeby/note-folder-autorename: Obsidian plugin to support folder-overview notes by keeping their folder in sync (github.com) rbrcsk/note-tools: A collection of my tools related to notetaking (github.com) jobindj/obsidian-mkdocs: Publish Obsidian Notes with MkDocs (github.com) \u2b50 Themes \ud83c\udfa2 \u00b6 kepano/obsidian-minimal: Minimal theme for Obsidian (github.com) jplattel/obsidian-clipper: A Chrome extension that easily clips selections to Obsidian (github.com) deathau/Notation-for-Obsidian: A theme for Obsidian, inspired by and borrowing elements from Notion (github.com) dxcore35/Suddha-theme: Obsidian theme (github.com) Developers \u00b6 - tallguyjenks (Bryan Jenks) (github.com) \u00b6 Links: PKM Source:","title":"Obsidian"},{"location":"Obsidian/Obsidian/#obsidian","text":"","title":"Obsidian"},{"location":"Obsidian/Obsidian/#contents","text":"Templater Obsidian Git","title":"Contents"},{"location":"Obsidian/Obsidian/#github","text":"\u2b50= Recommended obsidian-md \u00b7 GitHub Topics","title":"Github"},{"location":"Obsidian/Obsidian/#plugins-tools","text":"denolehov/obsidian-git: Backup your Obsidian.md vault with git (github.com) \u2b50 argenos/zotero-mdnotes: A Zotero plugin to export item metadata and notes as markdown files (github.com) liamcain/obsidian-calendar-plugin: Simple calendar widget for Obsidian. (github.com) \u2b50 tgrosinger/advanced-tables-obsidian: Improved table navigation, formatting, and manipulation in Obsidian.md (github.com) \u2b50 deathau/sliding-panes-obsidian: Andy Matuschak Mode as a plugin (github.com) SilentVoid13/Templater: A template plugin for obsidian (github.com) \u2b50 jamiebrynes7/obsidian-todoist-plugin: Materialize Todoist tasks in Obsidian notes (github.com) \u2b50 tgrosinger/slated-obsidian: Task management in Obsidian.md (github.com) mgmeyers/obsidian-kanban (github.com) \u2b50 argenos/nldates-obsidian: Work with dates in natural language in Obsidian (github.com) phibr0/obsidian-charts: Charts - Obsidian Plugin | Create editable, interactive and animated Charts in Obsidian (github.com) st3v3nmw/obsidian-spaced-repetition: Fight the forgetting curve & note aging by reviewing flashcards & notes using spaced repetition on Obsidian.md (github.com) mrjackphil/obsidian-text-expand: A simple text expand plugin for Obsidian.md (github.com) \u2b50 jplattel/obsidian-query-language: An Obsidian plugin allowing you to query your notes (github.com) liamcain/obsidian-periodic-notes: Create/manage your daily, weekly, and monthly notes in Obsidian (github.com) denolehov/obsidian-url-into-selection: Paste URLs into selected text \"notion style\" (github.com) \u2b50 darlal/obsidian-switcher-plus: Enhanced Quick Switcher plugin for Obsidian.md (github.com) ryanjamurphy/review-obsidian: Add the current note to a future daily note to remember to review it. (github.com) FHachez/obsidian-convert-url-to-iframe: Plugin for Obsidian.md to convert a selected URL to an iframe. (github.com) \u2b50 visini/obsidian-icons-plugin: Add icons to your Obsidian notes \u2013 Experimental Obsidian Plugin (github.com) \u2b50 Yeboster/autocomplete-obsidian: Obsidian plugin to provide text autocomplete (github.com) \u2b50 akosbalasko/yarle: Yarle - The ultimate converter of Evernote notes to Markdown (github.com) HEmile/juggl: An interactive, stylable and expandable graph view for Obsidian. Juggl is designed as an advanced 'local' graph view, where you can juggle all your thoughts with ease. (github.com) lynchjames/note-refactor-obsidian: Allows for text selections to be copied (refactored) into new notes and notes to be split into other notes. (github.com) zoni/obsidian-export: Rust library and CLI to export an Obsidian vault to regular Markdown (github.com) \u2b50 Liamballin/ObsidianBookmark: Chrome extension and nodejs server to allow web clipping to Obsidian. (github.com) \u2b50 obsidian-userland/publish: Open source Obsidian Publish alternative (github.com) \u2b50 djsudduth/keep-it-markdown: Convert Google Keep notes dynamically to markdown for Obsidian and Notion using the unofficial Keep API (github.com) \u2b50 pjeby/tag-wrangler: Rename, merge, toggle, and search tags from the Obsidian tag pane (github.com) argenos/hotkeysplus-obsidian: Adds hotkeys to toggle todos, ordered/unordered lists and blockquotes in Obsidian (github.com) pjeby/hot-reload: Automatically reload Obsidian plugins in development when their files are changed (github.com) kepano/obsidian-minimal-settings: Settings plugin to control colors and fonts in Minimal Theme (github.com) kepano/obsidian-hider: Hide Obsidian UI elements such as tooltips, status, titlebar and more (github.com) Vinzent03/find-unlinked-files: Find files, which are nowhere linked, so they are maybe lost in your vault. (github.com) tallguyjenks/Obsidian-For-Business: Using Obsidian.... For Business! (github.com) StefanoCecere/markdown_pandoc_book_template: a template to create pdf/ePub/html/docx books by Markdown via Pandoc (github.com) akaalias/obsidian-extract-pdf-highlights: Extract highlights, underlines and annotations from your PDFs into Obsidian (github.com) ryanjamurphy/workbench-obsidian: A plugin to help you collect working materials. (github.com) mgmeyers/obsidian-style-settings: Dynamically creates a user interface for adjusting theme, plugin, and snippet CSS variables (github.com) pyrochlore/obsidian-tracker: Track everything in daily notes (github.com) deathau/cm-show-whitespace-obsidian: A plugin for [Obsidian](https://obsidian.md) which shows whitespace in the editor. (github.com) DahaWong/obsidian-footlinks: Obsidian plugin that extracts urls from the main text to footer, offering a better reading/editing experience. (github.com) lukeleppan/better-word-count: Counts the words of selected text in the editor. (github.com) renehernandez/obsidian-readwise: Sync Readwise highlights into your obsidian vault (github.com) zephraph/obsidian-tools: An unofficial collection of tools that helps you build plugins for obsidian.md (github.com) avr/obsidian-reading-time (github.com) avirut/obsidian-metatemplates: Take advantage of YAML front-matter in generating notes from templates (for obsidian.md) (github.com) mrjackphil/obsidian-jump-to-link: Quick jump between links using hotkeys (github.com) mrjackphil/obsidian-crosslink-between-notes: This plugin adds a command which allows to add a link to the current note at the bottom of selected notes (github.com) danymat/Obsidian-Markdown-Parser: This repository will give you tools to parse and fetch useful informations of your notes in your Obsidian vault. (github.com) avirut/obsidian-query2table: Represent files returned by a query as a table of their YAML frontmatter (for obsidian.md) (github.com) aviskase/obsidian-link-indexer (github.com) hadynz/obsidian-kindle-plugin: Sync your Kindle notes and highlights directly into your Obsidian vault (github.com) erichalldev/obsidian-smart-random-note: A smart random note plugin for Obsidian (github.com) pjeby/pane-relief: Obsidian plugin for per-pane history, pane movement/navigation hotkeys, and more (github.com) HEmile/obsidian-search-on-internet: Add context menu items in Obsidian to search the internet. (github.com) meld-cp/obsidian-encrypt: Hide secrets in your Obsidian.md vault (github.com) THeK3nger/obsidian-plugin-template: Template for Obsidian.md Plugins (github.com) tgrosinger/recent-files-obsidian: Display a list of most recently opened files (github.com) joethei/obsidian-plantuml: Generate PlantUML Diagrams inside Obsidian.md (github.com) ryanjamurphy/vantage-obsidian: Vantage helps you build complex queries using Obsidian's native search tools. (github.com) trashhalo/obsidian-extract-url: Plugin to extract markdown out of urls (github.com) gavvvr/obsidian-imgur-plugin: Pastes images right to imgur.com (github.com) lukeleppan/obsidian-discordrpc: Update your Discord Status to show your friends what you are working on in Obsidian. With Discord Rich Presence. (github.com) dhruvik7/obsidian-daily-stats: Plugin to view your daily word count across all notes in your Obsidian.md vault. (github.com) ze-kel/DayOne-JSON-to-MD: Converts jsons from Day One app to Markdown. Intended for transferring from DayOne to Obsidian but should work with everything else. (github.com) whateverforever/zettelwarmer: CLI Tool for Zettlr/Obsidian to help you browse random notes. The older the note, the more likely it will be shown. (github.com) liamcain/obsidian-things-logbook: Sync your Things 3 Logbook with Obsidian (github.com) akaalias/obsidian-shuffle: Create custom and randomized writing prompts (github.com) kepano/obsidian-advanced-appearance: Change Obsidian colors, fonts and other cosmetic settings (github.com) phibr0/cycle-through-panes: Cycle through Panes - Obsidian Plugin (github.com) liamcain/obsidian-creases: Mark headings to be collapsed by default (github.com) pjeby/note-folder-autorename: Obsidian plugin to support folder-overview notes by keeping their folder in sync (github.com) rbrcsk/note-tools: A collection of my tools related to notetaking (github.com) jobindj/obsidian-mkdocs: Publish Obsidian Notes with MkDocs (github.com) \u2b50","title":"Plugins &amp; Tools \u2692\ufe0f"},{"location":"Obsidian/Obsidian/#themes","text":"kepano/obsidian-minimal: Minimal theme for Obsidian (github.com) jplattel/obsidian-clipper: A Chrome extension that easily clips selections to Obsidian (github.com) deathau/Notation-for-Obsidian: A theme for Obsidian, inspired by and borrowing elements from Notion (github.com) dxcore35/Suddha-theme: Obsidian theme (github.com)","title":"Themes \ud83c\udfa2"},{"location":"Obsidian/Obsidian/#developers","text":"","title":"Developers"},{"location":"Obsidian/Obsidian/#-tallguyjenks-bryan-jenks-githubcom","text":"Links: PKM Source:","title":"- tallguyjenks (Bryan Jenks) (github.com)"},{"location":"Obsidian/Publishing%20Workflow/","text":"Publishing Workflow \u00b6 Instead of paying for the built-in Obsidian Publish Feature this workflow utilizes the fast, simple, and nice looking MkDocs static sit generator to publish an Obsidian Vault. Initial Setup \u00b6 Fork the Obsidian-MkDocs Github repo template from jobindj/obsidian-mkdocs Note: if your obsidian vault is already a git repository you may want to utilize git submodules instead of nesting git repo's. Clone the newly forked repo into your local obsidian vault Move any notes you want published into the <repo-name>/docs folder Commit and push changes to trigger the Github Action to publish your notes Example Code: # navigate to obsidian vault's directory cd < path / to / obsidian / vault > # add a git submodule for the mkdocs repo under a folder named '_published' git submodule add git @github . com : jimbrig / obsidian_published . git _published # move some notes into the _published/docs folder Configuration \u00b6 Configure the published site's mkdocs.yml configuration file located in the root level of the MkDocs folder. See MkDocs Configuration Documentation for more details https://www.mkdocs.org/#adding-pages Links: Source:","title":"Publishing Workflow"},{"location":"Obsidian/Publishing%20Workflow/#publishing-workflow","text":"Instead of paying for the built-in Obsidian Publish Feature this workflow utilizes the fast, simple, and nice looking MkDocs static sit generator to publish an Obsidian Vault.","title":"Publishing Workflow"},{"location":"Obsidian/Publishing%20Workflow/#initial-setup","text":"Fork the Obsidian-MkDocs Github repo template from jobindj/obsidian-mkdocs Note: if your obsidian vault is already a git repository you may want to utilize git submodules instead of nesting git repo's. Clone the newly forked repo into your local obsidian vault Move any notes you want published into the <repo-name>/docs folder Commit and push changes to trigger the Github Action to publish your notes Example Code: # navigate to obsidian vault's directory cd < path / to / obsidian / vault > # add a git submodule for the mkdocs repo under a folder named '_published' git submodule add git @github . com : jimbrig / obsidian_published . git _published # move some notes into the _published/docs folder","title":"Initial Setup"},{"location":"Obsidian/Publishing%20Workflow/#configuration","text":"Configure the published site's mkdocs.yml configuration file located in the root level of the MkDocs folder. See MkDocs Configuration Documentation for more details https://www.mkdocs.org/#adding-pages Links: Source:","title":"Configuration"},{"location":"Obsidian/Templater%20Plugin%20Notes/","text":"Templater Plugin Notes \u00b6 Source: Introduction | Templater (silentvoid13.github.io) Links: Personal Knowledge Management | Obsidian","title":"Templater Plugin Notes"},{"location":"Obsidian/Templater%20Plugin%20Notes/#templater-plugin-notes","text":"Source: Introduction | Templater (silentvoid13.github.io) Links: Personal Knowledge Management | Obsidian","title":"Templater Plugin Notes"},{"location":"PKM/PKM/","text":"","title":"PKM"},{"location":"Powwater/Database%20Documentation/","text":"Database Documentation \u00b6 Schema \u00b6 Connecting \u00b6 Running Locally in Docker Container \u00b6 pgsync/pg_dump to retrieve SQL from remote hosted production database spin up docker container for postgres locally with correct credentials (password=p, port=5432, dbname = postgres, etc.) create copy of remote database's public schema in the newly created docker container connect to local container database instance from apps, API, etc. Resources: \u00b6 Tools \u00b6 Database Markup Language (DBML) dbdocs.io dbdiagram.io PostgreSQL psql pgcli GUI's \u00b6 pgAdmin4 Valentina Studio DBeaver VSCode RStudio R Packages \u00b6 DBI RPostgres RPostgreSQL Pool dbplyr dbplyr dbx connections sqlpetr sqldf and more... Links: PostgreSQL | PostgreSQL Tools | System Design | Databases Source: https://techdocs.powwater.org","title":"Database Documentation"},{"location":"Powwater/Database%20Documentation/#database-documentation","text":"","title":"Database Documentation"},{"location":"Powwater/Database%20Documentation/#schema","text":"","title":"Schema"},{"location":"Powwater/Database%20Documentation/#connecting","text":"","title":"Connecting"},{"location":"Powwater/Database%20Documentation/#running-locally-in-docker-container","text":"pgsync/pg_dump to retrieve SQL from remote hosted production database spin up docker container for postgres locally with correct credentials (password=p, port=5432, dbname = postgres, etc.) create copy of remote database's public schema in the newly created docker container connect to local container database instance from apps, API, etc.","title":"Running Locally in Docker Container"},{"location":"Powwater/Database%20Documentation/#resources","text":"","title":"Resources:"},{"location":"Powwater/Database%20Documentation/#tools","text":"Database Markup Language (DBML) dbdocs.io dbdiagram.io PostgreSQL psql pgcli","title":"Tools"},{"location":"Powwater/Database%20Documentation/#guis","text":"pgAdmin4 Valentina Studio DBeaver VSCode RStudio","title":"GUI's"},{"location":"Powwater/Database%20Documentation/#r-packages","text":"DBI RPostgres RPostgreSQL Pool dbplyr dbplyr dbx connections sqlpetr sqldf and more... Links: PostgreSQL | PostgreSQL Tools | System Design | Databases Source: https://techdocs.powwater.org","title":"R Packages"},{"location":"Productivity/Notes%20on%20Finishing%20Projects/","text":"Notes on Finishing Projects \u00b6 Set Limitations \u00b6 Time : Force time constraints on yourself to avoid wasting useless time that does not reach you closer to your end [[desired outcome]]. Tools : Limit the number of possible tools at your disposal. There's never enough time to try them all out and is a perfect excuse for you to trick yourself into thinking you're being productive when you are just procrastinating . Undo's : This is a big one. Limit yourself to avoid resetting and undoing projects mid-development at all costs . Learn to live with imperfection. Links: Source:","title":"Notes on Finishing Projects"},{"location":"Productivity/Notes%20on%20Finishing%20Projects/#notes-on-finishing-projects","text":"","title":"Notes on Finishing Projects"},{"location":"Productivity/Notes%20on%20Finishing%20Projects/#set-limitations","text":"Time : Force time constraints on yourself to avoid wasting useless time that does not reach you closer to your end [[desired outcome]]. Tools : Limit the number of possible tools at your disposal. There's never enough time to try them all out and is a perfect excuse for you to trick yourself into thinking you're being productive when you are just procrastinating . Undo's : This is a big one. Limit yourself to avoid resetting and undoing projects mid-development at all costs . Learn to live with imperfection. Links: Source:","title":"Set Limitations"},{"location":"Productivity/Productivity/","text":"","title":"Productivity"},{"location":"Productivity/Time%20Management/","text":"Time Management \u00b6 Time is merely an illusion - therefore I can procrastinate and it doesn't matter! Time Tracking \u00b6 Use tools (internal, external, or simple manual tracking) to decipher what you are spending (and wasting your time) on. Simply gathering data is not enough, you should reflect on the collected times and perform a \"post-mortem\" on what you could be spending your time on. Time Tracking Tools \u00b6 TMetric Toggl Clockify etc. Manual Tracking - at [[Tychobra]] we utilize an internal application we created ourselves called T3 for this, but anything works the key is to consistently and thorouly track you time regardless of the medium used to track it. Get Clear On Priorities \u00b6 Get clear on your priorities. Batch Tasks and Time Blocks \u00b6 Use your calendar, task manager coupled with labels, or a [[time block planner]] to block your time into chunks that contain similar tasks. Learn how to say NO \u00b6 Links: Sources:","title":"Time Management"},{"location":"Productivity/Time%20Management/#time-management","text":"Time is merely an illusion - therefore I can procrastinate and it doesn't matter!","title":"Time Management"},{"location":"Productivity/Time%20Management/#time-tracking","text":"Use tools (internal, external, or simple manual tracking) to decipher what you are spending (and wasting your time) on. Simply gathering data is not enough, you should reflect on the collected times and perform a \"post-mortem\" on what you could be spending your time on.","title":"Time Tracking"},{"location":"Productivity/Time%20Management/#time-tracking-tools","text":"TMetric Toggl Clockify etc. Manual Tracking - at [[Tychobra]] we utilize an internal application we created ourselves called T3 for this, but anything works the key is to consistently and thorouly track you time regardless of the medium used to track it.","title":"Time Tracking Tools"},{"location":"Productivity/Time%20Management/#get-clear-on-priorities","text":"Get clear on your priorities.","title":"Get Clear On Priorities"},{"location":"Productivity/Time%20Management/#batch-tasks-and-time-blocks","text":"Use your calendar, task manager coupled with labels, or a [[time block planner]] to block your time into chunks that contain similar tasks.","title":"Batch Tasks and Time Blocks"},{"location":"Productivity/Time%20Management/#learn-how-to-say-no","text":"Links: Sources:","title":"Learn how to say NO"},{"location":"Project%20Management/Ludicrously%20Complex%20Projects%20in%20Software%20Development/","text":"Ludicrously Complex Projects \u00b6 Clear Ownership \u00b6 Split out the project into separate buckets of work duties with leaders for each bucket: Administrative Technical Strategy and Planning Track Progress \u00b6 Communicate and track project's progression through sharing a project timeline and use it as a single source of truth . Keep it updated to reflect reality. Make Effective Decisions \u00b6 Brainstorm and be thoughtful by taking into account any and all long and short term implications. Optimize for efficiency Organize and Communicate Manage Dependencies \u00b6 Anticipate bottlenecks \u2013 Make a table or diagram that maps out who your team relies on, and who relies on your team. Keep tabs on it \u2013 Assign one owner from each side who looks after each dependency. Make sure the dependency owners understand and communicate the impact of changes to all upstream and downstream teams. Links: Source: The Top 8 Tips for Managing Complex Software Projects (atlassian.com)","title":"Ludicrously Complex Projects"},{"location":"Project%20Management/Ludicrously%20Complex%20Projects%20in%20Software%20Development/#ludicrously-complex-projects","text":"","title":"Ludicrously Complex Projects"},{"location":"Project%20Management/Ludicrously%20Complex%20Projects%20in%20Software%20Development/#clear-ownership","text":"Split out the project into separate buckets of work duties with leaders for each bucket: Administrative Technical Strategy and Planning","title":"Clear Ownership"},{"location":"Project%20Management/Ludicrously%20Complex%20Projects%20in%20Software%20Development/#track-progress","text":"Communicate and track project's progression through sharing a project timeline and use it as a single source of truth . Keep it updated to reflect reality.","title":"Track Progress"},{"location":"Project%20Management/Ludicrously%20Complex%20Projects%20in%20Software%20Development/#make-effective-decisions","text":"Brainstorm and be thoughtful by taking into account any and all long and short term implications. Optimize for efficiency Organize and Communicate","title":"Make Effective Decisions"},{"location":"Project%20Management/Ludicrously%20Complex%20Projects%20in%20Software%20Development/#manage-dependencies","text":"Anticipate bottlenecks \u2013 Make a table or diagram that maps out who your team relies on, and who relies on your team. Keep tabs on it \u2013 Assign one owner from each side who looks after each dependency. Make sure the dependency owners understand and communicate the impact of changes to all upstream and downstream teams. Links: Source: The Top 8 Tips for Managing Complex Software Projects (atlassian.com)","title":"Manage Dependencies"},{"location":"Project%20Management/Project%20Management%20Pipeline/","text":"Project Management Pipeline \u00b6 Having a good plan is the most important strategy for getting a project done . Step 1: Collection \u00b6 Collect all work related items that are part of the project, including: Physical Handwritten Notes and Brainstorms Meeting/Call Agendas and Notes Emails/Correspondence with Client Open Tasks in Task Manager Open Github Issues Slack Messages Data received from client etc. After collecting, prioritize the pieces as tasks that are part of the project - if get stuck prioritizing ask yourself: Which one has the most immediate hard deadline? Which task will make the most positive effect if it is finished ASAP? Are any of these tasks dependent on another one? Am I dependent on another person to complete something else before starting? Is there a task that I must get off my plate to clear my mind and move forward? Step 2: Develop a Process \u00b6 Next, outline and list out all the necessary steps to complete each task, asking these three questions: What are all the tasks and micro-tasks that must be done to complete this? Who needs to weigh in on, contribute to, perform a quality assurance check on, or sign off on the work? Is this the most efficient way to get from A to Z on this particular project? Step 3: Get Organized \u00b6 Committing to being organized and finding a structure that fits the project's needs gives a fresh insight into what needs to get done, re-prioritized, or reorganized. Set a time each week, such as every Friday afternoon or Monday morning, to review work items. This not only keeps your mind fresh but also helps you see all the things that are part of a bigger project and vision. Change happens, so you\u2019re probably updating a lot of tasks in the course of a week. This review process will help you stay on top of your moving work Step 4: Just Do It \u00b6 Now that you\u2019ve completed the first four steps, it\u2019s time to take action. Pull the trigger; press publish; deliver the final product. What do you do right now ? Based on David Allen\u2019s GTD methodology, consider these four things: Context. What can you do right now? Time available. What do you have time to do right now? Energy available. What are you able to accomplish right now? Priority. After answering the first three questions, start working on the highest priority item. Links: Project Management | PKM | Productivity Source: Five Steps to Getting a Project Done | LiquidPlanner","title":"Project Management Pipeline"},{"location":"Project%20Management/Project%20Management%20Pipeline/#project-management-pipeline","text":"Having a good plan is the most important strategy for getting a project done .","title":"Project Management Pipeline"},{"location":"Project%20Management/Project%20Management%20Pipeline/#step-1-collection","text":"Collect all work related items that are part of the project, including: Physical Handwritten Notes and Brainstorms Meeting/Call Agendas and Notes Emails/Correspondence with Client Open Tasks in Task Manager Open Github Issues Slack Messages Data received from client etc. After collecting, prioritize the pieces as tasks that are part of the project - if get stuck prioritizing ask yourself: Which one has the most immediate hard deadline? Which task will make the most positive effect if it is finished ASAP? Are any of these tasks dependent on another one? Am I dependent on another person to complete something else before starting? Is there a task that I must get off my plate to clear my mind and move forward?","title":"Step 1: Collection"},{"location":"Project%20Management/Project%20Management%20Pipeline/#step-2-develop-a-process","text":"Next, outline and list out all the necessary steps to complete each task, asking these three questions: What are all the tasks and micro-tasks that must be done to complete this? Who needs to weigh in on, contribute to, perform a quality assurance check on, or sign off on the work? Is this the most efficient way to get from A to Z on this particular project?","title":"Step 2: Develop a Process"},{"location":"Project%20Management/Project%20Management%20Pipeline/#step-3-get-organized","text":"Committing to being organized and finding a structure that fits the project's needs gives a fresh insight into what needs to get done, re-prioritized, or reorganized. Set a time each week, such as every Friday afternoon or Monday morning, to review work items. This not only keeps your mind fresh but also helps you see all the things that are part of a bigger project and vision. Change happens, so you\u2019re probably updating a lot of tasks in the course of a week. This review process will help you stay on top of your moving work","title":"Step 3: Get Organized"},{"location":"Project%20Management/Project%20Management%20Pipeline/#step-4-just-do-it","text":"Now that you\u2019ve completed the first four steps, it\u2019s time to take action. Pull the trigger; press publish; deliver the final product. What do you do right now ? Based on David Allen\u2019s GTD methodology, consider these four things: Context. What can you do right now? Time available. What do you have time to do right now? Energy available. What are you able to accomplish right now? Priority. After answering the first three questions, start working on the highest priority item. Links: Project Management | PKM | Productivity Source: Five Steps to Getting a Project Done | LiquidPlanner","title":"Step 4: Just Do It"},{"location":"Project%20Management/Project%20Management/","text":"","title":"Project Management"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/","text":"Taming a Chaotic Project \u00b6 It happens to projects big and small: one day, you wake up to a mess in your workspace. Sometimes it\u2019s because of a colleague who can\u2019t load-balance, or a budget decision that left you with fewer resources. And in a certain sense, it doesn\u2019t matter why the project got off track \u2013 but it definitely matters how you untangle it. 1. Recognize Warning Signs and Act Fast \u00b6 Project snags don\u2019t magically resolve themselves. The earlier you face these problems in the project\u2019s lifecycle, the more options you have to resolve them. Throughout the life of the project, get regular status updates. Encourage your team to forecast their remaining work, factoring in any project plan changes so that potential slippages are identified as soon as possible. Use collaborative project management software to keep everyone clear on where the project\u2014and their responsibilities\u2014stands at all times. 2. Find Out What\u2019s Gone Wrong \u00b6 Don\u2019t rush into a fix without first identifying why. Without taking the time to understand the root cause of the problem, any proposed solution will be a shot in the dark. Start by talking to your team. Get their view on what\u2019s hot and what\u2019s not, and elicit their ideas for a solution. Your team\u2019s engagement and commitment to any new plan going forward will be the critical contributor to its success. And a heads-up: When the going gets tough, these information-gathering exercises can deteriorate into finger-pointing sessions so don\u2019t let that happen. Instead, press for accurate information on how to reprioritize and restructure tasks. Team members might be reticent when it comes to delivering bad news and may choose to do so in bits and pieces. Be clear that you need all of the bad news right now, otherwise you\u2019ll be re-structuring the plan every week as the news rolls in incrementally. Be sure to share what you find so that subsequent chaotic projects don\u2019t suffer the same issues. 3. Revisit the Original Plan \u00b6 Don\u2019t forget why you\u2019re doing the project in the first place. Review the original business case and check in from time to time to verify that it\u2019s still valid. It\u2019s easy to focus on doing whatever\u2019s needed to hit that next deadline, but be aware of what that mono-focusing does to the overall project (and the consequences it has on any projects that follow). You might be able to hit that deadline by throwing additional resources at it, but if this plan trashes the overall budget and subsequent delivery schedules, then that\u2019s not the way to go. You want to play the long game here. Don\u2019t be afraid to consider the effects of killing the chaotic project and walking away. In reality, this is rarely an option as it can be devastating in terms of customer relations and company reputation, but you should always weigh it up. A dogged determination to see a project through is admirable, but if it makes you unable to deliver on other commitments, it can be catastrophic for your organization. Review the financial and your resources against the overall business plan. If the chaotic project isn\u2019t going to deliver what it set out to do, then look at how you can really get there at this point in the project. 4. Review Your Resources \u00b6 When scheduling issues occur, it\u2019s easy (and common practice) to simply throw extra bodies at the chaotic project and grow the team. But this rarely yields a great result. Instead, when your project hits a speed bump, consider the following options to get your project back on track: Make sure the right people are assigned to the right tasks. You might have to do some re-delegating and re-allocating. See if you can spread portions of a meaty task among a larger team so that more tasks can be worked on in parallel rather than serially. Identify team members that you can shift from non-critical work to critical path activities. Focus on competency, not availability. Add resources with the right experience and skills so they can hit the ground running. 5. Look for new Solutions \u00b6 Look at the project scope, and ask yourself: Is there anything planned that doesn\u2019t need to be here? Are there any activities or deliverables being added or gold-plated that could be dropped or scaled back without falling short of the original requirement? (A zero-tolerance approach to scope creep can often save the day). A note of caution: When re-planning or re-prioritizing, be wary of sacrificing quality for expediency. Time can often be saved by cutting test and validation activities, but this is a false economy as you\u2019ll end up doing the work later, post-delivery. And then it will be even costlier. Review the planned deliverables and activities and strip out the non-essentials. 6. Talk to your Client \u00b6 None of us likes to share our woes with our clients, but if the relationship is sound, your client will work with you to find a solution. Get a dialogue going and develop a workable plan. Start by changing parts of the plan with your client, like the delivery schedule, or agreed-upon delivery phases, rather than overhauling the one big deadline. Who knows\u2014you might be able to extend the overall schedule but still get your client the key deliverables when they need them. 7. Review Work Processes \u00b6 We\u2019re creatures of habit. This means we might be prone to keep working on a chaotic project in the same manner we always have because it\u2019s what we\u2019re used to. But when your project\u2019s struggling, it\u2019s time to find a new, creative approach. Start by talking to your team. Let them help you identify inefficiencies and bottlenecks, and together you can come up with smarter ways to get the work done. Another trick: Review stages are often a sticking point, so if documentation keeps getting bounced from one reviewer to another, schedule a meeting-based review. Get all the players in the room so issues can be ironed out swiftly in one session. 8. Check your Dependencies \u00b6 It\u2019s easy to leave some project activities loosely-defined when you\u2019re in the first stages of planning. For example, to be on the safe side you might schedule dependencies serially (e.g., development only starts when a design is fully complete, etc.). But when you\u2019re further down the project road, it\u2019s time to see if some of these tasks and activities can be re-scheduled in a more parallel manner. A word of caution: Be careful of over-doing this fast-tracking. There\u2019s always a chance you\u2019ll have to rework things down the road, but it\u2019s always worth looking at. Also, the client can be the bottleneck if you\u2019re waiting on stakeholder participation in certain events, so communicate what you need and when you need it. Be clear on what impact that any delays will have on delivery. Don\u2019t take all the pain yourself. Identify anything that\u2019s obstructing progress on the project and work with the team to smooth out the bumps. 9. Time for Overtime? \u00b6 This is often the first thought when schedules start to flounder. Overtime should be a last resort\u2014turned to when there are few to no other options left as a deadline looms. The strange truth about overtime is that it doesn\u2019t always yield higher levels of productivity. Instead, team members who work long hours over the course of time tend to pace themselves (either consciously or sub-consciously) accordingly. The result is that they end up applying themselves less during the non-overtime hours to save themselves for the overtime portion of their day. To combat this, try laying out a rough draft of a schedule that includes a small amount of overtime, and see if this delivers a schedule that works for the project. If it doesn\u2019t work on paper, it won\u2019t work in practice. 10. Keep Managing \u00b6 It\u2019s much easier to manage a team when everything\u2019s going well. But a lot of best practices can go out the window when it\u2019s an all-hands-to-the-pumps situation to keep things afloat. As a project manager, keep talking to the team and make sure everyone is clear on what\u2019s expected of them. Roles and responsibilities can become woolly when a new plan is quickly put in place. Morale can also suffer when things go awry as team members feel they haven\u2019t delivered\u2014or, even if they have, people get discouraged when they\u2019re part of a potentially unsuccessful venture. So, don\u2019t forget to acknowledge what is going well. Check your own behavior. Leading by example, is the most crucial when times are tough. Links: Source: Ten Tips For Taming That Chaotic Project | LiquidPlanner","title":"Taming a Chaotic Project"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#taming-a-chaotic-project","text":"It happens to projects big and small: one day, you wake up to a mess in your workspace. Sometimes it\u2019s because of a colleague who can\u2019t load-balance, or a budget decision that left you with fewer resources. And in a certain sense, it doesn\u2019t matter why the project got off track \u2013 but it definitely matters how you untangle it.","title":"Taming a Chaotic Project"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#1-recognize-warning-signs-and-act-fast","text":"Project snags don\u2019t magically resolve themselves. The earlier you face these problems in the project\u2019s lifecycle, the more options you have to resolve them. Throughout the life of the project, get regular status updates. Encourage your team to forecast their remaining work, factoring in any project plan changes so that potential slippages are identified as soon as possible. Use collaborative project management software to keep everyone clear on where the project\u2014and their responsibilities\u2014stands at all times.","title":"1. Recognize Warning Signs and Act Fast"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#2-find-out-whats-gone-wrong","text":"Don\u2019t rush into a fix without first identifying why. Without taking the time to understand the root cause of the problem, any proposed solution will be a shot in the dark. Start by talking to your team. Get their view on what\u2019s hot and what\u2019s not, and elicit their ideas for a solution. Your team\u2019s engagement and commitment to any new plan going forward will be the critical contributor to its success. And a heads-up: When the going gets tough, these information-gathering exercises can deteriorate into finger-pointing sessions so don\u2019t let that happen. Instead, press for accurate information on how to reprioritize and restructure tasks. Team members might be reticent when it comes to delivering bad news and may choose to do so in bits and pieces. Be clear that you need all of the bad news right now, otherwise you\u2019ll be re-structuring the plan every week as the news rolls in incrementally. Be sure to share what you find so that subsequent chaotic projects don\u2019t suffer the same issues.","title":"2. Find Out What\u2019s Gone Wrong"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#3-revisit-the-original-plan","text":"Don\u2019t forget why you\u2019re doing the project in the first place. Review the original business case and check in from time to time to verify that it\u2019s still valid. It\u2019s easy to focus on doing whatever\u2019s needed to hit that next deadline, but be aware of what that mono-focusing does to the overall project (and the consequences it has on any projects that follow). You might be able to hit that deadline by throwing additional resources at it, but if this plan trashes the overall budget and subsequent delivery schedules, then that\u2019s not the way to go. You want to play the long game here. Don\u2019t be afraid to consider the effects of killing the chaotic project and walking away. In reality, this is rarely an option as it can be devastating in terms of customer relations and company reputation, but you should always weigh it up. A dogged determination to see a project through is admirable, but if it makes you unable to deliver on other commitments, it can be catastrophic for your organization. Review the financial and your resources against the overall business plan. If the chaotic project isn\u2019t going to deliver what it set out to do, then look at how you can really get there at this point in the project.","title":"3. Revisit the Original Plan"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#4-review-your-resources","text":"When scheduling issues occur, it\u2019s easy (and common practice) to simply throw extra bodies at the chaotic project and grow the team. But this rarely yields a great result. Instead, when your project hits a speed bump, consider the following options to get your project back on track: Make sure the right people are assigned to the right tasks. You might have to do some re-delegating and re-allocating. See if you can spread portions of a meaty task among a larger team so that more tasks can be worked on in parallel rather than serially. Identify team members that you can shift from non-critical work to critical path activities. Focus on competency, not availability. Add resources with the right experience and skills so they can hit the ground running.","title":"4. Review Your Resources"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#5-look-for-new-solutions","text":"Look at the project scope, and ask yourself: Is there anything planned that doesn\u2019t need to be here? Are there any activities or deliverables being added or gold-plated that could be dropped or scaled back without falling short of the original requirement? (A zero-tolerance approach to scope creep can often save the day). A note of caution: When re-planning or re-prioritizing, be wary of sacrificing quality for expediency. Time can often be saved by cutting test and validation activities, but this is a false economy as you\u2019ll end up doing the work later, post-delivery. And then it will be even costlier. Review the planned deliverables and activities and strip out the non-essentials.","title":"5. Look for new Solutions"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#6-talk-to-your-client","text":"None of us likes to share our woes with our clients, but if the relationship is sound, your client will work with you to find a solution. Get a dialogue going and develop a workable plan. Start by changing parts of the plan with your client, like the delivery schedule, or agreed-upon delivery phases, rather than overhauling the one big deadline. Who knows\u2014you might be able to extend the overall schedule but still get your client the key deliverables when they need them.","title":"6. Talk to your Client"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#7-review-work-processes","text":"We\u2019re creatures of habit. This means we might be prone to keep working on a chaotic project in the same manner we always have because it\u2019s what we\u2019re used to. But when your project\u2019s struggling, it\u2019s time to find a new, creative approach. Start by talking to your team. Let them help you identify inefficiencies and bottlenecks, and together you can come up with smarter ways to get the work done. Another trick: Review stages are often a sticking point, so if documentation keeps getting bounced from one reviewer to another, schedule a meeting-based review. Get all the players in the room so issues can be ironed out swiftly in one session.","title":"7. Review Work Processes"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#8-check-your-dependencies","text":"It\u2019s easy to leave some project activities loosely-defined when you\u2019re in the first stages of planning. For example, to be on the safe side you might schedule dependencies serially (e.g., development only starts when a design is fully complete, etc.). But when you\u2019re further down the project road, it\u2019s time to see if some of these tasks and activities can be re-scheduled in a more parallel manner. A word of caution: Be careful of over-doing this fast-tracking. There\u2019s always a chance you\u2019ll have to rework things down the road, but it\u2019s always worth looking at. Also, the client can be the bottleneck if you\u2019re waiting on stakeholder participation in certain events, so communicate what you need and when you need it. Be clear on what impact that any delays will have on delivery. Don\u2019t take all the pain yourself. Identify anything that\u2019s obstructing progress on the project and work with the team to smooth out the bumps.","title":"8. Check your Dependencies"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#9-time-for-overtime","text":"This is often the first thought when schedules start to flounder. Overtime should be a last resort\u2014turned to when there are few to no other options left as a deadline looms. The strange truth about overtime is that it doesn\u2019t always yield higher levels of productivity. Instead, team members who work long hours over the course of time tend to pace themselves (either consciously or sub-consciously) accordingly. The result is that they end up applying themselves less during the non-overtime hours to save themselves for the overtime portion of their day. To combat this, try laying out a rough draft of a schedule that includes a small amount of overtime, and see if this delivers a schedule that works for the project. If it doesn\u2019t work on paper, it won\u2019t work in practice.","title":"9. Time for Overtime?"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#10-keep-managing","text":"It\u2019s much easier to manage a team when everything\u2019s going well. But a lot of best practices can go out the window when it\u2019s an all-hands-to-the-pumps situation to keep things afloat. As a project manager, keep talking to the team and make sure everyone is clear on what\u2019s expected of them. Roles and responsibilities can become woolly when a new plan is quickly put in place. Morale can also suffer when things go awry as team members feel they haven\u2019t delivered\u2014or, even if they have, people get discouraged when they\u2019re part of a potentially unsuccessful venture. So, don\u2019t forget to acknowledge what is going well. Check your own behavior. Leading by example, is the most crucial when times are tough. Links: Source: Ten Tips For Taming That Chaotic Project | LiquidPlanner","title":"10. Keep Managing"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/","text":"Base Package Hidden Gems in R \ud83d\udc8e \u00b6 autoload Family \u00b6 Related: delayedAssign and library These are amazingly underutilized functions from base R. Description: \u00b6 autoload creates a promise-to-evaluate autoloader and stores it with name name in the .AutoloadEnv environment. When R attempts to evaluate name , autoloader is run, the package is loaded and name is re-evaluated in the new package's environment ! The result is that R behaves as if package was loaded but it does not occupy memory. .Autoloaded contains the names of the packages for which autoloading has been promised. Usage \u00b6 autoload : autoload(name, package, reset = FALSE, ...) autoloader : autoloader(name, package, ...) .AutoloadEnv : see below .Autoloaded : see below Arguments \u00b6 name - string giving the name of an object package - string giving the name of a package containing the object reset - logical: for internal use by autoloader ... - other arguments to library Examples \u00b6 require ( stats ) autoload ( \"interpSpline\" , \"splines\" ) search () ls ( \"Autoloads\" ) . Autoloaded x <- sort ( stats :: rnorm ( 12 )) y <- x ^ 2 is <- interpSpline ( x , y ) search () ## now has splines detach ( \"package:splines\" ) search () is2 <- interpSpline ( x , y + x ) search () ## and again detach ( \"package:splines\" ) My Take \u00b6 I find autoload extremely useful for incorporating functions from packages in my [[.Rprofile| Rprofile ]]. For example, I like to utilized magrittr's %>% pipe and usually do not want to have to library it in for ad-hoc analysis on the fly; therefore by including autoload(\"%>%\", \"magrittr\") in my .Rprofile , I have complete access to %>% without magrittr cluttering up my search path or namespaced environment (especially useful on Windows). With that snippet included in my .Rprofile I can use %>% later on in the same R session and magrittr will not get attached until I use it. Other ideas for autoload useful functions would be: dplyr 's suite: mutate , filter , select , etc. lubdridate 's ymd and other date utility functions fs 's file navigation functions. ... Reduce \u00b6 Reduce function (note the capital \u201cR\u201d). Reduce takes a list or vector as input, and reduces it down to a single element. It works by applying a function to the first two elements of the vector or list, and then applying the same function to that result with the third element. This new result gets passed with the fourth element into the function and so on until a single object remains. If the input is a vector, the result will be a single number or character. On the other hand, inputting a list can have interesting results. A list of data frames can be reduced down to a single data frame, a list of vectors can be collapsed into a matrix, and so on. A simple, though not entirely useful, example of how this works is like so: test <- 1 : 10 result <- Reduce ( sum , test ) Here, result will equal 55 , which happens to be the sum of the vector test i.e. the sum of the integers 1 through 10. Reduce solves for this by first applying the sum function to 1 and 2 (the first two elements in test). This equals 3, which then gets summed with the next element in the vector, 3. This total of 6 gets added to 4, which equals 10, and so on. The process can be seen below. $1 + 2 = 3$ $3 + 3 = 6$ $6 + 4 = 10$ $10 + 5 = 15$ $15 + 6 = 21$ $21 + 7 = 28$ $28 + 8 = 36$ $36 + 9 = 45$ $45 + 10 = 55$ Now, how about something a little more useful? What if you had a list of vectors and you wanted to combine them into a matrix? test <- list ( 1 : 3 , 4 : 6 , 7 : 9 , 10 : 12 , 13 : 15 , 16 : 18 ) matrix_result <- Reduce ( rbind , test ) In this case, we have a list of six three-element vectors. Reduce applies rbind to the first two vectors, 1:3 and 4:6 initially. This creates a 2 x 3 matrix, where the first row is 1:3, and the second row is 4:6. 1 2 3 4 5 6 Then, the above result is combined (via rbind ) to the next vector in the list, 7:9. 1 2 3 4 5 6 7 8 9 This process continues, as you can see below: 1 2 3 4 5 6 7 8 9 10 11 12 Next: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Finally: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Thus, the final result is a single object \u2014 but in this case, is a 6 x 3 matrix because rbind collapsed all of the vectors of the list, test, into a single matrix. Similarly, you could run this example using cbind instead of rbind and that would collapse the vectors column-wise, rather than row-wise. Another example where Reduce comes in handy might be if you want to combine a collection of data frames into a single one. state_data <- list ( FL = data.frame ( state = c ( \"FL\" , \"FL\" , \"FL\" ), city = c ( \"Miami\" , \"Jacksonville\" , \"Saint Augustine\" )), NY = data.frame ( state = c ( \"NY\" , \"NY\" , \"NY\" ), city = c ( \"NYC\" , \"Buffalo\" , \"Rochester\" )), MD = data.frame ( state = c ( \"MD\" , \"MD\" , \"MD\" ), city = c ( \"Baltimore\" , \"Annapolis\" , \"Ocean City\" )) ) combined <- data.frame ( Reduce ( rbind , state_data )) Filter \u00b6 The Filter function does basically what it sounds like \u2014 it applies a filter to a vector, list, or data frame (which is actually a type of list). It takes two main inputs, a function that applies the filter, and the object for which the filter applies. Here\u2019s a simple example: test <- 1 : 10 less_than_5 <- Filter ( function ( x ) x < 5 , test ) This, once again, creates a vector of the first 10 positive integers. The Filter function applies function(x) x < 5 to each element, x , in the vector, test . In other words, it checks each element, x , for the Boolean expression, x < 5 . If an element is not less than 5, it gets filtered out. So you might be thinking\u2026can\u2019t this be done like this? less_than_5 <- test[test < 5] \u2026and the answer is\u2026yes. It can be done that way. Filter is more useful as a function in cases involving data frames or lists. Suppose, for instance, you want to remove all constant columns from a data frame. This is something that may be done when preprocessing data prior to modeling, as a constant attribute isn\u2019t particular useful. This is can be done in one line using Filter df <- data . frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( 1 , 1 , 1 ), d = c ( 3 , 4 , 5 )) without_constants <- Filter ( function ( x ) length ( unique ( x )) > 1 , df ) Alternatively, using dplyr\u2019s n_distinct function, which counts the number of distinct elements in a vector, you could do this: library ( dplyr ) df <- data . frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( 1 , 1 , 1 ), d = c ( 3 , 4 , 5 )) without_constants <- Filter ( function ( x ) n_distinct ( x ) > 1 , df ) In the example, we create a data frame with four columns \u2014 two of them are constant. Filter tests whether there is more than one unique value in each column. If there is only one unique value, then we know the column is constant, and it gets filtered out. Each element x is a vector, or column, in the data frame. If you wanted to just drop all columns that are all NAs, you could make a minor tweak like this: df <- data.frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( 1 , 1 , 1 ), d = c ( NA , NA , NA )) without_nas <- Filter ( function ( x ) ! all ( is.na ( x )), df ) Filter can also be used on a regular list as well. Suppose you have a list of vectors, where some of the vectors are characters, while others are numeric. If want to filter out all of the non-numeric vectors, you could call Filter : sample_list <- list ( a = c ( 1 , 2 , 3 ), b = c ( \"is\" , \"a\" , \"character\" ), c = c ( 4 , 5 , 6 ), d = c ( \"is\" , \"another\" , \"character\" )) only_numeric <- Filter ( function ( x ) is.numeric ( x ), sample_list ) rapply \u00b6 The rapply function is part of the apply family of functions in R. It has a few different uses, but one of my favorite applications for it is to apply a function to columns of a data frame that belong to a specific class, or have a particular data type. Let\u2019s say you want to get the sum of all of the numeric columns. df <- data.frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( \"r\" , \"is\" , \"awesome\" ), d = c ( 3 , 4 , 5 ), e = c ( \"some\" , \"other\" , \"character\" )) summed_columns <- rapply ( df , sum , class = \"numeric\" ) Similar to sapply or lapply , rapply takes a list / vector / data frame as input, along with a function to be applied. However, it can also take a \u201cclass\u201d parameter, which allows us to specify what class of object we want our function to be used for. rapply can also be used to recursively apply functions to nested lists (see examples from its documentation here ). rep \u00b6 The last function I want to mention for this post is the rep function. This can be used to repeat a value as many times as you want. So if you want to create a vector of 1000 5\u2019s, it could be done like this: rep(5, 1000) Here\u2019s a couple other examples: rep(\"a\", 500) rep(\"repeat this\", 100) If you pass a vector with more than one element to rep , the entire vector gets repeated the number of times you specify. rep(c(1,2,3), 100) The above code will create a vector with 300 elements \u2014 the number of elements in c(1,2,3) times 100, repeating 1, 2, 3 over and over. Links: Source:","title":"Base Package Hidden Gems in R \ud83d\udc8e"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#base-package-hidden-gems-in-r","text":"","title":"Base Package Hidden Gems in R \ud83d\udc8e"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#autoload-family","text":"Related: delayedAssign and library These are amazingly underutilized functions from base R.","title":"autoload Family"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#description","text":"autoload creates a promise-to-evaluate autoloader and stores it with name name in the .AutoloadEnv environment. When R attempts to evaluate name , autoloader is run, the package is loaded and name is re-evaluated in the new package's environment ! The result is that R behaves as if package was loaded but it does not occupy memory. .Autoloaded contains the names of the packages for which autoloading has been promised.","title":"Description:"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#usage","text":"autoload : autoload(name, package, reset = FALSE, ...) autoloader : autoloader(name, package, ...) .AutoloadEnv : see below .Autoloaded : see below","title":"Usage"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#arguments","text":"name - string giving the name of an object package - string giving the name of a package containing the object reset - logical: for internal use by autoloader ... - other arguments to library","title":"Arguments"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#examples","text":"require ( stats ) autoload ( \"interpSpline\" , \"splines\" ) search () ls ( \"Autoloads\" ) . Autoloaded x <- sort ( stats :: rnorm ( 12 )) y <- x ^ 2 is <- interpSpline ( x , y ) search () ## now has splines detach ( \"package:splines\" ) search () is2 <- interpSpline ( x , y + x ) search () ## and again detach ( \"package:splines\" )","title":"Examples"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#my-take","text":"I find autoload extremely useful for incorporating functions from packages in my [[.Rprofile| Rprofile ]]. For example, I like to utilized magrittr's %>% pipe and usually do not want to have to library it in for ad-hoc analysis on the fly; therefore by including autoload(\"%>%\", \"magrittr\") in my .Rprofile , I have complete access to %>% without magrittr cluttering up my search path or namespaced environment (especially useful on Windows). With that snippet included in my .Rprofile I can use %>% later on in the same R session and magrittr will not get attached until I use it. Other ideas for autoload useful functions would be: dplyr 's suite: mutate , filter , select , etc. lubdridate 's ymd and other date utility functions fs 's file navigation functions. ...","title":"My Take"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#reduce","text":"Reduce function (note the capital \u201cR\u201d). Reduce takes a list or vector as input, and reduces it down to a single element. It works by applying a function to the first two elements of the vector or list, and then applying the same function to that result with the third element. This new result gets passed with the fourth element into the function and so on until a single object remains. If the input is a vector, the result will be a single number or character. On the other hand, inputting a list can have interesting results. A list of data frames can be reduced down to a single data frame, a list of vectors can be collapsed into a matrix, and so on. A simple, though not entirely useful, example of how this works is like so: test <- 1 : 10 result <- Reduce ( sum , test ) Here, result will equal 55 , which happens to be the sum of the vector test i.e. the sum of the integers 1 through 10. Reduce solves for this by first applying the sum function to 1 and 2 (the first two elements in test). This equals 3, which then gets summed with the next element in the vector, 3. This total of 6 gets added to 4, which equals 10, and so on. The process can be seen below. $1 + 2 = 3$ $3 + 3 = 6$ $6 + 4 = 10$ $10 + 5 = 15$ $15 + 6 = 21$ $21 + 7 = 28$ $28 + 8 = 36$ $36 + 9 = 45$ $45 + 10 = 55$ Now, how about something a little more useful? What if you had a list of vectors and you wanted to combine them into a matrix? test <- list ( 1 : 3 , 4 : 6 , 7 : 9 , 10 : 12 , 13 : 15 , 16 : 18 ) matrix_result <- Reduce ( rbind , test ) In this case, we have a list of six three-element vectors. Reduce applies rbind to the first two vectors, 1:3 and 4:6 initially. This creates a 2 x 3 matrix, where the first row is 1:3, and the second row is 4:6. 1 2 3 4 5 6 Then, the above result is combined (via rbind ) to the next vector in the list, 7:9. 1 2 3 4 5 6 7 8 9 This process continues, as you can see below: 1 2 3 4 5 6 7 8 9 10 11 12 Next: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Finally: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Thus, the final result is a single object \u2014 but in this case, is a 6 x 3 matrix because rbind collapsed all of the vectors of the list, test, into a single matrix. Similarly, you could run this example using cbind instead of rbind and that would collapse the vectors column-wise, rather than row-wise. Another example where Reduce comes in handy might be if you want to combine a collection of data frames into a single one. state_data <- list ( FL = data.frame ( state = c ( \"FL\" , \"FL\" , \"FL\" ), city = c ( \"Miami\" , \"Jacksonville\" , \"Saint Augustine\" )), NY = data.frame ( state = c ( \"NY\" , \"NY\" , \"NY\" ), city = c ( \"NYC\" , \"Buffalo\" , \"Rochester\" )), MD = data.frame ( state = c ( \"MD\" , \"MD\" , \"MD\" ), city = c ( \"Baltimore\" , \"Annapolis\" , \"Ocean City\" )) ) combined <- data.frame ( Reduce ( rbind , state_data ))","title":"Reduce"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#filter","text":"The Filter function does basically what it sounds like \u2014 it applies a filter to a vector, list, or data frame (which is actually a type of list). It takes two main inputs, a function that applies the filter, and the object for which the filter applies. Here\u2019s a simple example: test <- 1 : 10 less_than_5 <- Filter ( function ( x ) x < 5 , test ) This, once again, creates a vector of the first 10 positive integers. The Filter function applies function(x) x < 5 to each element, x , in the vector, test . In other words, it checks each element, x , for the Boolean expression, x < 5 . If an element is not less than 5, it gets filtered out. So you might be thinking\u2026can\u2019t this be done like this? less_than_5 <- test[test < 5] \u2026and the answer is\u2026yes. It can be done that way. Filter is more useful as a function in cases involving data frames or lists. Suppose, for instance, you want to remove all constant columns from a data frame. This is something that may be done when preprocessing data prior to modeling, as a constant attribute isn\u2019t particular useful. This is can be done in one line using Filter df <- data . frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( 1 , 1 , 1 ), d = c ( 3 , 4 , 5 )) without_constants <- Filter ( function ( x ) length ( unique ( x )) > 1 , df ) Alternatively, using dplyr\u2019s n_distinct function, which counts the number of distinct elements in a vector, you could do this: library ( dplyr ) df <- data . frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( 1 , 1 , 1 ), d = c ( 3 , 4 , 5 )) without_constants <- Filter ( function ( x ) n_distinct ( x ) > 1 , df ) In the example, we create a data frame with four columns \u2014 two of them are constant. Filter tests whether there is more than one unique value in each column. If there is only one unique value, then we know the column is constant, and it gets filtered out. Each element x is a vector, or column, in the data frame. If you wanted to just drop all columns that are all NAs, you could make a minor tweak like this: df <- data.frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( 1 , 1 , 1 ), d = c ( NA , NA , NA )) without_nas <- Filter ( function ( x ) ! all ( is.na ( x )), df ) Filter can also be used on a regular list as well. Suppose you have a list of vectors, where some of the vectors are characters, while others are numeric. If want to filter out all of the non-numeric vectors, you could call Filter : sample_list <- list ( a = c ( 1 , 2 , 3 ), b = c ( \"is\" , \"a\" , \"character\" ), c = c ( 4 , 5 , 6 ), d = c ( \"is\" , \"another\" , \"character\" )) only_numeric <- Filter ( function ( x ) is.numeric ( x ), sample_list )","title":"Filter"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#rapply","text":"The rapply function is part of the apply family of functions in R. It has a few different uses, but one of my favorite applications for it is to apply a function to columns of a data frame that belong to a specific class, or have a particular data type. Let\u2019s say you want to get the sum of all of the numeric columns. df <- data.frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( \"r\" , \"is\" , \"awesome\" ), d = c ( 3 , 4 , 5 ), e = c ( \"some\" , \"other\" , \"character\" )) summed_columns <- rapply ( df , sum , class = \"numeric\" ) Similar to sapply or lapply , rapply takes a list / vector / data frame as input, along with a function to be applied. However, it can also take a \u201cclass\u201d parameter, which allows us to specify what class of object we want our function to be used for. rapply can also be used to recursively apply functions to nested lists (see examples from its documentation here ).","title":"rapply"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#rep","text":"The last function I want to mention for this post is the rep function. This can be used to repeat a value as many times as you want. So if you want to create a vector of 1000 5\u2019s, it could be done like this: rep(5, 1000) Here\u2019s a couple other examples: rep(\"a\", 500) rep(\"repeat this\", 100) If you pass a vector with more than one element to rep , the entire vector gets repeated the number of times you specify. rep(c(1,2,3), 100) The above code will create a vector with 300 elements \u2014 the number of elements in c(1,2,3) times 100, repeating 1, 2, 3 over and over. Links: Source:","title":"rep"},{"location":"R/Data%20Validation%20Packages%20in%20R/","text":"Data Validation Packages and Tools in R \u00b6 Packages \u00b6 pointblank assertr assertthat checkmate ruler ensurer tester sealr validateIt More leaned towards validation: naniar skimr validate Miscellaneous/Related rstudio/shinyvalidate shinyFeedback rjsonvalidate validator lumberjack : Track changes in data with ease Lumberjack-App : The lumberjack-app is a Shiny Application that predicts tree volume in cubic feet from a linear model using data from the \"trees\" dataset in the R datasets package","title":"Data Validation Packages and Tools in R"},{"location":"R/Data%20Validation%20Packages%20in%20R/#data-validation-packages-and-tools-in-r","text":"","title":"Data Validation Packages and Tools in R"},{"location":"R/Data%20Validation%20Packages%20in%20R/#packages","text":"pointblank assertr assertthat checkmate ruler ensurer tester sealr validateIt More leaned towards validation: naniar skimr validate Miscellaneous/Related rstudio/shinyvalidate shinyFeedback rjsonvalidate validator lumberjack : Track changes in data with ease Lumberjack-App : The lumberjack-app is a Shiny Application that predicts tree volume in cubic feet from a linear model using data from the \"trees\" dataset in the R datasets package","title":"Packages"},{"location":"R/Databases%20with%20R%20Resources/","text":"Databases with R Resources \u00b6 Links: Source:","title":"Databases with R Resources"},{"location":"R/Databases%20with%20R%20Resources/#databases-with-r-resources","text":"Links: Source:","title":"Databases with R Resources"},{"location":"R/EDA%20Packages%20in%20R/","text":"EDA Packages in R \u00b6 Exploratory Data Analysis | R for Data Science (had.co.nz) dataMaid (CRAN package) - automated checks of data validity. DataExplorer (CRAN package) - automated data exploration (including univariate and bivariate plots, PCA) and treatment. funModeling (CRAN package) - automated EDA, simple feature engineering and outlier detection. SmartEDA ( CRAN | Github ) - automated generation of descriptive statistics and uni- and bivariate plots, parallel coordinate plots. Details can be found in a dedicated paper . autoEDA (GitHub package) - automated EDA with uni- and bivariate plots. An article with an introduction can be found on LinkedIn . visdat (CRAN package) - 6 exploratory/diagnostic plots for initial data analysis. dlookr (CRAN package) - tools for data quality diagnosis, basic exploration and feature transformations. xray (CRAN package) - first look at the data - distributions and anomalies. More in the blog post . arsenal (CRAN package) - statistical summaries (models and exploration) and quick reporting. RtutoR (CRAN package) - learning material with a automatic reports module. More at R-Bloggers . exploreR (CRAN package) - exploration based on univariate linear regression. summarytools (CRAN package) - table to summarise datasets and perform simple uni- and bivariate analyses. inspectdf (CRAN package) - tools for column-wise exploration and comparison of data frames. Examples are provided in a README of the GitHub repo . explore (CRAN package) - interactive Shiny app for comprehensive dataset exploration (including uni- and bivariate relationships, correlation analysis and simple modeling with decision trees) and stand-alone function for quick exploration. Examples are given in a vignette . skimr (CRAN package) - well formatted summaries of data frames, vector and matrices. Examples are provided in a vignette . janitor (CRAN package) - a tools for fast data cleaning. All functionalities are introduced in the vignette . autoplotly (CRAN package) - a library for fast visualization of statistical results supported by ggfortify. Details can be found in the vignette or JOSS paper brinton (CRAN package) - packages for quick exploration and visualization. Details can be found in the documentation . AEDA (GitHub package) - summary statistics, correlation analysis, cluster analysis, PCA & other projections. automatic-data-explorer (GitHub package) - basic EDA and creating Markdown reports from multiple R scripts. xda (GitHub package) - basic data summaries. modeler (GitHub package) - tools for exploration and pre-processing. IEDA (GitHub package) - EDA simplified through interactive visualization. dfvis (GitHub package) - ggplot2 based implementation of tabplot. Domain-specific packages \u00b6 compMS2Miner: An Automatable Metabolite Identification, Visualization, and Data-Sharing R Package for High-Resolution LC-MS Data Sets RBioPlot (GitHub package) - automated data analysis and visualization for molecular biology. Details can be found in the paper at NCBI . ExPanDaR - package for interactive data visualization. Designed for longitudinal data, but can be also used with other types of data after setting an artificial time variable. Shiny apps with examples are provided on the github website of the package . brolgar (GitHub package) - tools to assist in longitudinal data analysis POMA (Bioconductor package) - structured, reproducible and easy-to-use workflow for the visualization, pre-processing, exploratory data analysis and statistical analysis of mass spectrometry data. POMA R/Shiny version available here . featuretoolsR (CRAN package) - R port to Python library for automated feature engineering. vtreat (CRAN package) - data treatment (pre-processing) that includes dealing with missing data and large categorical variables. Details can be found in the paper about vtreat . report - automated modeling report generation. FactoInvestigate (CRAN package) - has an automatic reporting module which selects best plots that summarise different projection techniques. gtsummary (GitHub package) - presentation-ready tables summarizing data sets, regression models, and more. clean (CRAN package) - fast data cleaning. finalfit (CRAN package) - tables and plots to quickly visualize regression results. modelsummary (GitHub package) - summary tables for regression models. Python libraries \u00b6 General Packages \u00b6 DataPrep (pip library) - data preparation library with an EDA package. Dora (pip library) - data cleaning, featuring engineering and simple modeling tools. statsModels (pip library) - collection of statistical tools, including EDA. TPOT (pip library) - autoML tool with feature engineering module. HoloViews (pip library) - automated visualization based on short data annotations. lens (pip library) - fast calculation of summary statistics and correlations. Presentation about the library . pandas-profiling - popular library for quick data summaries and correlation analysis. speedML (pip library) - large library for ML with module dedicated to fast EDA. edaviz - Python library for fast data exploration that provides functions for dataset overviews, bivariate plots and finding good predictors. (Free version only works for small datasets). AutoViz - Python library for automated visualization. ExploriPy - Python library for various EDA tasks. pandas-summary - simple extension to pandas.describe. sweetviz - visualizations for automated EDA. Related packages \u00b6 featuretools - library for automated feature engineering. pyvtreat - Python version of the R's vtreat package. autoimpute - easier handling of missing values. Auto_TS - automated time series modeling. Stata packages \u00b6 eda - a package that produces a pdf report with all permutations of univariate and bivariate visualizations and tables. Notably, three-dimensional displays are also possible. Web services \u00b6 DIVE - MIT's tools for data exploration that tries to choose best (most informative) visualizations. Automatic Statistician - tool for automated EDA and modeling. Several Shiny apps by R Squared Computing, including visulizer and descriptr . Standalone software \u00b6 auto-eda - automatic EDA with SQL. elycite - tools for exploration and modelling available (locally) as an web application. Designed for NLP problems. Papers and short articles \u00b6 Methods and tools for autoEDA \u00b6 Interactive Data Exploration with \u201cBig Data Tukey Plots\u201d - automated visualization of big data. Extracting Top-K Insights from Multi-dimensional Data . Agency plus Automation: Designing Artificial Intelligence into Interactive Systems The Landscape of R Packages for Automated Exploratory Data Analysis Issues in Automating Exploratory Data Analysis Automating anomaly detection for exploratory data analytics Task-Oriented Optimal Sequencing of Visualization Charts A Rank-by-Feature Framework for Interactive Exploration of Multidimensional Data - A paper that describe many measures that can be used to sort 1d and 2d data displays. Towards a benefit-based optimizer for Interactive Data Analysis Spotfire: an information exploration environment AlphaClean: Automatic Generation ofData Cleaning Pipelines Testing MS Excel's autoEDA tool Visualization recommendation frameworks \u00b6 Foresight: Recommending Visual Insights - Foresight is a system that helps the user rapidly discover visual insights from large high-dimensional datasets. DIVE: A Mixed-Initiative System Supporting Integrated Data Exploration Workflows . The web app is available on MIT website . Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations . Voyager 2: Augmenting Visual Analysis with Partial View Specifications VizML: A Machine Learning Approach to Visualization Recommendation VizDeck: Streamlining Exploratory Visual Analytics of Scientific Data Augmented analytics \u00b6 Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication . Conference presentations \u00b6 Automating exploratory data analysis tasks with eda - Billy Buchanan","title":"EDA Packages in R"},{"location":"R/EDA%20Packages%20in%20R/#eda-packages-in-r","text":"Exploratory Data Analysis | R for Data Science (had.co.nz) dataMaid (CRAN package) - automated checks of data validity. DataExplorer (CRAN package) - automated data exploration (including univariate and bivariate plots, PCA) and treatment. funModeling (CRAN package) - automated EDA, simple feature engineering and outlier detection. SmartEDA ( CRAN | Github ) - automated generation of descriptive statistics and uni- and bivariate plots, parallel coordinate plots. Details can be found in a dedicated paper . autoEDA (GitHub package) - automated EDA with uni- and bivariate plots. An article with an introduction can be found on LinkedIn . visdat (CRAN package) - 6 exploratory/diagnostic plots for initial data analysis. dlookr (CRAN package) - tools for data quality diagnosis, basic exploration and feature transformations. xray (CRAN package) - first look at the data - distributions and anomalies. More in the blog post . arsenal (CRAN package) - statistical summaries (models and exploration) and quick reporting. RtutoR (CRAN package) - learning material with a automatic reports module. More at R-Bloggers . exploreR (CRAN package) - exploration based on univariate linear regression. summarytools (CRAN package) - table to summarise datasets and perform simple uni- and bivariate analyses. inspectdf (CRAN package) - tools for column-wise exploration and comparison of data frames. Examples are provided in a README of the GitHub repo . explore (CRAN package) - interactive Shiny app for comprehensive dataset exploration (including uni- and bivariate relationships, correlation analysis and simple modeling with decision trees) and stand-alone function for quick exploration. Examples are given in a vignette . skimr (CRAN package) - well formatted summaries of data frames, vector and matrices. Examples are provided in a vignette . janitor (CRAN package) - a tools for fast data cleaning. All functionalities are introduced in the vignette . autoplotly (CRAN package) - a library for fast visualization of statistical results supported by ggfortify. Details can be found in the vignette or JOSS paper brinton (CRAN package) - packages for quick exploration and visualization. Details can be found in the documentation . AEDA (GitHub package) - summary statistics, correlation analysis, cluster analysis, PCA & other projections. automatic-data-explorer (GitHub package) - basic EDA and creating Markdown reports from multiple R scripts. xda (GitHub package) - basic data summaries. modeler (GitHub package) - tools for exploration and pre-processing. IEDA (GitHub package) - EDA simplified through interactive visualization. dfvis (GitHub package) - ggplot2 based implementation of tabplot.","title":"EDA Packages in R"},{"location":"R/EDA%20Packages%20in%20R/#domain-specific-packages","text":"compMS2Miner: An Automatable Metabolite Identification, Visualization, and Data-Sharing R Package for High-Resolution LC-MS Data Sets RBioPlot (GitHub package) - automated data analysis and visualization for molecular biology. Details can be found in the paper at NCBI . ExPanDaR - package for interactive data visualization. Designed for longitudinal data, but can be also used with other types of data after setting an artificial time variable. Shiny apps with examples are provided on the github website of the package . brolgar (GitHub package) - tools to assist in longitudinal data analysis POMA (Bioconductor package) - structured, reproducible and easy-to-use workflow for the visualization, pre-processing, exploratory data analysis and statistical analysis of mass spectrometry data. POMA R/Shiny version available here . featuretoolsR (CRAN package) - R port to Python library for automated feature engineering. vtreat (CRAN package) - data treatment (pre-processing) that includes dealing with missing data and large categorical variables. Details can be found in the paper about vtreat . report - automated modeling report generation. FactoInvestigate (CRAN package) - has an automatic reporting module which selects best plots that summarise different projection techniques. gtsummary (GitHub package) - presentation-ready tables summarizing data sets, regression models, and more. clean (CRAN package) - fast data cleaning. finalfit (CRAN package) - tables and plots to quickly visualize regression results. modelsummary (GitHub package) - summary tables for regression models.","title":"Domain-specific packages"},{"location":"R/EDA%20Packages%20in%20R/#python-libraries","text":"","title":"Python libraries"},{"location":"R/EDA%20Packages%20in%20R/#general-packages","text":"DataPrep (pip library) - data preparation library with an EDA package. Dora (pip library) - data cleaning, featuring engineering and simple modeling tools. statsModels (pip library) - collection of statistical tools, including EDA. TPOT (pip library) - autoML tool with feature engineering module. HoloViews (pip library) - automated visualization based on short data annotations. lens (pip library) - fast calculation of summary statistics and correlations. Presentation about the library . pandas-profiling - popular library for quick data summaries and correlation analysis. speedML (pip library) - large library for ML with module dedicated to fast EDA. edaviz - Python library for fast data exploration that provides functions for dataset overviews, bivariate plots and finding good predictors. (Free version only works for small datasets). AutoViz - Python library for automated visualization. ExploriPy - Python library for various EDA tasks. pandas-summary - simple extension to pandas.describe. sweetviz - visualizations for automated EDA.","title":"General Packages"},{"location":"R/EDA%20Packages%20in%20R/#related-packages","text":"featuretools - library for automated feature engineering. pyvtreat - Python version of the R's vtreat package. autoimpute - easier handling of missing values. Auto_TS - automated time series modeling.","title":"Related packages"},{"location":"R/EDA%20Packages%20in%20R/#stata-packages","text":"eda - a package that produces a pdf report with all permutations of univariate and bivariate visualizations and tables. Notably, three-dimensional displays are also possible.","title":"Stata packages"},{"location":"R/EDA%20Packages%20in%20R/#web-services","text":"DIVE - MIT's tools for data exploration that tries to choose best (most informative) visualizations. Automatic Statistician - tool for automated EDA and modeling. Several Shiny apps by R Squared Computing, including visulizer and descriptr .","title":"Web services"},{"location":"R/EDA%20Packages%20in%20R/#standalone-software","text":"auto-eda - automatic EDA with SQL. elycite - tools for exploration and modelling available (locally) as an web application. Designed for NLP problems.","title":"Standalone software"},{"location":"R/EDA%20Packages%20in%20R/#papers-and-short-articles","text":"","title":"Papers and short articles"},{"location":"R/EDA%20Packages%20in%20R/#methods-and-tools-for-autoeda","text":"Interactive Data Exploration with \u201cBig Data Tukey Plots\u201d - automated visualization of big data. Extracting Top-K Insights from Multi-dimensional Data . Agency plus Automation: Designing Artificial Intelligence into Interactive Systems The Landscape of R Packages for Automated Exploratory Data Analysis Issues in Automating Exploratory Data Analysis Automating anomaly detection for exploratory data analytics Task-Oriented Optimal Sequencing of Visualization Charts A Rank-by-Feature Framework for Interactive Exploration of Multidimensional Data - A paper that describe many measures that can be used to sort 1d and 2d data displays. Towards a benefit-based optimizer for Interactive Data Analysis Spotfire: an information exploration environment AlphaClean: Automatic Generation ofData Cleaning Pipelines Testing MS Excel's autoEDA tool","title":"Methods and tools for autoEDA"},{"location":"R/EDA%20Packages%20in%20R/#visualization-recommendation-frameworks","text":"Foresight: Recommending Visual Insights - Foresight is a system that helps the user rapidly discover visual insights from large high-dimensional datasets. DIVE: A Mixed-Initiative System Supporting Integrated Data Exploration Workflows . The web app is available on MIT website . Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations . Voyager 2: Augmenting Visual Analysis with Partial View Specifications VizML: A Machine Learning Approach to Visualization Recommendation VizDeck: Streamlining Exploratory Visual Analytics of Scientific Data","title":"Visualization recommendation frameworks"},{"location":"R/EDA%20Packages%20in%20R/#augmented-analytics","text":"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication .","title":"Augmented analytics"},{"location":"R/EDA%20Packages%20in%20R/#conference-presentations","text":"Automating exploratory data analysis tasks with eda - Billy Buchanan","title":"Conference presentations"},{"location":"R/Plumber%20Logging/","text":"Plumber Logging \u00b6 The plumber R package is used to expose R functions as API endpoints. Due to plumber\u2019s incredible flexibility, most major API design decisions are left up to the developer. One important consideration to be made when developing APIs is how to log information about API requests and responses. This information can be used to determine how plumber APIs are performing and how they are being utilized. An example of logging API requests in plumber is included in the package documentation . That example uses a filter to log information about incoming requests before a response has been generated. This is certainly a valid approach, but it means that the log cannot contain details about the response since the response hasn\u2019t been created yet. In this post we will look at an alternative approach to logging plumber APIs that uses preroute and postroute hooks to log information about each API request and its associated response. Logging \u00b6 Logging packages for R: logger Example API \u00b6 In this example, I use the logger package to generate the actual log entries. Using this package isn\u2019t required, but it does provide some convenient functionality that we will take advantage of. Since we will be registering hooks for our API, we will need both a plumber.R file and an entrypoint.R file. The plumber.R file contains the following: # plumber.R library ( plumber ) #* @apiTitle Logging Example #* @apiDescription Simple example API for implementing logging with Plumber #* Echo back the input #* @param msg The message to echo #* @get /echo function ( msg = \"\" ) { list ( msg = paste0 ( \"The message is: '\" , msg , \"'\" )) } #* Plot a histogram #* @png #* @get /plot function () { rand <- rnorm ( 100 ) hist ( rand ) } Now that we\u2019ve defined two endpoints ( /echo and /plot ), we can use entrypoint.R to setup logging using preroute and postroute hooks. First, we need to configure the logger package: # entrypoint.R library ( plumber ) # logging library ( logger ) # Specify how logs are written log_dir <- \"logs\" if ( ! fs :: dir_exists ( log_dir )) fs :: dir_create ( log_dir ) log_appender ( appender_tee ( tempfile ( \"plumber_\" , log_dir , \".log\" ))) The log_appender() function is used to specify which appender method is used for logging. Here we use appender_tee() so that logs will be written to stdout and to a specific file path. We create a directory called logs/ in the current working directory to store the resulting logs. Every log file is assigned a unique name using tempfile() . This prevents errors that can occur if concurrent processes try to write to the same file. Now, we need to create a helper function that we will use when creating log entries: convert_empty <- function ( string ) { if ( string == \"\" ) { \"-\" } else { string } } This function takes an empty string and converts it into a dash ( \"-\" ). We will use this to ensure that empty log values still get recorded so that it is easy to read the log files. We\u2019re now ready to create our plumber router and register the hooks necessary for logging: pr <- plumb ( \"plumber.R\" ) pr $ registerHooks ( list ( preroute = function () { # Start timer for log info tictoc :: tic () }, postroute = function ( req , res ) { end <- tictoc :: toc ( quiet = TRUE ) # Log details about the request and the response log_info ( '{convert_empty(req$REMOTE_ADDR)} \"{convert_empty(req$HTTP_USER_AGENT)}\" {convert_empty(req$HTTP_HOST)} {convert_empty(req$REQUEST_METHOD)} {convert_empty(req$PATH_INFO)} {convert_empty(res$status)} {round(end$toc - end$tic, digits = getOption(\"digits\", 5))}' ) } ) ) pr Links: R Development Source: Plumber Logging \u00b7 R Views (rstudio.com)","title":"Plumber Logging"},{"location":"R/Plumber%20Logging/#plumber-logging","text":"The plumber R package is used to expose R functions as API endpoints. Due to plumber\u2019s incredible flexibility, most major API design decisions are left up to the developer. One important consideration to be made when developing APIs is how to log information about API requests and responses. This information can be used to determine how plumber APIs are performing and how they are being utilized. An example of logging API requests in plumber is included in the package documentation . That example uses a filter to log information about incoming requests before a response has been generated. This is certainly a valid approach, but it means that the log cannot contain details about the response since the response hasn\u2019t been created yet. In this post we will look at an alternative approach to logging plumber APIs that uses preroute and postroute hooks to log information about each API request and its associated response.","title":"Plumber Logging"},{"location":"R/Plumber%20Logging/#logging","text":"Logging packages for R: logger","title":"Logging"},{"location":"R/Plumber%20Logging/#example-api","text":"In this example, I use the logger package to generate the actual log entries. Using this package isn\u2019t required, but it does provide some convenient functionality that we will take advantage of. Since we will be registering hooks for our API, we will need both a plumber.R file and an entrypoint.R file. The plumber.R file contains the following: # plumber.R library ( plumber ) #* @apiTitle Logging Example #* @apiDescription Simple example API for implementing logging with Plumber #* Echo back the input #* @param msg The message to echo #* @get /echo function ( msg = \"\" ) { list ( msg = paste0 ( \"The message is: '\" , msg , \"'\" )) } #* Plot a histogram #* @png #* @get /plot function () { rand <- rnorm ( 100 ) hist ( rand ) } Now that we\u2019ve defined two endpoints ( /echo and /plot ), we can use entrypoint.R to setup logging using preroute and postroute hooks. First, we need to configure the logger package: # entrypoint.R library ( plumber ) # logging library ( logger ) # Specify how logs are written log_dir <- \"logs\" if ( ! fs :: dir_exists ( log_dir )) fs :: dir_create ( log_dir ) log_appender ( appender_tee ( tempfile ( \"plumber_\" , log_dir , \".log\" ))) The log_appender() function is used to specify which appender method is used for logging. Here we use appender_tee() so that logs will be written to stdout and to a specific file path. We create a directory called logs/ in the current working directory to store the resulting logs. Every log file is assigned a unique name using tempfile() . This prevents errors that can occur if concurrent processes try to write to the same file. Now, we need to create a helper function that we will use when creating log entries: convert_empty <- function ( string ) { if ( string == \"\" ) { \"-\" } else { string } } This function takes an empty string and converts it into a dash ( \"-\" ). We will use this to ensure that empty log values still get recorded so that it is easy to read the log files. We\u2019re now ready to create our plumber router and register the hooks necessary for logging: pr <- plumb ( \"plumber.R\" ) pr $ registerHooks ( list ( preroute = function () { # Start timer for log info tictoc :: tic () }, postroute = function ( req , res ) { end <- tictoc :: toc ( quiet = TRUE ) # Log details about the request and the response log_info ( '{convert_empty(req$REMOTE_ADDR)} \"{convert_empty(req$HTTP_USER_AGENT)}\" {convert_empty(req$HTTP_HOST)} {convert_empty(req$REQUEST_METHOD)} {convert_empty(req$PATH_INFO)} {convert_empty(res$status)} {round(end$toc - end$tic, digits = getOption(\"digits\", 5))}' ) } ) ) pr Links: R Development Source: Plumber Logging \u00b7 R Views (rstudio.com)","title":"Example API"},{"location":"R/Plumber%20REST%20APIs%20in%20R/","text":"Building REST APIs with R and Plumber \u00b6 REST stands for \u201cRepresentational State Transfer\u201d, meaning it represents a set of rules developers follow when creating APIs (i.e. you get a responding piece of data, the response, whenever you make a request to a particular URL). Every request is composed of these four parts: Endpoint - a part of the URL - The endpoint for https://example.com/predict is /predict . Method - a type of request you\u2019re sending; used to perform one of these actions: Create, Read, Update, Delete (CRUD) . Can be one of the following: GET POST PUT PATCH DELETE Headers \u2013 used for providing information (think authentication credentials, for example). They are provided as key-value pairs. Body \u2013 information that is sent to the server. Used only when not making GET requests. Most of the time, the response returned after making a request is in JSON format. The alternative format is XML, but JSON is more common. You can also return other objects, such as images instead. You\u2019ll learn how to do that today. R allows you to develop REST APIs with the plumber package. You can read the official documentation here. It\u2019s easy to repurpose any R script file to an API with plumber, because you only have to decorate your functions with comments. You\u2019ll see all about it in a bit.","title":"Building REST APIs with R and Plumber"},{"location":"R/Plumber%20REST%20APIs%20in%20R/#building-rest-apis-with-r-and-plumber","text":"REST stands for \u201cRepresentational State Transfer\u201d, meaning it represents a set of rules developers follow when creating APIs (i.e. you get a responding piece of data, the response, whenever you make a request to a particular URL). Every request is composed of these four parts: Endpoint - a part of the URL - The endpoint for https://example.com/predict is /predict . Method - a type of request you\u2019re sending; used to perform one of these actions: Create, Read, Update, Delete (CRUD) . Can be one of the following: GET POST PUT PATCH DELETE Headers \u2013 used for providing information (think authentication credentials, for example). They are provided as key-value pairs. Body \u2013 information that is sent to the server. Used only when not making GET requests. Most of the time, the response returned after making a request is in JSON format. The alternative format is XML, but JSON is more common. You can also return other objects, such as images instead. You\u2019ll learn how to do that today. R allows you to develop REST APIs with the plumber package. You can read the official documentation here. It\u2019s easy to repurpose any R script file to an API with plumber, because you only have to decorate your functions with comments. You\u2019ll see all about it in a bit.","title":"Building REST APIs with R and Plumber"},{"location":"R/Plumber%20Resources/","text":"Plumber Resources \u00b6 Links: Source:","title":"Plumber Resources"},{"location":"R/Plumber%20Resources/#plumber-resources","text":"Links: Source:","title":"Plumber Resources"},{"location":"R/R%20Books%20List/","text":"R Books List \u00b6 Chapter 4 Dependency utilities | Outstanding User Interfaces with Shiny Welcome | Mastering Shiny Engineering Production-Grade Shiny Apps 5.3 Shiny | R Markdown: The Definitive Guide Shiny (Bonus) | R, Not the Best Practices rOpenSci Packages: Development, Maintenance, and Peer Review Welcome! | R Packages Welcome | R for Data Science Welcome | Advanced R R Programming for Data Science R Markdown Cookbook Data Science at the Command Line, 1e blogdown: Creating Websites with R Markdown R Markdown: The Definitive Guide Efficient R programming JavaScript for R Data Skills for Reproducible Science Working with Data in R Introduction to Research Methods Fundamentals of Data Visualization Geocomputation with R Mastering Software Development in R Github actions with R Modern R with the tidyverse Chapter 5 Data validation | Data Preparation: Essential Steps Before & After Analysis Designing and Building Data Science Solutions","title":"R Books List"},{"location":"R/R%20Books%20List/#r-books-list","text":"Chapter 4 Dependency utilities | Outstanding User Interfaces with Shiny Welcome | Mastering Shiny Engineering Production-Grade Shiny Apps 5.3 Shiny | R Markdown: The Definitive Guide Shiny (Bonus) | R, Not the Best Practices rOpenSci Packages: Development, Maintenance, and Peer Review Welcome! | R Packages Welcome | R for Data Science Welcome | Advanced R R Programming for Data Science R Markdown Cookbook Data Science at the Command Line, 1e blogdown: Creating Websites with R Markdown R Markdown: The Definitive Guide Efficient R programming JavaScript for R Data Skills for Reproducible Science Working with Data in R Introduction to Research Methods Fundamentals of Data Visualization Geocomputation with R Mastering Software Development in R Github actions with R Modern R with the tidyverse Chapter 5 Data validation | Data Preparation: Essential Steps Before & After Analysis Designing and Building Data Science Solutions","title":"R Books List"},{"location":"R/R%20Development/","text":"R Development \u00b6 Plumber Logging","title":"R Development"},{"location":"R/R%20Development/#r-development","text":"Plumber Logging","title":"R Development"},{"location":"R/R%20Miscellaneous%20Notes/","text":"R Miscellaneous Notes \u00b6 ### Package Development Guidelines The script should be used to document all changes and additions to package code. In addition, git-flow branches should be used for all added features and bug fixes. The **development** branch serves as the \u201ctest\u201d / \u201cwork-in-progress\u201d branch while the **master** branch is the \u201cproduction\u201d branch and should only be updated when creating a new **release**. For example, say I wanted to add a new feature / function - the following steps would be used: Using *Git-Flow*, a new branch should be created off the current **development** branch with a name corresponding to the new feature. Once in the new branch, add functions via the **devhist.R** script via . Edit the new .R file and commit changes and push to feature branch. Add tests for new function and document additions via , and document this in the **devhist.R** script. Add example usage of the new feature via , and document this in the **devhist.R** script. Document, Test, Check, Build, and Install. Finally, merge feature branch with development branch using git-flow and if desired create a new release and merge with master branch updating the package version. Ideas \u00b6 Functions \u00b6 Workflow Functions \u00b6 Project Configuration Version Control and Git Directory Structure Project Variables External Data Sources Caching Pipelines Databases and Query\u2019s GoogleDrive Files Metadata Codebooks, Data Dictionaries, Name Tables Documentation CI Testing Code Review Actuarial Functions \u00b6 Compare to Prior Retentions Policy Periods Grouping by Occurrence Exposure Lagging Experience Mod Factors Industry Loss Costs Data Validation Checks Manual Adjustments Triangle Formation LDF Derivation LDF Interpolation Discount Factors Data Summaries Loss Costs Severity Frequency Credibiilty Allocation Projections Development Methods BF Methods Parrallelogram Methods Evaluation Dates / Year-Month Time Series Data Persistency NCCI Credibility Simulations and Confidence Intervals / Levels Benchmarking Contigency Tables Survival Analysis Utility Functions \u00b6 Parsing Dates Excel Data Structures \u00b6 Lossruns Triangles Exposures Industry Loss Costs RStudio Addins \u00b6 Project Init Dependencies Styling Create Script with Header Git Setup Name Table Addin Excel Integration Addin(s) Remote / Raw Data Directories (Persistent Data Storage) Shiny App Init Find and Replace Data Manipulation (MiniUI with a Pivot Table) Markdown Addins Prefixer / @importFrom Clipboard Templates \u00b6 Project Directory Package Directory Shiny App Directory Markdown HTML Template README Template Package DESCRIPTION template Excel Output Template Shiny Apps HTML Templates HTML Widget Templates PDF Acturial Report Template Latex Tables Shiny Apps \u00b6 Modules Highcharter DT CSS, Bootstrap, Java, HTML Options \u00b6 Global options .Rprofile .Renviron Git BB & GH Conflicts DT Knitr Pre-Commit Hooks Shiny Highcharter Data \u00b6 CRM - People, Emails, Phones, Expertise, Clients, Budgets, etc. Historical Archived Data (Versioning) Git LFS Example Lossruns, Exposures, Industry Data Triangles, LDFs, Discount Factors Simulation Outputs Transactional Data Liberty, Sedgwick, etc extraction. Resources and Learning \u00b6 https://rtask.thinkr.fr/when-development-starts-with-documentation/ https://emilyriederer.netlify.com/post/rmarkdown-driven-development/ https://r-pkgs.org/index.html https://github.com/ThinkR-open/golem https://thinkr-open.github.io/building-shiny-apps-workflow/ https://github.com/ThinkR-open/chameleon https://github.com/rorynolan/exampletestr https://github.com/ThinkR-open/attachment/blob/master/devstuff_history.R Links: Source:","title":"R Miscellaneous Notes"},{"location":"R/R%20Miscellaneous%20Notes/#r-miscellaneous-notes","text":"### Package Development Guidelines The script should be used to document all changes and additions to package code. In addition, git-flow branches should be used for all added features and bug fixes. The **development** branch serves as the \u201ctest\u201d / \u201cwork-in-progress\u201d branch while the **master** branch is the \u201cproduction\u201d branch and should only be updated when creating a new **release**. For example, say I wanted to add a new feature / function - the following steps would be used: Using *Git-Flow*, a new branch should be created off the current **development** branch with a name corresponding to the new feature. Once in the new branch, add functions via the **devhist.R** script via . Edit the new .R file and commit changes and push to feature branch. Add tests for new function and document additions via , and document this in the **devhist.R** script. Add example usage of the new feature via , and document this in the **devhist.R** script. Document, Test, Check, Build, and Install. Finally, merge feature branch with development branch using git-flow and if desired create a new release and merge with master branch updating the package version.","title":"R Miscellaneous Notes"},{"location":"R/R%20Miscellaneous%20Notes/#ideas","text":"","title":"Ideas"},{"location":"R/R%20Miscellaneous%20Notes/#functions","text":"","title":"Functions"},{"location":"R/R%20Miscellaneous%20Notes/#workflow-functions","text":"Project Configuration Version Control and Git Directory Structure Project Variables External Data Sources Caching Pipelines Databases and Query\u2019s GoogleDrive Files Metadata Codebooks, Data Dictionaries, Name Tables Documentation CI Testing Code Review","title":"Workflow Functions"},{"location":"R/R%20Miscellaneous%20Notes/#actuarial-functions","text":"Compare to Prior Retentions Policy Periods Grouping by Occurrence Exposure Lagging Experience Mod Factors Industry Loss Costs Data Validation Checks Manual Adjustments Triangle Formation LDF Derivation LDF Interpolation Discount Factors Data Summaries Loss Costs Severity Frequency Credibiilty Allocation Projections Development Methods BF Methods Parrallelogram Methods Evaluation Dates / Year-Month Time Series Data Persistency NCCI Credibility Simulations and Confidence Intervals / Levels Benchmarking Contigency Tables Survival Analysis","title":"Actuarial Functions"},{"location":"R/R%20Miscellaneous%20Notes/#utility-functions","text":"Parsing Dates Excel","title":"Utility Functions"},{"location":"R/R%20Miscellaneous%20Notes/#data-structures","text":"Lossruns Triangles Exposures Industry Loss Costs","title":"Data Structures"},{"location":"R/R%20Miscellaneous%20Notes/#rstudio-addins","text":"Project Init Dependencies Styling Create Script with Header Git Setup Name Table Addin Excel Integration Addin(s) Remote / Raw Data Directories (Persistent Data Storage) Shiny App Init Find and Replace Data Manipulation (MiniUI with a Pivot Table) Markdown Addins Prefixer / @importFrom Clipboard","title":"RStudio Addins"},{"location":"R/R%20Miscellaneous%20Notes/#templates","text":"Project Directory Package Directory Shiny App Directory Markdown HTML Template README Template Package DESCRIPTION template Excel Output Template Shiny Apps HTML Templates HTML Widget Templates PDF Acturial Report Template Latex Tables","title":"Templates"},{"location":"R/R%20Miscellaneous%20Notes/#shiny-apps","text":"Modules Highcharter DT CSS, Bootstrap, Java, HTML","title":"Shiny Apps"},{"location":"R/R%20Miscellaneous%20Notes/#options","text":"Global options .Rprofile .Renviron Git BB & GH Conflicts DT Knitr Pre-Commit Hooks Shiny Highcharter","title":"Options"},{"location":"R/R%20Miscellaneous%20Notes/#data","text":"CRM - People, Emails, Phones, Expertise, Clients, Budgets, etc. Historical Archived Data (Versioning) Git LFS Example Lossruns, Exposures, Industry Data Triangles, LDFs, Discount Factors Simulation Outputs Transactional Data Liberty, Sedgwick, etc extraction.","title":"Data"},{"location":"R/R%20Miscellaneous%20Notes/#resources-and-learning","text":"https://rtask.thinkr.fr/when-development-starts-with-documentation/ https://emilyriederer.netlify.com/post/rmarkdown-driven-development/ https://r-pkgs.org/index.html https://github.com/ThinkR-open/golem https://thinkr-open.github.io/building-shiny-apps-workflow/ https://github.com/ThinkR-open/chameleon https://github.com/rorynolan/exampletestr https://github.com/ThinkR-open/attachment/blob/master/devstuff_history.R Links: Source:","title":"Resources and Learning"},{"location":"R/RStudio%20Configuration%20Notes/","text":"RStudio Configuration \u00b6 Directories \u00b6 %localappdata%\\RStudio-Desktop - RStudio Desktop Internal State %appdata%\\RStudio - RStudio Configuration Directory (Preferences)","title":"RStudio Configuration"},{"location":"R/RStudio%20Configuration%20Notes/#rstudio-configuration","text":"","title":"RStudio Configuration"},{"location":"R/RStudio%20Configuration%20Notes/#directories","text":"%localappdata%\\RStudio-Desktop - RStudio Desktop Internal State %appdata%\\RStudio - RStudio Configuration Directory (Preferences)","title":"Directories"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/","text":"Shiny Apps as Packages in R \u00b6 Books \u00b6 Engineering Shiny Book Chapter on Structuring App as Package \u2b50 Mastering Shiny Package Chapter \u2b50 Blog Posts \u00b6 Workflow: Part 1 Workflow: Part 2 Articles \u00b6 Building Shiny App as a Package \u2b50 Packaging Shiny Applications: A Deep Dive \u2b50 Best Practices for Developing Robust Shiny Dashboards as R Packages \u2b50 Dean Attali: Packaging Shiny Apps Further Reading \u00b6 R Package Development \u00b6 These resources are for general package development within the R ecosystem. R-Core & CRAN \u00b6 Official R Manuals Writing R Extensions \u2b50 CRAN Task Views Homepage \u2b50 Robust Research Web Technologies \u2b50 rOpenSci \u00b6 rOpenSci Package DevGuide \u2b50 BioConductor \u00b6 BioConductor Package Guidelines \u2b50 Building Packages for BioConductor Unit Testing Creating Workflow Packages \u2b50 RStudio \u00b6 Developing Packages with RStudio Building, Testing and Distributing Packages Writing Package Documentation RStudio Video: Auto-Magic Package Development Books \u00b6 See R-Project's Book Listing for more resources on all topics R related. R Packages (Hadley Wickham) \u2b50 Advanced R (Hadley Wickam) Advanced R Course (Florian Priv\u00e9) R Package Workshop Bookdown \u2b50 Best Practices: R Development Workshop \u2b50 Advanced Topics: R Development Workshop Package Development in R Package Development Chapter in \"Modern R with Tidyverse\" \u2b50 Tutorials \u00b6 Writing an R package from scratch (Hilary Parker) Writing an R package from scratch (Updated) (Thomas Westlake) usethis workflow for package development (Emil Hvitfeldt) R package primer (Karl Broman) \u2b50 R Package Development Pictorial (Matthew J Denny) Building R Packages with Devtools (Jiddu Alexander) Developing R packages (Jeff Leek) R Package Tutorial (Colautti Lab) Instructions for creating your own R package (MIT) How to Create and Distribute an R Package (Shian Su) Workshops \u00b6 R Forwards Package Workshop (Chicago, February 23, 2019) Write your own R package (UBC STAT 545) Blogs \u00b6 How to develop good R packages (for open science) (Malle Salmon) R-Task: RMD First Development \u2b50 Style \u00b6 Tidyverse Google Jean Fan A Computational Analysis of the Dynamics of R Style Based on 94 Million Lines of Code from All CRAN Packages in the Past 20 Years. (Yen, C.Y., Chang, M.H.W., Chan, C.H.) Shiny Modules \u00b6 https://shiny.rstudio.com/articles/modules.html Desktop Applications \u00b6 https://www.travishinkelman.com/deploy-shiny-electron/ Links: Source:","title":"Shiny Apps as Packages in R"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#shiny-apps-as-packages-in-r","text":"","title":"Shiny Apps as Packages in R"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#books","text":"Engineering Shiny Book Chapter on Structuring App as Package \u2b50 Mastering Shiny Package Chapter \u2b50","title":"Books"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#blog-posts","text":"Workflow: Part 1 Workflow: Part 2","title":"Blog Posts"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#articles","text":"Building Shiny App as a Package \u2b50 Packaging Shiny Applications: A Deep Dive \u2b50 Best Practices for Developing Robust Shiny Dashboards as R Packages \u2b50 Dean Attali: Packaging Shiny Apps","title":"Articles"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#further-reading","text":"","title":"Further Reading"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#r-package-development","text":"These resources are for general package development within the R ecosystem.","title":"R Package Development"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#r-core-cran","text":"Official R Manuals Writing R Extensions \u2b50 CRAN Task Views Homepage \u2b50 Robust Research Web Technologies \u2b50","title":"R-Core &amp; CRAN"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#ropensci","text":"rOpenSci Package DevGuide \u2b50","title":"rOpenSci"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#bioconductor","text":"BioConductor Package Guidelines \u2b50 Building Packages for BioConductor Unit Testing Creating Workflow Packages \u2b50","title":"BioConductor"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#rstudio","text":"Developing Packages with RStudio Building, Testing and Distributing Packages Writing Package Documentation RStudio Video: Auto-Magic Package Development","title":"RStudio"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#books_1","text":"See R-Project's Book Listing for more resources on all topics R related. R Packages (Hadley Wickham) \u2b50 Advanced R (Hadley Wickam) Advanced R Course (Florian Priv\u00e9) R Package Workshop Bookdown \u2b50 Best Practices: R Development Workshop \u2b50 Advanced Topics: R Development Workshop Package Development in R Package Development Chapter in \"Modern R with Tidyverse\" \u2b50","title":"Books"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#tutorials","text":"Writing an R package from scratch (Hilary Parker) Writing an R package from scratch (Updated) (Thomas Westlake) usethis workflow for package development (Emil Hvitfeldt) R package primer (Karl Broman) \u2b50 R Package Development Pictorial (Matthew J Denny) Building R Packages with Devtools (Jiddu Alexander) Developing R packages (Jeff Leek) R Package Tutorial (Colautti Lab) Instructions for creating your own R package (MIT) How to Create and Distribute an R Package (Shian Su)","title":"Tutorials"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#workshops","text":"R Forwards Package Workshop (Chicago, February 23, 2019) Write your own R package (UBC STAT 545)","title":"Workshops"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#blogs","text":"How to develop good R packages (for open science) (Malle Salmon) R-Task: RMD First Development \u2b50","title":"Blogs"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#style","text":"Tidyverse Google Jean Fan A Computational Analysis of the Dynamics of R Style Based on 94 Million Lines of Code from All CRAN Packages in the Past 20 Years. (Yen, C.Y., Chang, M.H.W., Chan, C.H.)","title":"Style"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#shiny-modules","text":"https://shiny.rstudio.com/articles/modules.html","title":"Shiny Modules"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#desktop-applications","text":"https://www.travishinkelman.com/deploy-shiny-electron/ Links: Source:","title":"Desktop Applications"},{"location":"R/Tools%20Package%20Hidden%20Gems%20in%20R/","text":"Check Unstated Dependencies in Tests: tools:::check_packages_used_in_tests() Source: r-devel/r-svn) tools ::: check_packages_used_in_tests ( dir = \".\" , testdir = \"tests/testthat\" )","title":"Tools Package Hidden Gems in R"},{"location":"R/Useful%20Packages%20in%20R%20List/","text":"Useful R Packages to Look Into \u00b6 Awesome Lists \u00b6 qinwf/awesome-R: A curated list of awesome R packages, frameworks and software grabear/awesome-rshiny: An awesome R-shiny list! R Universe \u00b6 R-Universe provides monorepos for R packages and renders them into nice websites. Source Github Repository Website data-cleaning r-universe/data-cleaning data-cleaning.r-universe.dev rOpenSci r-universe/ropensci ropensci.r-universe.dev RStudio r-universe/rstudio rstudio.r-universe.dev Appsilon r-universe/appsilon appsilon.r-universe.dev r-dbi r-universe/r-dbi r-dbi.r-universe.dev JohnCoene r-universe/johncoene johncoene.r-universe.dev HenrikBengtsson r-universe/henrikbengtsson henrikbengtsson.r-universe.dev daattali r-universe/daattali daattali.r-universe.dev eddelbuettel r-universe/eddelbuettel eddelbuettel.r-universe.dev r-forge r-universe/r-forge r-forge.r-universe.dev dreamRs r-universe/dreamrs dreamrs.r-universe.dev MarkEdmondson1234 r-universe/markedmondson1234 markedmondson1234.r-universe.dev jimhester r-universe/jimhester jimhester.r-universe.dev RinteRface r-universe/rinterface rinterface.r-universe.dev ColinFay r-universe/colinfay colinfay.r-universe.dev tidyverse r-universe/tidyverse tidyverse.r-universe.dev r-spatial r-universe/r-spatial r-spatial.r-universe.dev cboettig r-universe/cboettig cboettig.r-universe.dev yihui (Yihui Xie) r-universe/yihui: yihui yihui.r-universe.dev (Various) r-universe/test: test test.r-universe.dev the cloudyr project r-universe/cloudyr cloudyr.r-universe.dev Microsoft Azure r-universe/azure azure.r-universe.dev R Shiny Related \u00b6 ThinkR-open/golem: A Framework for Building Robust Shiny Apps colinfay/golemexamples tyronehunt/GolemShinyTemplate ColinFay/glouton: 'JS-cookies' in Shiny ColinFay/cordes: Boilerplate for Wrapping Node Modules in R packages r4fun/keys: Keyboard Shortcuts for shiny ColinFay/crrry: 'crrri' recipes for shiny ColinFay/dockerfiler: Easy Dockerfile Creation from R yonicd/whereami: Reliably return location where command is called from in R ColinFay/minifyr: Wrapper around node-minify NodeJS module grabear/shiny-pathfinder-loot: An R shiny app used to keep up with loot tables on Google Sheets during Pathfinder/RPG sessions","title":"Useful R Packages to Look Into"},{"location":"R/Useful%20Packages%20in%20R%20List/#useful-r-packages-to-look-into","text":"","title":"Useful R Packages to Look Into"},{"location":"R/Useful%20Packages%20in%20R%20List/#awesome-lists","text":"qinwf/awesome-R: A curated list of awesome R packages, frameworks and software grabear/awesome-rshiny: An awesome R-shiny list!","title":"Awesome Lists"},{"location":"R/Useful%20Packages%20in%20R%20List/#r-universe","text":"R-Universe provides monorepos for R packages and renders them into nice websites. Source Github Repository Website data-cleaning r-universe/data-cleaning data-cleaning.r-universe.dev rOpenSci r-universe/ropensci ropensci.r-universe.dev RStudio r-universe/rstudio rstudio.r-universe.dev Appsilon r-universe/appsilon appsilon.r-universe.dev r-dbi r-universe/r-dbi r-dbi.r-universe.dev JohnCoene r-universe/johncoene johncoene.r-universe.dev HenrikBengtsson r-universe/henrikbengtsson henrikbengtsson.r-universe.dev daattali r-universe/daattali daattali.r-universe.dev eddelbuettel r-universe/eddelbuettel eddelbuettel.r-universe.dev r-forge r-universe/r-forge r-forge.r-universe.dev dreamRs r-universe/dreamrs dreamrs.r-universe.dev MarkEdmondson1234 r-universe/markedmondson1234 markedmondson1234.r-universe.dev jimhester r-universe/jimhester jimhester.r-universe.dev RinteRface r-universe/rinterface rinterface.r-universe.dev ColinFay r-universe/colinfay colinfay.r-universe.dev tidyverse r-universe/tidyverse tidyverse.r-universe.dev r-spatial r-universe/r-spatial r-spatial.r-universe.dev cboettig r-universe/cboettig cboettig.r-universe.dev yihui (Yihui Xie) r-universe/yihui: yihui yihui.r-universe.dev (Various) r-universe/test: test test.r-universe.dev the cloudyr project r-universe/cloudyr cloudyr.r-universe.dev Microsoft Azure r-universe/azure azure.r-universe.dev","title":"R Universe"},{"location":"R/Useful%20Packages%20in%20R%20List/#r-shiny-related","text":"ThinkR-open/golem: A Framework for Building Robust Shiny Apps colinfay/golemexamples tyronehunt/GolemShinyTemplate ColinFay/glouton: 'JS-cookies' in Shiny ColinFay/cordes: Boilerplate for Wrapping Node Modules in R packages r4fun/keys: Keyboard Shortcuts for shiny ColinFay/crrry: 'crrri' recipes for shiny ColinFay/dockerfiler: Easy Dockerfile Creation from R yonicd/whereami: Reliably return location where command is called from in R ColinFay/minifyr: Wrapper around node-minify NodeJS module grabear/shiny-pathfinder-loot: An R shiny app used to keep up with loot tables on Google Sheets during Pathfinder/RPG sessions","title":"R Shiny Related"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/","text":"Utils Package Hidden Gems \u00b6 readClipboard and writeClipboard \u00b6 One of my favorite duo of functions from utils is readCLipboard and writeClipboard . If you\u2019re doing some manipulation to get a quick answer between R and Excel, these functions can come in handy. readClipboard reads in whatever is currently on the Clipboard. For example, let\u2019s copy a column of cells from Excel. We can now run readClipboard() in R. The result of running this command is a vector containing the column of cells we just copied. Each cell corresponds to an element in the vector. Similarly, if we want to write a vector of elements to the clipboard, we can the writeClipboard command: test <- c ( \"write\" , \"to\" , \"clipboard\" ) writeClipboard ( test ) Now, the vector test has been copied to the clipboard. If you paste the result in Excel, you\u2019ll see a column of cells corresponding to the vector you just copied. combn \u00b6 The combn function is useful for getting the possible combinations of an input vector. For instance, let\u2019s say we want to get all of the possible 2-element combinations of a vector, we could do this: food <- c ( \"apple\" , \"grape\" , \"orange\" , \"pear\" , \"peach\" , \"banana\" ) combn ( food , 2 ) In general, the first parameter of combn is the vector of elements you want to get possible combinations from. The second parameter is the number of elements you want in each combination. So if you need to get all possible 3-element or 4-element combinations, you would just need to change this number to three or four. combn(food, 3) combn(food, 4) We can also add a parameter called simplify to make the function return a list of each combination, rather than giving back a matrix output like above. combn(food, 3, simplify = FALSE) fileSnapshot \u00b6 The fileSnapshot function is one R\u2019s collection of file manipulation functions. To learn more about file manipulation and getting information on files in R, check out this post . fileSnapshot will list and provide details about the files in a directory. This function returns a list of objects. # get file snapshot of current directory snapshot <- fileSnapshot () # or file snapshot of another directory snapshot <- fileSnapshot ( \"C:/some/other/directory\" ) fileSnapshot returns a list, which here we will just call \u201csnapshot\u201d. The most useful piece of information can be garnered from this by referencing \u201cinfo\u201d: snapshot$info Here, snapshot$info is a data frame showing information about the files in the input folder parameter. Its headers include: size ==> size of file isdir ==> is file a directory? ==> TRUE or FALSE mode ==> the file permissions in octal mtime ==> last modified time stamp ctime ==> time stamp created atime ==> time stamp last accessed exe ==> type of executable (or \u201cno\u201d if not an executable) download.file \u00b6 download.file does just what it sounds like \u2013 downloads a file from the internet to the destination provided in the function\u2019s input. The first parameter is the URL of the file you wish to download. The second parameter is the name you want to give to the downloaded file. Below, we download a file and call it \u201ccensus_data.csv\u201d. download . file ( \"https://www2.census.gov/programs-surveys/popest/datasets/2010/2010-eval-estimates/cc-est2010-alldata.csv\" , \"census_data.csv\" ) fix - modify an object on the fly \u00b6 The utils package also has the ability to modify objects on the fly with the fix function. For instance, let\u2019s say you define a function interactively, and you want to make some modification. some_func <- function ( num ) { 3 * num + 1 } Now, let\u2019s modify the function with fix : fix(some_func) : When you call fix , it comes up with an editor allowing you to modify the definition of the function. You can also call fix to modify a vector or data frame. fix(iris)","title":"Utils Package Hidden Gems"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/#utils-package-hidden-gems","text":"","title":"Utils Package Hidden Gems"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/#readclipboard-andwriteclipboard","text":"One of my favorite duo of functions from utils is readCLipboard and writeClipboard . If you\u2019re doing some manipulation to get a quick answer between R and Excel, these functions can come in handy. readClipboard reads in whatever is currently on the Clipboard. For example, let\u2019s copy a column of cells from Excel. We can now run readClipboard() in R. The result of running this command is a vector containing the column of cells we just copied. Each cell corresponds to an element in the vector. Similarly, if we want to write a vector of elements to the clipboard, we can the writeClipboard command: test <- c ( \"write\" , \"to\" , \"clipboard\" ) writeClipboard ( test ) Now, the vector test has been copied to the clipboard. If you paste the result in Excel, you\u2019ll see a column of cells corresponding to the vector you just copied.","title":"readClipboard andwriteClipboard"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/#combn","text":"The combn function is useful for getting the possible combinations of an input vector. For instance, let\u2019s say we want to get all of the possible 2-element combinations of a vector, we could do this: food <- c ( \"apple\" , \"grape\" , \"orange\" , \"pear\" , \"peach\" , \"banana\" ) combn ( food , 2 ) In general, the first parameter of combn is the vector of elements you want to get possible combinations from. The second parameter is the number of elements you want in each combination. So if you need to get all possible 3-element or 4-element combinations, you would just need to change this number to three or four. combn(food, 3) combn(food, 4) We can also add a parameter called simplify to make the function return a list of each combination, rather than giving back a matrix output like above. combn(food, 3, simplify = FALSE)","title":"combn"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/#filesnapshot","text":"The fileSnapshot function is one R\u2019s collection of file manipulation functions. To learn more about file manipulation and getting information on files in R, check out this post . fileSnapshot will list and provide details about the files in a directory. This function returns a list of objects. # get file snapshot of current directory snapshot <- fileSnapshot () # or file snapshot of another directory snapshot <- fileSnapshot ( \"C:/some/other/directory\" ) fileSnapshot returns a list, which here we will just call \u201csnapshot\u201d. The most useful piece of information can be garnered from this by referencing \u201cinfo\u201d: snapshot$info Here, snapshot$info is a data frame showing information about the files in the input folder parameter. Its headers include: size ==> size of file isdir ==> is file a directory? ==> TRUE or FALSE mode ==> the file permissions in octal mtime ==> last modified time stamp ctime ==> time stamp created atime ==> time stamp last accessed exe ==> type of executable (or \u201cno\u201d if not an executable)","title":"fileSnapshot"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/#downloadfile","text":"download.file does just what it sounds like \u2013 downloads a file from the internet to the destination provided in the function\u2019s input. The first parameter is the URL of the file you wish to download. The second parameter is the name you want to give to the downloaded file. Below, we download a file and call it \u201ccensus_data.csv\u201d. download . file ( \"https://www2.census.gov/programs-surveys/popest/datasets/2010/2010-eval-estimates/cc-est2010-alldata.csv\" , \"census_data.csv\" )","title":"download.file"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/#fix-modify-an-object-on-the-fly","text":"The utils package also has the ability to modify objects on the fly with the fix function. For instance, let\u2019s say you define a function interactively, and you want to make some modification. some_func <- function ( num ) { 3 * num + 1 } Now, let\u2019s modify the function with fix : fix(some_func) : When you call fix , it comes up with an editor allowing you to modify the definition of the function. You can also call fix to modify a vector or data frame. fix(iris)","title":"fix - modify an object on the fly"},{"location":"System%20Design/System%20Design%20Primer/","text":"System Design Primer \u00b6 Performance vs scalability \u00b6 A service is scalable if it results in increased performance in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow. 1 Another way to look at performance vs scalability: If you have a performance problem, your system is slow for a single user. If you have a scalability problem, your system is fast for a single user but slow under heavy load. Source(s) and further reading \u00b6 A word on scalability Scalability, availability, stability, patterns Latency vs throughput \u00b6 Latency is the time to perform some action or to produce some result. Throughput is the number of such actions or results per unit of time. Generally, you should aim for maximal throughput with acceptable latency . Source(s) and further reading \u00b6 Availability vs consistency \u00b6 CAP theorem \u00b6 Source: CAP theorem revisited In a distributed computer system, you can only support two of the following guarantees: Consistency - Every read receives the most recent write or an error Availability - Every request receives a response, without guarantee that it contains the most recent version of the information Partition Tolerance - The system continues to operate despite arbitrary partitioning due to network failures Networks aren't reliable, so you'll need to support partition tolerance. You'll need to make a software tradeoff between consistency and availability. CP - consistency and partition tolerance \u00b6 Waiting for a response from the partitioned node might result in a timeout error. CP is a good choice if your business needs require atomic reads and writes. AP - availability and partition tolerance \u00b6 Responses return the most readily available version of the data available on any node, which might not be the latest. Writes might take some time to propagate when the partition is resolved. AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors. Source(s) and further reading \u00b6 CAP theorem revisited A plain english introduction to CAP theorem CAP FAQ The CAP theorem Understanding latency vs throughput Links: System Design | [[Web Development]] | Databases Source: donnemartin/system-design-primer","title":"System Design Primer"},{"location":"System%20Design/System%20Design%20Primer/#system-design-primer","text":"","title":"System Design Primer"},{"location":"System%20Design/System%20Design%20Primer/#performance-vs-scalability","text":"A service is scalable if it results in increased performance in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow. 1 Another way to look at performance vs scalability: If you have a performance problem, your system is slow for a single user. If you have a scalability problem, your system is fast for a single user but slow under heavy load.","title":"Performance vs scalability"},{"location":"System%20Design/System%20Design%20Primer/#sources-and-further-reading","text":"A word on scalability Scalability, availability, stability, patterns","title":"Source(s) and further reading"},{"location":"System%20Design/System%20Design%20Primer/#latency-vs-throughput","text":"Latency is the time to perform some action or to produce some result. Throughput is the number of such actions or results per unit of time. Generally, you should aim for maximal throughput with acceptable latency .","title":"Latency vs throughput"},{"location":"System%20Design/System%20Design%20Primer/#sources-and-further-reading_1","text":"","title":"Source(s) and further reading"},{"location":"System%20Design/System%20Design%20Primer/#availability-vs-consistency","text":"","title":"Availability vs consistency"},{"location":"System%20Design/System%20Design%20Primer/#cap-theorem","text":"Source: CAP theorem revisited In a distributed computer system, you can only support two of the following guarantees: Consistency - Every read receives the most recent write or an error Availability - Every request receives a response, without guarantee that it contains the most recent version of the information Partition Tolerance - The system continues to operate despite arbitrary partitioning due to network failures Networks aren't reliable, so you'll need to support partition tolerance. You'll need to make a software tradeoff between consistency and availability.","title":"CAP theorem"},{"location":"System%20Design/System%20Design%20Primer/#cp-consistency-and-partition-tolerance","text":"Waiting for a response from the partitioned node might result in a timeout error. CP is a good choice if your business needs require atomic reads and writes.","title":"CP - consistency and partition tolerance"},{"location":"System%20Design/System%20Design%20Primer/#ap-availability-and-partition-tolerance","text":"Responses return the most readily available version of the data available on any node, which might not be the latest. Writes might take some time to propagate when the partition is resolved. AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors.","title":"AP - availability and partition tolerance"},{"location":"System%20Design/System%20Design%20Primer/#sources-and-further-reading_2","text":"CAP theorem revisited A plain english introduction to CAP theorem CAP FAQ The CAP theorem Understanding latency vs throughput Links: System Design | [[Web Development]] | Databases Source: donnemartin/system-design-primer","title":"Source(s) and further reading"},{"location":"System%20Design/System%20Design/","text":"","title":"System Design"},{"location":"WSL/WSL%20Commands%20and%20Installs/","text":"WSL Commands and Installs \u00b6 New WSLg GUI Applications \u00b6 Install Edge with new WSLg version of WSL that supports GUI applications natively. sudo apt update && sudo apt upgrade sudo curl https://packages.microsoft.com/repos/edge/pool/main/m/microsoft-edge-dev/microsoft-edge-dev_91.0.852.0-1_amd64.deb -o /tmp/edge.deb sudo apt install /tmp/edge.deb \\- y Install RStudio: sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 sudo add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/\" sudo apt update sudo apt install r-base sudo apt-get install gdebi-core wget https://download1.rstudio.org/desktop/bionic/amd64/rstudio-1.2.5042-amd64.deb sudo gdebi rstudio-1.2.5042-amd64.deb Links: Source:","title":"WSL Commands and Installs"},{"location":"WSL/WSL%20Commands%20and%20Installs/#wsl-commands-and-installs","text":"","title":"WSL Commands and Installs"},{"location":"WSL/WSL%20Commands%20and%20Installs/#new-wslg-gui-applications","text":"Install Edge with new WSLg version of WSL that supports GUI applications natively. sudo apt update && sudo apt upgrade sudo curl https://packages.microsoft.com/repos/edge/pool/main/m/microsoft-edge-dev/microsoft-edge-dev_91.0.852.0-1_amd64.deb -o /tmp/edge.deb sudo apt install /tmp/edge.deb \\- y Install RStudio: sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 sudo add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/\" sudo apt update sudo apt install r-base sudo apt-get install gdebi-core wget https://download1.rstudio.org/desktop/bionic/amd64/rstudio-1.2.5042-amd64.deb sudo gdebi rstudio-1.2.5042-amd64.deb Links: Source:","title":"New WSLg GUI Applications"},{"location":"WSL/WSL/","text":"WSL- Contents \u00b6 Contents: \u00b6 Backlinks: Sources:","title":"WSL- Contents"},{"location":"WSL/WSL/#wsl-contents","text":"","title":"WSL- Contents"},{"location":"WSL/WSL/#contents","text":"Backlinks: Sources:","title":"Contents:"},{"location":"Web%20Development/APIs/","text":"APIs \u00b6 Links: System Design Source: An Introduction to APIs | Zapier","title":"APIs"},{"location":"Web%20Development/APIs/#apis","text":"Links: System Design Source: An Introduction to APIs | Zapier","title":"APIs"},{"location":"Web%20Development/Backend%20Web%20Architecture/","text":"Source : https://www.codecademy.com/articles/back-end-architecture Backend Web Architecture \u00b6 Software engineers seem to always be discussing the front-end and the back-end of their apps. But what exactly does this mean? The front-end is the code that is executed on the client side. This code (typically HTML, CSS, and JavaScript) runs in the user\u2019s browser and creates the user interface. The back-end is the code that runs on the server, that receives requests from the clients, and contains the logic to send the appropriate data back to the client. The back-end also includes the database, which will persistently store all of the data for the application. This article focuses on the hardware and software on the server-side that make this possible. Review HTTP and REST if you want to refresh your memory on these topics. These are the main conventions that provide structure to the request-response cycle between clients and servers. Let\u2019s start by reviewing the client-server relationship, and then we can start to put the pieces all together! What are the clients? \u00b6 The clients are anything that send requests to the back-end. They are often browsers that make requests for the HTML and JavaScript code that they will execute to display websites to the end user. However, there many different kinds of clients: they might be a mobile application, an application running on another server, or even a web enabled smart appliance. What is a back-end? \u00b6 The back-end is all of the technology required to process the incoming request and generate and send the response to the client. This typically includes three major parts: The server. This is the computer that receives requests. The app. This is the application running on the server that listens for requests, retrieves information from the database, and sends a response. The database. Databases are used to organize and persist data. What is a server? \u00b6 A server is simply a computer that listens for incoming requests. Though there are machines made and optimized for this particular purpose, any computer that is connected to a network can act as a server. In fact, you will often use your very own computer as server when developing apps. What are the core functions of the app? \u00b6 The server runs an app that contains logic about how to respond to various requests based on the HTTP verb and the Uniform Resource Identifier (URI) . The pair of an HTTP verb and a URI is called a route and matching them based on a request is called routing . Some of these handler functions will be middleware . In this context, middleware is any code that executes between the server receiving a request and sending a response. These middleware functions might modify the request object, query the database, or otherwise process the incoming request. Middleware functions typically end by passing control to the next middleware function, rather than by sending a response. Eventually, a middleware function will be called that ends the request-response cycle by sending an HTTP response back to the client. Often, programmers will use a framework like Express or Ruby on Rails to simplify the logic of routing. For now, just think that each route can have one or many handler functions that are executed whenever a request to that route (HTTP verb and URI) is matched. What kinds of responses can a server send? \u00b6 The data that the server sends back can come in different forms. For example, a server might serve up an HTML file, send data as JSON, or it might send back only an HTTP status code . You\u2019ve probably seen the status code \u201c404 - Not Found\u201d whenever you've tried navigating to a URI that doesn\u2019t exist, but there are many more status codes that indicate what happened when the server received the request. What is a database, and why do we need to use them? \u00b6 Databases are commonly used on the back-end of web applications. These databases provide an interface to save data in a persistent way to memory. Storing the data in a database both reduces the load on the main memory of the server CPU and allows the data to be retrieved if the server crashes or loses power. Many requests sent to the server might require a database query. A client might request information that is stored in the database, or a client might submit data with their request to be added to the database. What is a Web API, really? \u00b6 An API is a collection of clearly defined methods of communication between different software components. More specifically, a Web API is the interface created by the back-end: the collection of endpoints and the resources these endpoints expose. A Web API is defined by the types of requests that it can handle, which is determined by the routes that it defines, and the types of responses that the clients can expect to receive after hitting those routes. One Web API can be used to provide data for different front-ends. Since a Web API can provide data without really specifying how the data is viewed, multiple different HTML pages or mobile applications can be created to view the data from the Web API. Other principles of the request-response cycle: \u00b6 The server typically cannot initiate responses without requests! Every request needs a response, even if it\u2019s just a 404 status code indicating that the content was not found. Otherwise your client will be left hanging (indefinitely waiting). The server should not send more than one response per request. This will throw errors in your code. Mapping out a request \u00b6 Let\u2019s make all of this a bit more concrete, by following an example of the main steps that happen when a client makes a request to the server. Alice is shopping on SuperCoolShop.com. She clicks on a picture of a cover for her smartphone, and that click event makes a GET request to http://www.SuperCoolShop.com/products/66432 . Remember, GET describes the kind of request (the client is just asking for data, not changing anything). The URI (uniform resource identifier) /products/66432 specifies that the client is looking for more information about a product, and that product, has an id of 66432. SuperCoolShop has an huge number of products, and many different categories for filtering through them, so the actual URI would be more complicated than this. But this is the general principle for how requests and resource identifiers work. Alice\u2019s request travels across the internet to one of SuperCoolShop\u2019s servers. This is one of the slower steps in the process, because the request cannot go faster than the speed of light, and it might have a long distance to travel. For this reason, major websites with users all over the world will have many different servers, and they will direct users to the server that is closest to them! The server, which is actively listening for requests from all users, receives Alice\u2019s request! Event listeners that match this request (the HTTP verb: GET, and the URI: /products/66432 ) are triggered. The code that runs on the server between the request and the response is called middleware . In processing the request, the server code makes a database query to get more information about this smartphone case. The database contains all of the other information that Alice wants to know about this smartphone case: the name of the product, the price of the product, a few product reviews, and a string that will provide a path to the image of the product. The database query is executed, and the database sends the requested data back to the server. It\u2019s worth noting that database queries are one of the slower steps in this process. Reading and writing from static memory is fairly slow, and the database might be on a different machine than the original server. This query itself might have to go across the internet! The server receives the data that it needs from the database, and it is now ready to construct and send its response back to the client. This response body has all of the information needed by the browser to show Alice more details (price, reviews, size, etc) about the phone case she\u2019s interested in. The response header will contain an HTTP status code 200 to indicate that the request has succeeded. The response travels across the internet, back to Alice\u2019s computer. Alice\u2019s browser receives the response and uses that information to create and render the view that Alice ultimately sees!","title":"Backend Web Architecture"},{"location":"Web%20Development/Backend%20Web%20Architecture/#backend-web-architecture","text":"Software engineers seem to always be discussing the front-end and the back-end of their apps. But what exactly does this mean? The front-end is the code that is executed on the client side. This code (typically HTML, CSS, and JavaScript) runs in the user\u2019s browser and creates the user interface. The back-end is the code that runs on the server, that receives requests from the clients, and contains the logic to send the appropriate data back to the client. The back-end also includes the database, which will persistently store all of the data for the application. This article focuses on the hardware and software on the server-side that make this possible. Review HTTP and REST if you want to refresh your memory on these topics. These are the main conventions that provide structure to the request-response cycle between clients and servers. Let\u2019s start by reviewing the client-server relationship, and then we can start to put the pieces all together!","title":"Backend Web Architecture"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-are-the-clients","text":"The clients are anything that send requests to the back-end. They are often browsers that make requests for the HTML and JavaScript code that they will execute to display websites to the end user. However, there many different kinds of clients: they might be a mobile application, an application running on another server, or even a web enabled smart appliance.","title":"What are the clients?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-is-a-back-end","text":"The back-end is all of the technology required to process the incoming request and generate and send the response to the client. This typically includes three major parts: The server. This is the computer that receives requests. The app. This is the application running on the server that listens for requests, retrieves information from the database, and sends a response. The database. Databases are used to organize and persist data.","title":"What is a back-end?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-is-a-server","text":"A server is simply a computer that listens for incoming requests. Though there are machines made and optimized for this particular purpose, any computer that is connected to a network can act as a server. In fact, you will often use your very own computer as server when developing apps.","title":"What is a server?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-are-the-core-functions-of-the-app","text":"The server runs an app that contains logic about how to respond to various requests based on the HTTP verb and the Uniform Resource Identifier (URI) . The pair of an HTTP verb and a URI is called a route and matching them based on a request is called routing . Some of these handler functions will be middleware . In this context, middleware is any code that executes between the server receiving a request and sending a response. These middleware functions might modify the request object, query the database, or otherwise process the incoming request. Middleware functions typically end by passing control to the next middleware function, rather than by sending a response. Eventually, a middleware function will be called that ends the request-response cycle by sending an HTTP response back to the client. Often, programmers will use a framework like Express or Ruby on Rails to simplify the logic of routing. For now, just think that each route can have one or many handler functions that are executed whenever a request to that route (HTTP verb and URI) is matched.","title":"What are the core functions of the app?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-kinds-of-responses-can-a-server-send","text":"The data that the server sends back can come in different forms. For example, a server might serve up an HTML file, send data as JSON, or it might send back only an HTTP status code . You\u2019ve probably seen the status code \u201c404 - Not Found\u201d whenever you've tried navigating to a URI that doesn\u2019t exist, but there are many more status codes that indicate what happened when the server received the request.","title":"What kinds of responses can a server send?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-is-a-database-and-why-do-we-need-to-use-them","text":"Databases are commonly used on the back-end of web applications. These databases provide an interface to save data in a persistent way to memory. Storing the data in a database both reduces the load on the main memory of the server CPU and allows the data to be retrieved if the server crashes or loses power. Many requests sent to the server might require a database query. A client might request information that is stored in the database, or a client might submit data with their request to be added to the database.","title":"What is a database, and why do we need to use them?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-is-a-web-api-really","text":"An API is a collection of clearly defined methods of communication between different software components. More specifically, a Web API is the interface created by the back-end: the collection of endpoints and the resources these endpoints expose. A Web API is defined by the types of requests that it can handle, which is determined by the routes that it defines, and the types of responses that the clients can expect to receive after hitting those routes. One Web API can be used to provide data for different front-ends. Since a Web API can provide data without really specifying how the data is viewed, multiple different HTML pages or mobile applications can be created to view the data from the Web API.","title":"What is a Web API, really?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#other-principles-of-the-request-response-cycle","text":"The server typically cannot initiate responses without requests! Every request needs a response, even if it\u2019s just a 404 status code indicating that the content was not found. Otherwise your client will be left hanging (indefinitely waiting). The server should not send more than one response per request. This will throw errors in your code.","title":"Other principles of the request-response cycle:"},{"location":"Web%20Development/Backend%20Web%20Architecture/#mapping-out-a-request","text":"Let\u2019s make all of this a bit more concrete, by following an example of the main steps that happen when a client makes a request to the server. Alice is shopping on SuperCoolShop.com. She clicks on a picture of a cover for her smartphone, and that click event makes a GET request to http://www.SuperCoolShop.com/products/66432 . Remember, GET describes the kind of request (the client is just asking for data, not changing anything). The URI (uniform resource identifier) /products/66432 specifies that the client is looking for more information about a product, and that product, has an id of 66432. SuperCoolShop has an huge number of products, and many different categories for filtering through them, so the actual URI would be more complicated than this. But this is the general principle for how requests and resource identifiers work. Alice\u2019s request travels across the internet to one of SuperCoolShop\u2019s servers. This is one of the slower steps in the process, because the request cannot go faster than the speed of light, and it might have a long distance to travel. For this reason, major websites with users all over the world will have many different servers, and they will direct users to the server that is closest to them! The server, which is actively listening for requests from all users, receives Alice\u2019s request! Event listeners that match this request (the HTTP verb: GET, and the URI: /products/66432 ) are triggered. The code that runs on the server between the request and the response is called middleware . In processing the request, the server code makes a database query to get more information about this smartphone case. The database contains all of the other information that Alice wants to know about this smartphone case: the name of the product, the price of the product, a few product reviews, and a string that will provide a path to the image of the product. The database query is executed, and the database sends the requested data back to the server. It\u2019s worth noting that database queries are one of the slower steps in this process. Reading and writing from static memory is fairly slow, and the database might be on a different machine than the original server. This query itself might have to go across the internet! The server receives the data that it needs from the database, and it is now ready to construct and send its response back to the client. This response body has all of the information needed by the browser to show Alice more details (price, reviews, size, etc) about the phone case she\u2019s interested in. The response header will contain an HTTP status code 200 to indicate that the request has succeeded. The response travels across the internet, back to Alice\u2019s computer. Alice\u2019s browser receives the response and uses that information to create and render the view that Alice ultimately sees!","title":"Mapping out a request"},{"location":"Web%20Development/HTTP%20Requests%20Notes/","text":"HTTP Requests | Codecademy \u00b6 Understand the basics of how your web browser communicates with the internet. Background: \u00b6 This page is generated by a web of HTML, CSS, and Javascript, sent to you by Codecademy via the internet. The internet is made up of a bunch of resources hosted on different servers. The term \u201cresource\u201d corresponds to any entity on the web, including HTML files, stylesheets, images, videos, and scripts. To access content on the internet, your browser must ask these servers for the resources it wants, and then display these resources to you. This protocol of requests and responses enables you view this page in your browser. This article focuses on one fundamental part of how the internet functions: HTTP. What is HTTP? \u00b6 HTTP stands for Hypertext Transfer Protocol and is used to structure requests and responses over the internet. HTTP requires data to be transferred from one point to another over the network. The transfer of resources happens using TCP (Transmission Control Protocol). In viewing this webpage, TCP manages the channels between your browser and the server (in this case, codecademy.com). TCP is used to manage many types of internet connections in which one computer or device wants to send something to another. HTTP is the command language that the devices on both sides of the connection must follow in order to communicate. HTTP & TCP: How it Works \u00b6 When you type an address such as www.codecademy.com into your browser, you are commanding it to open a TCP channel to the server that responds to that URL (or Uniform Resource Locator, which you can read more about on Wikipedia ). A URL is like your home address or phone number because it describes how to reach you. In this situation, your computer, which is making the request, is called the client. The URL you are requesting is the address that belongs to the server. Once the TCP connection is established, the client sends a HTTP GET request to the server to retrieve the webpage it should display. After the server has sent the response, it closes the TCP connection. If you open the website in your browser again, or if your browser automatically requests something from the server, a new connection is opened which follows the same process described above. GET requests are one kind of HTTP method a client can call. You can learn more about the other common ones ( POST , PUT and DELETE ) in this article . Let\u2019s explore an example of how GET requests (the most common type of request) are used to help your computer (the client) access resources on the web. Suppose you want to check out the latest course offerings from http://codecademy.com . After you type the URL into your browser, your browser will extract the http part and recognize that it is the name of the network protocol to use. Then, it takes the domain name from the URL, in this case \u201ccodecademy.com\u201d, and asks the internet Domain Name Server to return an Internet Protocol (IP) address. Now the client knows the destination\u2019s IP address. It then opens a connection to the server at that address, using the http protocol as specified. It will initiate a GET request to the server which contains the IP address of the host and optionally a data payload. The GET request contains the following text: GET / HTTP/1.1Host: www.codecademy.com This identifies the type of request, the path on www.codecademy.com (in this case, \u201c/\u201c) and the protocol \u201cHTTP/1.1.\u201d HTTP/1.1 is a revision of the first HTTP, which is now called HTTP/1.0. In HTTP/1.0, every resource request requires a separate connection to the server. HTTP/1.1 uses one connection more than once, so that additional content (like images or stylesheets) is retrieved even after the page has been retrieved. As a result, requests using HTTP/1.1 have less delay than those using HTTP/1.0. The second line of the request contains the address of the server which is \"www.codecademy.com\" . There may be additional lines as well depending on what data your browser chooses to send. If the server is able to locate the path requested, the server might respond with the header: HTTP/1.1 200 OKContent-Type: text/html This header is followed by the content requested, which in this case is the information needed to render www.codecademy.com . The first line of the header, HTTP/1.1 200 OK , is confirmation that the server understands the protocol that the client wants to communicate with ( HTTP/1.1 ), and an HTTP status code signifying that the resource was found on the server. The third line, Content-Type: text/html , shows the type of content that it will be sending to the client. If the server is not able to locate the path requested by the client, it will respond with the header: HTTP/1.1 404 NOT FOUND In this case, the server identifies that it understands the HTTP protocol, but the 404 NOT FOUND status code signifies that the specific piece of content requested was not found. This might happen if the content was moved or if you typed in the URL path incorrectly or if the page was removed. You can read more about the 404 status code, commonly called a 404 error, here . An Analogy: \u00b6 It can be tricky to understand how HTTP functions because it\u2019s difficult to examine what your browser is actually doing. (And perhaps also because we explained it using acronyms that may be new to you.) Let\u2019s review what we learned by using an analogy that could be more familiar to you. Imagine the internet is a town. You are a client and your address determines where you can be reached. Businesses in town, such as Codecademy.com, serve requests that are sent to them. The other houses are filled with other clients like you that are making requests and expecting responses from these businesses in town. This town also has a crazy fast mail service, an army of mail delivery staff that can travel on trains that move at the speed of light. Suppose you want to read the morning newspaper. In order to retrieve it, you write down what you need in a language called HTTP and ask your local mail delivery staff agent to retrieve it from a specific business. The mail delivery person agrees and builds a railroad track (connection) between your house and the business nearly instantly, and rides the train car labeled \u201cTCP\u201d to the address of the business you provided. Upon arriving at the business, she asks the first of several free employees ready to fulfill the request. The employee searches for the page of the newspaper that you requested but cannot find it and communicates that back to the mail delivery person. The mail delivery person returns on the light speed train, ripping up the tracks on the way back, and tells you that there was a problem \u201c404 Not Found.\u201d After you check the spelling of what you had written, you realize that you misspelled the newspaper title. You correct it and provide the corrected title to the mail delivery person. This time the mail delivery person is able to retrieve it from the business. You can now read your newspaper in peace until you decide you want to read the next page, at which point, you would make another request and give it to the mail delivery person. What is HTTPS? \u00b6 Since your HTTP request can be read by anyone at certain network junctures, it might not be a good idea to deliver information such as your credit card or password using this protocol. Fortunately, many servers support HTTPS, short for HTTP Secure, which allows you to encrypt data that you send and receive. You can read more about HTTPS on Wikipedia . HTTPS is important to use when passing sensitive or personal information to and from websites. However, it is up to the businesses maintaining the servers to set it up. In order to support HTTPS, the business must apply for a certificate from a Certificate Authority . Source","title":"HTTP Requests | Codecademy"},{"location":"Web%20Development/HTTP%20Requests%20Notes/#http-requests-codecademy","text":"Understand the basics of how your web browser communicates with the internet.","title":"HTTP Requests | Codecademy"},{"location":"Web%20Development/HTTP%20Requests%20Notes/#background","text":"This page is generated by a web of HTML, CSS, and Javascript, sent to you by Codecademy via the internet. The internet is made up of a bunch of resources hosted on different servers. The term \u201cresource\u201d corresponds to any entity on the web, including HTML files, stylesheets, images, videos, and scripts. To access content on the internet, your browser must ask these servers for the resources it wants, and then display these resources to you. This protocol of requests and responses enables you view this page in your browser. This article focuses on one fundamental part of how the internet functions: HTTP.","title":"Background:"},{"location":"Web%20Development/HTTP%20Requests%20Notes/#what-is-http","text":"HTTP stands for Hypertext Transfer Protocol and is used to structure requests and responses over the internet. HTTP requires data to be transferred from one point to another over the network. The transfer of resources happens using TCP (Transmission Control Protocol). In viewing this webpage, TCP manages the channels between your browser and the server (in this case, codecademy.com). TCP is used to manage many types of internet connections in which one computer or device wants to send something to another. HTTP is the command language that the devices on both sides of the connection must follow in order to communicate.","title":"What is HTTP?"},{"location":"Web%20Development/HTTP%20Requests%20Notes/#http-tcp-how-it-works","text":"When you type an address such as www.codecademy.com into your browser, you are commanding it to open a TCP channel to the server that responds to that URL (or Uniform Resource Locator, which you can read more about on Wikipedia ). A URL is like your home address or phone number because it describes how to reach you. In this situation, your computer, which is making the request, is called the client. The URL you are requesting is the address that belongs to the server. Once the TCP connection is established, the client sends a HTTP GET request to the server to retrieve the webpage it should display. After the server has sent the response, it closes the TCP connection. If you open the website in your browser again, or if your browser automatically requests something from the server, a new connection is opened which follows the same process described above. GET requests are one kind of HTTP method a client can call. You can learn more about the other common ones ( POST , PUT and DELETE ) in this article . Let\u2019s explore an example of how GET requests (the most common type of request) are used to help your computer (the client) access resources on the web. Suppose you want to check out the latest course offerings from http://codecademy.com . After you type the URL into your browser, your browser will extract the http part and recognize that it is the name of the network protocol to use. Then, it takes the domain name from the URL, in this case \u201ccodecademy.com\u201d, and asks the internet Domain Name Server to return an Internet Protocol (IP) address. Now the client knows the destination\u2019s IP address. It then opens a connection to the server at that address, using the http protocol as specified. It will initiate a GET request to the server which contains the IP address of the host and optionally a data payload. The GET request contains the following text: GET / HTTP/1.1Host: www.codecademy.com This identifies the type of request, the path on www.codecademy.com (in this case, \u201c/\u201c) and the protocol \u201cHTTP/1.1.\u201d HTTP/1.1 is a revision of the first HTTP, which is now called HTTP/1.0. In HTTP/1.0, every resource request requires a separate connection to the server. HTTP/1.1 uses one connection more than once, so that additional content (like images or stylesheets) is retrieved even after the page has been retrieved. As a result, requests using HTTP/1.1 have less delay than those using HTTP/1.0. The second line of the request contains the address of the server which is \"www.codecademy.com\" . There may be additional lines as well depending on what data your browser chooses to send. If the server is able to locate the path requested, the server might respond with the header: HTTP/1.1 200 OKContent-Type: text/html This header is followed by the content requested, which in this case is the information needed to render www.codecademy.com . The first line of the header, HTTP/1.1 200 OK , is confirmation that the server understands the protocol that the client wants to communicate with ( HTTP/1.1 ), and an HTTP status code signifying that the resource was found on the server. The third line, Content-Type: text/html , shows the type of content that it will be sending to the client. If the server is not able to locate the path requested by the client, it will respond with the header: HTTP/1.1 404 NOT FOUND In this case, the server identifies that it understands the HTTP protocol, but the 404 NOT FOUND status code signifies that the specific piece of content requested was not found. This might happen if the content was moved or if you typed in the URL path incorrectly or if the page was removed. You can read more about the 404 status code, commonly called a 404 error, here .","title":"HTTP &amp; TCP: How it Works"},{"location":"Web%20Development/HTTP%20Requests%20Notes/#an-analogy","text":"It can be tricky to understand how HTTP functions because it\u2019s difficult to examine what your browser is actually doing. (And perhaps also because we explained it using acronyms that may be new to you.) Let\u2019s review what we learned by using an analogy that could be more familiar to you. Imagine the internet is a town. You are a client and your address determines where you can be reached. Businesses in town, such as Codecademy.com, serve requests that are sent to them. The other houses are filled with other clients like you that are making requests and expecting responses from these businesses in town. This town also has a crazy fast mail service, an army of mail delivery staff that can travel on trains that move at the speed of light. Suppose you want to read the morning newspaper. In order to retrieve it, you write down what you need in a language called HTTP and ask your local mail delivery staff agent to retrieve it from a specific business. The mail delivery person agrees and builds a railroad track (connection) between your house and the business nearly instantly, and rides the train car labeled \u201cTCP\u201d to the address of the business you provided. Upon arriving at the business, she asks the first of several free employees ready to fulfill the request. The employee searches for the page of the newspaper that you requested but cannot find it and communicates that back to the mail delivery person. The mail delivery person returns on the light speed train, ripping up the tracks on the way back, and tells you that there was a problem \u201c404 Not Found.\u201d After you check the spelling of what you had written, you realize that you misspelled the newspaper title. You correct it and provide the corrected title to the mail delivery person. This time the mail delivery person is able to retrieve it from the business. You can now read your newspaper in peace until you decide you want to read the next page, at which point, you would make another request and give it to the mail delivery person.","title":"An Analogy:"},{"location":"Web%20Development/HTTP%20Requests%20Notes/#what-is-https","text":"Since your HTTP request can be read by anyone at certain network junctures, it might not be a good idea to deliver information such as your credit card or password using this protocol. Fortunately, many servers support HTTPS, short for HTTP Secure, which allows you to encrypt data that you send and receive. You can read more about HTTPS on Wikipedia . HTTPS is important to use when passing sensitive or personal information to and from websites. However, it is up to the businesses maintaining the servers to set it up. In order to support HTTPS, the business must apply for a certificate from a Certificate Authority . Source","title":"What is HTTPS?"},{"location":"Web%20Development/Javascript/","text":"Javascript \u00b6 getElementById < html > < body > < h2 > JavaScript in Body </ h2 > < p id = \"demo\" ></ p > //we can use class selctor also < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; //we can use onClick() funcion also </ script > </ body > </ html > Display properties in javascript JavaScript can \"display\" data in different ways: Writing into an HTML element, using innerHTML. Writing into the HTML output using document.write(). Writing into an alert box, using window.alert(). Writing into the browser console, using console.log(). we can write this code in","title":"Javascript"},{"location":"Web%20Development/Javascript/#javascript","text":"getElementById < html > < body > < h2 > JavaScript in Body </ h2 > < p id = \"demo\" ></ p > //we can use class selctor also < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; //we can use onClick() funcion also </ script > </ body > </ html > Display properties in javascript JavaScript can \"display\" data in different ways: Writing into an HTML element, using innerHTML. Writing into the HTML output using document.write(). Writing into an alert box, using window.alert(). Writing into the browser console, using console.log(). we can write this code in","title":"Javascript"},{"location":"Web%20Development/WebDev%20Resource%20List/","text":"WebDev Resource List \u00b6 - MDN Web Docs \u2b50 \u00b6 Links: Source:","title":"WebDev Resource List"},{"location":"Web%20Development/WebDev%20Resource%20List/#webdev-resource-list","text":"","title":"WebDev Resource List"},{"location":"Web%20Development/WebDev%20Resource%20List/#-mdn-web-docs","text":"Links: Source:","title":"- MDN Web Docs \u2b50"},{"location":"Windows/How%20to%20Cleanup%20Windows%20from%20Command%20Line/","text":"A clean and tidy computer is hard to maintain over time on windows, and therefore one must be tediously maintaining their system for optimal performance. This means running a scan for malware, cleaning your hard drive using cleanmgr and sfc /scannow , uninstalling programs that you no longer need, checking for autostart programs (using msconfig ) and enabling Windows' Automatic Update . Always remember to perform periodic backups, or at least to set restore points. Should you experience an actual problem, try to recall the last thing you did, or the last thing you installed before the problem appeared for the first time. Use the resmon command to identify the processes that are causing your problem. Even for serious problems, rather than reinstalling Windows, you are better off repairing of your installation or, for Windows 8 and later versions, executing the DISM.exe /Online /Cleanup-image /Restorehealth command. This allows you to repair the operating system without losing data. To help you analyze the duet.exe process on your computer, the following programs have proven to be helpful: Security Task Manager displays all running Windows tasks, including embedded hidden processes, such as keyboard and browser monitoring or autostart entries. A unique security risk rating indicates the likelihood of the process being potential spyware, malware or a Trojan. Malwarebytes Anti-Malware detects and removes sleeping spyware, adware, Trojans, keyloggers, malware and trackers from your hard drive. Reference: \u00b6 Utilities: cleanmgr or the Disk Cleanup (run as Admin for full system cleanup) uninstall tool or geek.exe msconfig sysinternals suite of executables Security Task Manager Malwarebytes Commands: wuauclt /ShowWindowsUpdate resmon msconfig cleanmgr sfc /scannow DISM.exe /Online /Cleanup-image /Restorehealth","title":"How to Cleanup Windows from Command Line"},{"location":"Windows/How%20to%20Cleanup%20Windows%20from%20Command%20Line/#reference","text":"Utilities: cleanmgr or the Disk Cleanup (run as Admin for full system cleanup) uninstall tool or geek.exe msconfig sysinternals suite of executables Security Task Manager Malwarebytes Commands: wuauclt /ShowWindowsUpdate resmon msconfig cleanmgr sfc /scannow DISM.exe /Online /Cleanup-image /Restorehealth","title":"Reference:"},{"location":"Windows/SFC%20and%20DISM%20Commands/","text":"SFC Scannow and DISM Commands \u00b6 On Windows 10, when you start noticing random errors, problems booting up, or features not working as expected, there's a good chance that one or multiple system files might have gone missing or corrupted for unknown reasons. Usually, problems with system files could occur as a result of an issue installing a system update, driver, or application, or while making changes to the installation manually. If you happen to come across this issue, you can use the Windows 10 System File Checker (SFC), which is a command-line tool designed to scan the integrity and restore missing or corrupted system files with working replacements. Use the System File Checker tool to repair damaged system files automatically or manually if the tool refuses to work. On windows these commands are crucial for keeping a clean system. I run these commands so often that I made a .reg registry entry script to add them to my Desktop\u2019s Context Menu (see below for details or visit my dotfiles repo for the actual .reg scripts to run: The System File Checker - sfc /scannow The Deployment Image Servicing and Management tool - DISM.exe NOTE: these must be ran as an ADMINISTRATOR # powershell or CMD as ADMIN # system file checker sfc / scannow # DISM Dism / Online / Cleanup-Image / ScanHealth Dism / Online / Cleanup-Image / CheckHealth Dism / Online / Cleanup-Image / RestoreHealth SFC \u00b6 Visit Microsoft's Docs on SFC - Link The sfc /scannow command is a well known way to do an integrity check of all Windows 10 system files. sfc.exe is the System File Checker tool which can be helpful in many scenarios and fix various issues with Windows 10. You can save you time by adding a special context menu entry to launch it directly with one click. Add SFC /Scannow to Desktop Context Menu: \u00b6 Registry Script: Windows Registry Editor Version 5.00 [ HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\SFCScannow] \"Icon\" = \"cmd.exe\" \"MUIVerb\" = \"SFC /Scannow\" \"Position\" = \"Bottom\" \"SubCommands\" = \"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\01Scannow] \"HasLUAShield\" = \"\" \"MUIVerb\" = \"Run SFC /Scannow\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\01Scannow\\command] @ = \"PowerShell -windowstyle hidden -command \\\"Start-Process cmd -ArgumentList '/s,/k, sfc.exe /scannow' -Verb runAs\\\"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\02ViewLog] \"MUIVerb\" = \"View log for SFC\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\02ViewLog\\command] @ = \"PowerShell (Select-String [SR] $env:windir\\\\Logs\\\\CBS\\\\CBS.log -s).Line >\\\"$env:userprofile\\\\Desktop\\\\SFC_LOG.txt\\\"\" DISM \u00b6 Visit Microsoft's Official Docs on DISM - Link How to Fix Windows 10 using DISM (winaero.com) Deployment Image Servicing and Management (DISM.exe) is a command-line tool that can be used to service and prepare Windows images, including those used for Windows PE, Windows Recovery Environment (Windows RE) and Windows Setup. DISM can be used to service a Windows image (.wim) or a virtual hard disk (.vhd or.vhdx). The Component store is a core feature of Windows 10 which stores all of the files related to the OS grouped by components and as hardlinks. With new servicing model introduced in Vista, some system files are shared between two components and they are all hardlinked to the system32 folder. When the OS is serviced, the component store is updated. The Component Store is part of the Windows Imaging and Servicing stack. There is a special console tool called DISM which ships with Windows 10 by default. It can be used to fix Windows Component Store corruption. It is especially useful when the usual command \" sfc /scannow \" cannot repair damaged system files. The DISM tool writes the following log files: C:\\Windows\\Logs\\CBS\\CBS.log C:\\Windows\\Logs\\DISM\\dism.log They can be used to analyze errors and see completed operations. Add DISM to Desktop Context Menu: \u00b6 Registry Script: Windows Registry Editor Version 5.00 [ HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\DismContextMenu] \"Icon\" = \"WmiPrvSE.exe\" \"MUIVerb\" = \"Repair Windows Image\" \"Position\" = \"Bottom\" \"SubCommands\" = \"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\CheckHealth] \"HasLUAShield\" = \"\" \"MUIVerb\" = \"Check Health of Windows Image\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\CheckHealth\\command] @ = \"PowerShell -windowstyle hidden -command \\\"Start-Process cmd -ArgumentList '/s,/k, Dism /Online /Cleanup-Image /CheckHealth' -Verb runAs\\\"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\RestoreHealth] \"HasLUAShield\" = \"\" \"MUIVerb\" = \"Repair Windows Image\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\RestoreHealth\\command] @ = \"PowerShell -windowstyle hidden -command \\\"Start-Process cmd -ArgumentList '/s,/k, Dism /Online /Cleanup-Image /RestoreHealth' -Verb runAs\\\"\" How it Works \u00b6 Check Health of Windows Image. This command executes DISM as follows: Dism /Online /Cleanup-Image /CheckHealth . The key option here is CheckHealth. We use it to check if some process has marked the Component Store as corrupted and whether the corruption is repairable. This command is not supposed to fix any issues. It only reports about problems if they are present and if the CBS store is flagged. This command doesn't create a log file. Repair Windows Image . This command starts DISM with the following arguments: Dism /Online /Cleanup-Image /RestoreHealth . The DISM tool started with the /RestoreHealth option will scan the component store for corruption and perform the required repair operations automatically. It will create a log file. The whole process can take several hours, so be patient. On hard drives, it will take longer compared to an SSD. Both commands start elevated from PowerShell . Removing Registry Edits \u00b6 To undo the Context Menu REGEDIT entries here\u2019s the scripts: Remove SFC Context Menu: Windows Registry Editor Version 5.00 [ - HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\SFCScannow] Remove DISM Context Menu: Windows Registry Editor Version 5.00 [ - HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\DismContextMenu] JIMMY BRIGGS - 2021","title":"SFC Scannow and DISM Commands"},{"location":"Windows/SFC%20and%20DISM%20Commands/#sfc-scannow-and-dism-commands","text":"On Windows 10, when you start noticing random errors, problems booting up, or features not working as expected, there's a good chance that one or multiple system files might have gone missing or corrupted for unknown reasons. Usually, problems with system files could occur as a result of an issue installing a system update, driver, or application, or while making changes to the installation manually. If you happen to come across this issue, you can use the Windows 10 System File Checker (SFC), which is a command-line tool designed to scan the integrity and restore missing or corrupted system files with working replacements. Use the System File Checker tool to repair damaged system files automatically or manually if the tool refuses to work. On windows these commands are crucial for keeping a clean system. I run these commands so often that I made a .reg registry entry script to add them to my Desktop\u2019s Context Menu (see below for details or visit my dotfiles repo for the actual .reg scripts to run: The System File Checker - sfc /scannow The Deployment Image Servicing and Management tool - DISM.exe NOTE: these must be ran as an ADMINISTRATOR # powershell or CMD as ADMIN # system file checker sfc / scannow # DISM Dism / Online / Cleanup-Image / ScanHealth Dism / Online / Cleanup-Image / CheckHealth Dism / Online / Cleanup-Image / RestoreHealth","title":"SFC Scannow and DISM Commands"},{"location":"Windows/SFC%20and%20DISM%20Commands/#sfc","text":"Visit Microsoft's Docs on SFC - Link The sfc /scannow command is a well known way to do an integrity check of all Windows 10 system files. sfc.exe is the System File Checker tool which can be helpful in many scenarios and fix various issues with Windows 10. You can save you time by adding a special context menu entry to launch it directly with one click.","title":"SFC"},{"location":"Windows/SFC%20and%20DISM%20Commands/#add-sfc-scannow-to-desktop-context-menu","text":"Registry Script: Windows Registry Editor Version 5.00 [ HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\SFCScannow] \"Icon\" = \"cmd.exe\" \"MUIVerb\" = \"SFC /Scannow\" \"Position\" = \"Bottom\" \"SubCommands\" = \"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\01Scannow] \"HasLUAShield\" = \"\" \"MUIVerb\" = \"Run SFC /Scannow\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\01Scannow\\command] @ = \"PowerShell -windowstyle hidden -command \\\"Start-Process cmd -ArgumentList '/s,/k, sfc.exe /scannow' -Verb runAs\\\"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\02ViewLog] \"MUIVerb\" = \"View log for SFC\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\02ViewLog\\command] @ = \"PowerShell (Select-String [SR] $env:windir\\\\Logs\\\\CBS\\\\CBS.log -s).Line >\\\"$env:userprofile\\\\Desktop\\\\SFC_LOG.txt\\\"\"","title":"Add SFC /Scannow to Desktop Context Menu:"},{"location":"Windows/SFC%20and%20DISM%20Commands/#dism","text":"Visit Microsoft's Official Docs on DISM - Link How to Fix Windows 10 using DISM (winaero.com) Deployment Image Servicing and Management (DISM.exe) is a command-line tool that can be used to service and prepare Windows images, including those used for Windows PE, Windows Recovery Environment (Windows RE) and Windows Setup. DISM can be used to service a Windows image (.wim) or a virtual hard disk (.vhd or.vhdx). The Component store is a core feature of Windows 10 which stores all of the files related to the OS grouped by components and as hardlinks. With new servicing model introduced in Vista, some system files are shared between two components and they are all hardlinked to the system32 folder. When the OS is serviced, the component store is updated. The Component Store is part of the Windows Imaging and Servicing stack. There is a special console tool called DISM which ships with Windows 10 by default. It can be used to fix Windows Component Store corruption. It is especially useful when the usual command \" sfc /scannow \" cannot repair damaged system files. The DISM tool writes the following log files: C:\\Windows\\Logs\\CBS\\CBS.log C:\\Windows\\Logs\\DISM\\dism.log They can be used to analyze errors and see completed operations.","title":"DISM"},{"location":"Windows/SFC%20and%20DISM%20Commands/#add-dism-to-desktop-context-menu","text":"Registry Script: Windows Registry Editor Version 5.00 [ HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\DismContextMenu] \"Icon\" = \"WmiPrvSE.exe\" \"MUIVerb\" = \"Repair Windows Image\" \"Position\" = \"Bottom\" \"SubCommands\" = \"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\CheckHealth] \"HasLUAShield\" = \"\" \"MUIVerb\" = \"Check Health of Windows Image\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\CheckHealth\\command] @ = \"PowerShell -windowstyle hidden -command \\\"Start-Process cmd -ArgumentList '/s,/k, Dism /Online /Cleanup-Image /CheckHealth' -Verb runAs\\\"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\RestoreHealth] \"HasLUAShield\" = \"\" \"MUIVerb\" = \"Repair Windows Image\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\RestoreHealth\\command] @ = \"PowerShell -windowstyle hidden -command \\\"Start-Process cmd -ArgumentList '/s,/k, Dism /Online /Cleanup-Image /RestoreHealth' -Verb runAs\\\"\"","title":"Add DISM to Desktop Context Menu:"},{"location":"Windows/SFC%20and%20DISM%20Commands/#how-it-works","text":"Check Health of Windows Image. This command executes DISM as follows: Dism /Online /Cleanup-Image /CheckHealth . The key option here is CheckHealth. We use it to check if some process has marked the Component Store as corrupted and whether the corruption is repairable. This command is not supposed to fix any issues. It only reports about problems if they are present and if the CBS store is flagged. This command doesn't create a log file. Repair Windows Image . This command starts DISM with the following arguments: Dism /Online /Cleanup-Image /RestoreHealth . The DISM tool started with the /RestoreHealth option will scan the component store for corruption and perform the required repair operations automatically. It will create a log file. The whole process can take several hours, so be patient. On hard drives, it will take longer compared to an SSD. Both commands start elevated from PowerShell .","title":"How it Works"},{"location":"Windows/SFC%20and%20DISM%20Commands/#removing-registry-edits","text":"To undo the Context Menu REGEDIT entries here\u2019s the scripts: Remove SFC Context Menu: Windows Registry Editor Version 5.00 [ - HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\SFCScannow] Remove DISM Context Menu: Windows Registry Editor Version 5.00 [ - HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\DismContextMenu] JIMMY BRIGGS - 2021","title":"Removing Registry Edits"},{"location":"Windows/Using%20diskusage%20Command%20in%20Windows/","text":"Windows New Command Line Tool - diskusage \u00b6 If you are running a Windows Insider Preview build (20277, 21277, or later) you can now utilize a great new Windows CLI feature: diskusage . Note that I am writing this as of December 17, 2020. Remember that this tool is still in development so some features aren't completely stable, but whatever, let's look into it! Using diskusage \u00b6 Disk Usage allows you to check out how much space your drives are taking up using the Command Prompt. There are some other handy parameters for analysis, too. Open up your command prompt and run diskusage /? : As of can see the command comes fully loaded with a suite of flags, commands, and arguments. Caveat: Bytes Only \u00b6 One caveat of the tool is that it currently only recognizes bytes , so if you wanted to search for file larger than 1.73 gigabytes you would have to input 1,857,573,355.52 bytes! I suggest using a simple tool to convert from GB/KB/MB \\<-> Bytes like a search engine's unit conversion tool , a dedicated website like Byte Converter or better yet a command line utility like humaize-bytes . For those who cannot remember the binary prefix multipliers here's a reference conversion table: When Disk Usage is fully functional, you'll be able to create specific configuration files that contain the search and analysis options you regularly use, saving you additional time when analyzing your data.","title":"Using diskusage Command in Windows"},{"location":"Windows/Using%20diskusage%20Command%20in%20Windows/#windows-new-command-line-tool-diskusage","text":"If you are running a Windows Insider Preview build (20277, 21277, or later) you can now utilize a great new Windows CLI feature: diskusage . Note that I am writing this as of December 17, 2020. Remember that this tool is still in development so some features aren't completely stable, but whatever, let's look into it!","title":"Windows New Command Line Tool - diskusage"},{"location":"Windows/Using%20diskusage%20Command%20in%20Windows/#using-diskusage","text":"Disk Usage allows you to check out how much space your drives are taking up using the Command Prompt. There are some other handy parameters for analysis, too. Open up your command prompt and run diskusage /? : As of can see the command comes fully loaded with a suite of flags, commands, and arguments.","title":"Using diskusage"},{"location":"Windows/Using%20diskusage%20Command%20in%20Windows/#caveat-bytes-only","text":"One caveat of the tool is that it currently only recognizes bytes , so if you wanted to search for file larger than 1.73 gigabytes you would have to input 1,857,573,355.52 bytes! I suggest using a simple tool to convert from GB/KB/MB \\<-> Bytes like a search engine's unit conversion tool , a dedicated website like Byte Converter or better yet a command line utility like humaize-bytes . For those who cannot remember the binary prefix multipliers here's a reference conversion table: When Disk Usage is fully functional, you'll be able to create specific configuration files that contain the search and analysis options you regularly use, saving you additional time when analyzing your data.","title":"Caveat: Bytes Only"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/","text":"WinGet CLI Settings \u00b6 You can configure WinGet by editing the settings.json file. Running winget settings will open the file in the default json editor, if no editor is configured, notepad.exe is used. File Location \u00b6 Settings file is located in %LOCALAPPDATA%\\Packages\\Microsoft.DesktopAppInstaller_8wekyb3d8bbwe\\LocalState\\settings.json If you are using the non-packaged WinGet version by building it from source code, the file will be located under %LOCALAPPDATA%\\Microsoft\\WinGet\\Settings\\settings.json Source \u00b6 The source settings involve configuration to the WinGet source. \"source\": { \"autoUpdateIntervalInMinutes\": 3 }, autoUpdateIntervalInMinutes \u00b6 A positive integer represents the update interval in minutes. The check for updates only happens when a source is used. A zero will disable the check for updates to a source. Any other values are invalid. Disable: 0 Default: 5 To manually update the source use winget source update Visual \u00b6 The visual settings involve visual elements that are displayed by WinGet \"visual\": { \"progressBar\": \"accent\" }, progressBar \u00b6 Color of the progress bar that WinGet displays when not specified by arguments. accent (default) retro rainbow Experimental Features \u00b6 To allow work to be done and distributed to early adopters for feedback, settings can be used to enable \"experimental\" features. The experimentalFeatures settings involve the configuration of these \"experimental\" features. Individual features can be enabled under this node. The example below shows sample experimental features. \"experimentalFeatures\": { \"experimentalCmd\": true, \"experimentalArg\": false }, experimentalMSStore \u00b6 Microsoft Store App support in WinGet is currently implemented as an experimental feature. It supports a curated list of utility apps from Microsoft Store. You can enable the feature as shown below. \"experimentalFeatures\": { \"experimentalMSStore\": true }, list \u00b6 While work is in progress on list, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"list\": true }, upgrade \u00b6 While work is in progress on upgrade, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"upgrade\": true }, uninstall \u00b6 While work is in progress on uninstall, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"uninstall\": true }, import \u00b6 While work is in progress for import, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"import\": true }, restSource \u00b6 While work is in progress for rest source support, the feature is hidden behind a feature toggle. Enabling this will not change how client works currently and will allow testing any additional rest sources added. One can enable it as below: \"experimentalFeatures\": { \"restSource\": true },","title":"WinGet CLI Settings"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#winget-cli-settings","text":"You can configure WinGet by editing the settings.json file. Running winget settings will open the file in the default json editor, if no editor is configured, notepad.exe is used.","title":"WinGet CLI Settings"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#file-location","text":"Settings file is located in %LOCALAPPDATA%\\Packages\\Microsoft.DesktopAppInstaller_8wekyb3d8bbwe\\LocalState\\settings.json If you are using the non-packaged WinGet version by building it from source code, the file will be located under %LOCALAPPDATA%\\Microsoft\\WinGet\\Settings\\settings.json","title":"File Location"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#source","text":"The source settings involve configuration to the WinGet source. \"source\": { \"autoUpdateIntervalInMinutes\": 3 },","title":"Source"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#autoupdateintervalinminutes","text":"A positive integer represents the update interval in minutes. The check for updates only happens when a source is used. A zero will disable the check for updates to a source. Any other values are invalid. Disable: 0 Default: 5 To manually update the source use winget source update","title":"autoUpdateIntervalInMinutes"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#visual","text":"The visual settings involve visual elements that are displayed by WinGet \"visual\": { \"progressBar\": \"accent\" },","title":"Visual"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#progressbar","text":"Color of the progress bar that WinGet displays when not specified by arguments. accent (default) retro rainbow","title":"progressBar"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#experimental-features","text":"To allow work to be done and distributed to early adopters for feedback, settings can be used to enable \"experimental\" features. The experimentalFeatures settings involve the configuration of these \"experimental\" features. Individual features can be enabled under this node. The example below shows sample experimental features. \"experimentalFeatures\": { \"experimentalCmd\": true, \"experimentalArg\": false },","title":"Experimental Features"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#experimentalmsstore","text":"Microsoft Store App support in WinGet is currently implemented as an experimental feature. It supports a curated list of utility apps from Microsoft Store. You can enable the feature as shown below. \"experimentalFeatures\": { \"experimentalMSStore\": true },","title":"experimentalMSStore"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#list","text":"While work is in progress on list, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"list\": true },","title":"list"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#upgrade","text":"While work is in progress on upgrade, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"upgrade\": true },","title":"upgrade"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#uninstall","text":"While work is in progress on uninstall, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"uninstall\": true },","title":"uninstall"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#import","text":"While work is in progress for import, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"import\": true },","title":"import"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#restsource","text":"While work is in progress for rest source support, the feature is hidden behind a feature toggle. Enabling this will not change how client works currently and will allow testing any additional rest sources added. One can enable it as below: \"experimentalFeatures\": { \"restSource\": true },","title":"restSource"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/","text":"Windows Command Prompt \u00b6 Command Line - Getting started with CMD \u00b6 cmd documentation: Getting started with cmd This section provides an overview of what cmd is, and why a developer might want to use it. It should also mention any large subjects within cmd, and link out to the related topics. Since the Documentation for cmd is new, you may need to create initial versions of those related topics. Commands in CMD # The available commands will be displayed, including a brief description, in tabular format. In Windows 10 the following commands are listed: Command Description ASSOC Displays or modifies file extension associations. ATTRIB Displays or changes file attributes. BREAK Sets or clears extended CTRL+C checking. BCDEDIT Sets properties in boot database to control boot loading. CACLS Displays or modifies access control lists (ACLs) of files. CALL Calls one batch program from another. CD Displays the name of or changes the current directory. CHCP Displays or sets the active code page number. CHDIR Displays the name of or changes the current directory. CHKDSK Checks a disk and displays a status report. CHKNTFS Displays or modifies the checking of disk at boot time. CLS Clears the screen. CMD Starts a new instance of the Windows command interpreter. COLOR Sets the default console foreground and background colors. COMP Compares the contents of two files or sets of files. COMPACT Displays or alters the compression of files on NTFS partitions. CONVERT Converts FAT volumes to NTFS. You cannot convert the current drive. COPY Copies one or more files to another location. DATE Displays or sets the date. DEL Deletes one or more files. DIR Displays a list of files and subdirectories in a directory. DISKPART Displays or configures Disk Partition properties. DOSKEY Edits command lines, recalls Windows commands, and creates macros. DRIVERQUERY Displays current device driver status and properties. ECHO Displays messages, or turns command echoing on or off. ENDLOCAL Ends localization of environment changes in a batch file. ERASE Deletes one or more files. EXIT Quits the CMD.EXE program (command interpreter). FC Compares two files or sets of files, and displays the differences between them. FIND Searches for a text string in a file or files. FINDSTR Searches for strings in files. FOR Runs a specified command for each file in a set of files. FORMAT Formats a disk for use with Windows. FSUTIL Displays or configures the file system properties. FTYPE Displays or modifies file types used in file extension associations. GOTO Directs the Windows command interpreter to a labeled line in a batch program. GPRESULT Displays Group Policy information for machine or user. GRAFTABL Enables Windows to display an extended character set in graphics mode. HELP Provides Help information for Windows commands. ICACLS Display, modify, backup, or restore ACLs for files and directories. IF Performs conditional processing in batch programs. LABEL Creates, changes, or deletes the volume label of a disk. MD Creates a directory. MKDIR Creates a directory. MKLINK Creates Symbolic Links and Hard Links MODE Configures a system device. MORE Displays output one screen at a time. MOVE Moves one or more files from one directory to another directory. OPENFILES Displays files opened by remote users for a file share. PATH Displays or sets a search path for executable files. PAUSE Suspends processing of a batch file and displays a message. POPD Restores the previous value of the current directory saved by PUSHD. PRINT Prints a text file. PROMPT Changes the Windows command prompt. PUSHD Saves the current directory then changes it. RD Removes a directory. RECOVER Recovers readable information from a bad or defective disk. REM Records comments (remarks) in batch files or CONFIG.SYS. REN Renames a file or files. RENAME Renames a file or files. REPLACE Replaces files. RMDIR Removes a directory. ROBOCOPY Advanced utility to copy files and directory trees SET Displays, sets, or removes Windows environment variables. SETLOCAL Begins localization of environment changes in a batch file. SC Displays or configures services (background processes). SCHTASKS Schedules commands and programs to run on a computer. SHIFT Shifts the position of replaceable parameters in batch files. SHUTDOWN Allows proper local or remote shutdown of machine. SORT Sorts input. START Starts a separate window to run a specified program or command. SUBST Associates a path with a drive letter. SYSTEMINFO Displays machine specific properties and configuration. TASKLIST Displays all currently running tasks including services. TASKKILL Kill or stop a running process or application. TIME Displays or sets the system time. TITLE Sets the window title for a CMD.EXE session. TREE Graphically displays the directory structure of a drive or path. TYPE Displays the contents of a text file. VER Displays the Windows version. VERIFY Tells Windows whether to verify that your files are written correctly to a disk. VOL Displays a disk volume label and serial number. XCOPY Copies files and directory trees. WMIC Displays WMI information inside interactive command shell. To get more insight about a specific command use the /? option, e.g. the tree command gives: tree / ? Graphically displays the folder structure of a drive or path . TREE [ drive: ][ path ] [ /F ] [ /A ] / F Display the names of the files in each folder . / A Use ASCII instead of extended characters . Features # Microsoft Command Prompt is a command-line interpreter (CLI) for the Windows operating systems. A CLI is program intended primarily to read operating system instructions typed on a keyboard by the user. It is therefore addressed also as a command-line interface , to contrast it with graphical interfaces. As these interfaces (whether textual or graphical) shield the user from directly accessing to the operating system kernel, they are also said shells . Given the name of the Command Prompt executable file, cmd.exe , the Command Prompt is friendly named cmd . Given its OS piloting role, it is also said the console . Like other shells, cmd can read batch of instructions from a file. In this case the cmd shell acts as a language interpreter and the file content can be regarded as an actual program. When executing these batch programs, there is no intermediate compilation phase. They are typically read, interpreted and executed line by line. Since there is no compilation, there is no production of a separated executable file. For this reason the programs are denoted batch scripts or shell scripts . Note that the instructions entered interactively might have a slightly different syntax from those submitted as a script, but the general principle is that what can be entered from the command line can be also put in a file for later reuse. Hello World # Command Prompt batch scripts have extension .cmd or .bat , the latter for compatibility reasons. To create a hello-word-script, you first need a place where to type it. For simple scripts, also the Windows Notepad will do. If you are serious about shell scripting, you need more effective tools. There are anyway several free alternatives, such as Notepad++ . In your designated editor type: echo Hello World pause Save it as hello.cmd If you are using \"Notepad\" as an editor, you should pay much attention to the saved name, as Notepad tends to add always a .txt extension to your files, which means that the actual name of your file might be hello.cmd.txt . To avoid this, in the save dialog box: In the File name field enter the name in double quotes, e.g. \"hello.cmd\" In the Save as type field select All Files, instead of the default Text Document option. If the file has been saved properly, its icon should be similar to (Windows Vista): You may also consider to disable the option \"Hide extension for known file types\" in File Explorer folder view options. In this case, file names are always displayed with their extensions. To execute hello.cmd there are two possibilities. If you are using the Windows graphical shell, just double click on its icon. If you want to use the Command Prompt itself, you must first identify the directory where you saved hello.cmd . In this regard, if you open File Explorer with +E. In the windows listing files, you normally read the name of the directory path containing them. You can therefore identify the directory of hello.cmd . Windows directory names tend to be quite long and typing them is error prone. It is better if you select and copy the directory path in the clipboard for later pasting. Start the Command Prompt. You read a line similar to this. Microsoft Windows [Version ...] (c) ... Microsoft Corporation. All rights reserved. C:\\Users\\...> The version/year of Windows of course depends on yours. In the the final line, before > , you read the path of the directory which is current. You should make current the directory where your script is. For this reason enter the change directory command cd , using a line similar to the following: cd <dirpath> Instead of <dirpath> , paste the name of the directory you previously copied. To paste the directory path, in Windows 10, you just need to type Ctrl-C, as you would in an editor. For older systems you should be able to do this by right clicking in the cmd window. After entering the command, note that current path, before > , changes accordingly. You can now run your hello script by simply entering: hello Comments # The script prints an output similar to: C :\\ Users \\... > echo Hello World Hello World C :\\ Users \\... > pause Press any key to continue . . . The lines hosting the symbol > restate the script instructions as if you had entered interactively. This can be disabled writing: @echo off as the first line of your script. This might reduce the clutter, but you have less hints on what is going on, with respect to those script commands that do not give visible outputs. The last command, pause , prompts you to hit any key. When you do, you exit hello . If you run hello from the console, you don't really need it, because, when hello terminates its execution, cmd.exe remains open and you can to read hello output. When double-clicking in Explorer, you start cmd.exe for the time necessary to execute hello . When hello terminates, cmd.exe does the same and you have no possibility to read hello output. pause command prevents hello from exiting until you hit a key, which gives also the possibility to read the output. Finally, despite the name of the script is hello.cmd , it is not necessary to type the whole name, its hello stem is sufficient. This mechanism works for executables too, with extension .exe . What if there is a script hello.cmd and an executable hello.exe in the same directory? The former has priority in the Command Prompt, so hello.cmd will be executed. Navigating in cmd # One of the most common things you'll need to do in the command prompt is navigate your file system. To do this, we'll utilize the cd and dir keywords. Start by opening up a command prompt using one of the methods mentioned here . You most likely see something similar to what's below, where UserName is your user. C:\\Users\\UserName> Regardless of where in your file structure you are, if your system is like most, we can start with this command: cd C:\\ This will change your current directory to the C:\\ drive. Notice how the screen now looks like this C:\\> Next, run a dir so we can see anything in the C:\\ drive dir This will show you a list of files and folders with some information about them, similar to this: There's lots of good info here, but for basic navigation, we just care about the right-most column. Notice how we have a Users folder. That means we can run this cd Users Now if you run dir again, you'll see all the files and folders in your C:\\Users directory. Now, we didn't find what we wanted here, so let's go back to the parent folder. Rather than type the path to it, we can use .. to go up one folder like so cd .. Now we are back in C:\\ . If you want to go up multiple folders at once, you can put a backslash and another set of periods like so: cd ..\\.. , but we only needed one folder. Now we want to look in that Program Files folder. To avoid confusing the system, it's a good idea to put quotes around the directories, especially when there are spaces in the name. So this time, we'll use this command C:\\>cd \"Program Files\" Now you are in C:\\Program Files> and a dir command now will tell you anything that's in here. So, say we get tired of wandering around to find the folder and looked up exactly where we were needing to go. Turns out it's C:\\Windows\\Logs Rather than do a .. to Windows to Logs , we can just put the full path like so: cd \"C:\\Windows\\Logs\" And that's the basics of navigating the command prompt. You can now move through all your folders so you can run your other commands in the proper places. Opening a Command Prompt # The command prompt comes pre-installed on all Windows NT, Windows CE, OS/2 and eComStation operating systems, and exists as cmd.exe , typically located in C:\\Windows\\system32\\cmd.exe On Windows 7 the fastest ways to open the command prompt are: Press , type \"cmd\" and then press Enter. Press +R, type \"cmd\" then then press Enter. It can also be opened by navigating to the executable and double-clicking on it. In some cases you might need to run cmd with elevated permissions, in this case right click and select \"Run as administrator\". This can also be achieved by pressing Control+ Shift+Enter instead of Enter. Source Must Know Commands \u00b6 Assoc \u00b6 Most files in Windows are associated with a specific program that is assigned to open the file by default. At times, remembering these associations can become confusing. You can remind yourself by entering the command assoc to display a full list of file name extensions and program associations. You can also extend the command to change file associations. For example, assoc .txt= will change the file association for text files to whatever program you enter after the equal sign. The Assoc command itself will reveal both the extension names and program names, which will help you properly use this command. In Windows 10, you can view a more user-friendly interface that also lets you change file type associations on the spot. Head to Settings (Windows + I) > Apps > Default apps > Choose default app by file type . Cipher \u00b6 Deleting files on a mechanical hard drive doesn't really delete them at all. Instead, it marks the files as no longer accessible and the space they took up as free. The files remain recoverable until the system overwrites them with new data, which can take some time. The cipher command, however, wipes a directory by writing random data to it. To wipe your C drive, for example, you'd use the cipher /w:d command, which will wipe free space on the drive. The command does not overwrite undeleted data, so you will not wipe out files you need by running this command. You can use a host of other cipher commands, however, they are generally redundant with BitLocker enabled versions of Windows . Driverquery \u00b6 Drivers remain among the most important software installed on a PC. Improperly configured or missing drivers can cause all sorts of trouble, so its good to have access to a list of what's on your PC. That's exactly what the driverquery command does. You can extend it to driverquery -v to obtain more information, including the directory in which the driver is installed. For best use, I like to pipe the verbose output ( -v ) into a CSV ( -fo \"csv\" ) via: driverquery -v -fo \"csv\" >> My-Drivers.csv File Compare \u00b6 You can use this command to identify differences in text between two files. It's particularly useful for writers and programmers trying to find small changes between two versions of a file. Simply type fc and then the directory path and file name of the two files you want to compare. You can also extend the command in several ways. Typing /b compares only binary output, /c disregards the case of text in the comparison, and /l only compares ASCII text. So, for example, you could use the following: fc /l \"C:\\\\Program Files (x86)\\\\example1.doc\" \"C:\\\\Program Files (x86)\\\\example2.doc\" The above command compares ASCII text in two word documents. ipconfig \u00b6 This command relays the IP address that your computer is currently using. However, if you're behind a router (like most computers today), you'll instead receive the local network address of the router. Still, ipconfig is useful because of its extensions. ipconfig /release followed by ipconfig /renew can force your Windows PC into asking for a new IP address, which is useful if your computer claims one isn't available. You can also use ipconfig /flushdns to refresh your DNS address. These commands are great if the Windows network troubleshooter chokes, which does happen on occasion. Netstat \u00b6 Entering the command netstat -an will provide you with a list of currently open ports and related IP addresses. This command will also tell you what state the port is in; listening, established, or closed. This is a great command for when you're trying to troubleshoot devices connected to your PC or when you fear a Trojan infected your system and you're trying to locate a malicious connection. Ping \u00b6 Sometimes, you need to know whether or not packets are making it to a specific networked device. That's where ping comes in handy. Typing ping followed by an IP address or web domain will send a series of test packets to the specified address. If they arrive and are returned, you know the device is capable of communicating with your PC; if it fails, you know that there's something blocking communication between the device and your computer. This can help you decide if the root of the issue is an improper configuration or a failure of network hardware. PathPing \u00b6 This is a more advanced version of ping that's useful if there are multiple routers between your PC and the device you're testing. Like ping, you use this command by typing pathping followed by the IP address, but unlike ping, pathping also relays some information about the route the test packets take. Tracert \u00b6 The tracert command is similar to pathping. Once again, type tracert followed by the IP address or domain you'd like to trace. You'll receive information about each step in the route between your PC and the target. Unlike pathping, however, tracert also tracks how much time (in milliseconds) each hop between servers or devices takes. Powercfg \u00b6 Powercfg is a very powerful command for managing and tracking how your computer uses energy. You can use the command powercfg hibernate on and powercfg hibernate off to manage hibernation, and you can also use the command powercfg /a to view the power-saving states currently available on your PC. Another useful command is powercfg /devicequery s1_supported , which displays a list of devices on your computer that support connected standby. When enabled, you can use these devices to bring your computer out of standby, even remotely. You can enable this by selecting the device in Device Manager , opening its properties, going to the Power Management tab, and then checking the Allow this device to wake the computer box. Powercfg /lastwake will show you what device last woke your PC from a sleep state. You can use this command to troubleshoot your PC if it seems to wake from sleep at random. You can use the powercfg /energy command to build a detailed power consumption report for your PC. The report saves to the directory indicated after the command finishes. This report will let you know of any system faults that might increase power consumption, like devices blocking certain sleep modes, or poorly configured to respond to your power management settings. Windows 8 added powercfg /batteryreport , which provides a detailed analysis of battery use, if applicable. Normally output to your Windows user directory, the report provides details about the time and length of charge and discharge cycles, lifetime average battery life, and estimated battery capacity. Shutdown \u00b6 Windows 8 introduced the shutdown command that, you guessed it, shuts down your computer . This is, of course, redundant with the already easily accessed shutdown button, but what's not redundant is the shutdown /r /o command, which restarts your PC and launches the Advanced Start Options menu, which is where you can access Safe Mode and Windows recovery utilities. This is useful if you want to restart your computer for troubleshooting purposes. Systeminfo \u00b6 This command will give you a detailed configuration overview of your computer. The list covers your operating system and hardware. For example, you can look up the original Windows installation date, the last boot time, your BIOS version, total and available memory, installed hotfixes, network card configurations, and more. Use systeminfo /s followed by the host name of a computer on your local network, to remotely grab the information for that system. This may require additional syntax elements for the domain, user name, and password, like this: systeminfo / s [ host_name ] / u [ domain ] \\ [ user_name ] / p [ user_password ] System File Checker \u00b6 System File Checker is an automatic scan and repair tool that focuses on Windows system files. You will need to run the command prompt with administrator privileges and enter the command sfc /scannow . If SFC finds any corrupt or missing files, it will automatically replace them using cached copies kept by Windows for this purpose alone. The command can require a half-hour to run on older notebooks. Tasklist \u00b6 You can use the tasklist command to provide a current list of all tasks running on your PC. Though somewhat redundant with Task Manager, the command may sometimes find tasks hidden from view in that utility. There's also a wide range of modifiers. Tasklist -svc shows services related to each task, use tasklist -v to obtain more detail on each task, and tasklist -m will locate DLL files associated with active tasks. These commands are useful for advanced troubleshooting. Our reader Eric noted that you can \"get the name of the executable associated with the particular process ID you're interested in.\" The command for that operation is tasklist | find [process id]. Taskkill \u00b6 Tasks that appear in the tasklist command will have an executable and process ID (a four- or five-digit number) associated with them. You can force stop a program using taskkill -im followed by the executable's name, or taskkill -pid followed by the process ID. Again, this is a bit redundant with Task Manager, but you can use it to kill otherwise unresponsive or hidden programs. Chkdsk \u00b6 Windows automatically marks your drive for a diagnostic chkdsk scan when symptoms indicate that a local drive has bad sectors, lost clusters, or other logical or physical errors. If you suspect your hard drive is failing, you can manually initiate a scan. The most basic command is chkdsk c: , which will immediately scan the C: drive, without a need to restart the computer. If you add parameters like /f, /r, /x, or /b, such as in chkdsk /f /r /x /b c: , chkdsk will also fix errors, recover data, dismount the drive, or clear the list of bad sectors, respectively. These actions require a reboot, as they can only run with Windows powered down. If you see chkdsk run at startup, let it do its thing. If it gets stuck, however, refer to the chkdsk troubleshooting article . schtasks \u00b6 Schtasks is your command prompt access to the Task Scheduler, one of many underrated Windows administrative tools. While you can use the GUI to manage your scheduled tasks, the command prompt lets you copy&paste complex commands to set up multiple similar tasks without having to click through various options. Ultimately, it's much easier to use, once you've committed key parameters to memory. For example, you could schedule your computer to reboot at 11pm every Friday: schtasks /create /sc weekly /d FRI /tn \"auto reboot computer weekly\" /st 23:00 /tr \"shutdown -r -f -t 10\" To complement your weekly reboot, you could schedule tasks to launch specific programs on startup: schtasks /create /sc onstart /tn \"launch Chrome on startup\" /tr \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\Chrome.exe\" To duplicate the above command for different programs, just copy, paste, and modify it as needed. Command and Conquer Your Windows PC \u00b6 This article can only give you a taste of what's hidden within the Windows command line. When including all variables, there are literally hundreds of commands. Download Microsoft's command line reference guide (in Edge or Internet Explorer) for advanced support and troubleshooting. Tired of the command prompt? Time to try the new Windows Terminal !","title":"Windows Command Prompt"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#windows-command-prompt","text":"","title":"Windows Command Prompt"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#command-line-getting-started-with-cmd","text":"cmd documentation: Getting started with cmd This section provides an overview of what cmd is, and why a developer might want to use it. It should also mention any large subjects within cmd, and link out to the related topics. Since the Documentation for cmd is new, you may need to create initial versions of those related topics. Commands in CMD # The available commands will be displayed, including a brief description, in tabular format. In Windows 10 the following commands are listed: Command Description ASSOC Displays or modifies file extension associations. ATTRIB Displays or changes file attributes. BREAK Sets or clears extended CTRL+C checking. BCDEDIT Sets properties in boot database to control boot loading. CACLS Displays or modifies access control lists (ACLs) of files. CALL Calls one batch program from another. CD Displays the name of or changes the current directory. CHCP Displays or sets the active code page number. CHDIR Displays the name of or changes the current directory. CHKDSK Checks a disk and displays a status report. CHKNTFS Displays or modifies the checking of disk at boot time. CLS Clears the screen. CMD Starts a new instance of the Windows command interpreter. COLOR Sets the default console foreground and background colors. COMP Compares the contents of two files or sets of files. COMPACT Displays or alters the compression of files on NTFS partitions. CONVERT Converts FAT volumes to NTFS. You cannot convert the current drive. COPY Copies one or more files to another location. DATE Displays or sets the date. DEL Deletes one or more files. DIR Displays a list of files and subdirectories in a directory. DISKPART Displays or configures Disk Partition properties. DOSKEY Edits command lines, recalls Windows commands, and creates macros. DRIVERQUERY Displays current device driver status and properties. ECHO Displays messages, or turns command echoing on or off. ENDLOCAL Ends localization of environment changes in a batch file. ERASE Deletes one or more files. EXIT Quits the CMD.EXE program (command interpreter). FC Compares two files or sets of files, and displays the differences between them. FIND Searches for a text string in a file or files. FINDSTR Searches for strings in files. FOR Runs a specified command for each file in a set of files. FORMAT Formats a disk for use with Windows. FSUTIL Displays or configures the file system properties. FTYPE Displays or modifies file types used in file extension associations. GOTO Directs the Windows command interpreter to a labeled line in a batch program. GPRESULT Displays Group Policy information for machine or user. GRAFTABL Enables Windows to display an extended character set in graphics mode. HELP Provides Help information for Windows commands. ICACLS Display, modify, backup, or restore ACLs for files and directories. IF Performs conditional processing in batch programs. LABEL Creates, changes, or deletes the volume label of a disk. MD Creates a directory. MKDIR Creates a directory. MKLINK Creates Symbolic Links and Hard Links MODE Configures a system device. MORE Displays output one screen at a time. MOVE Moves one or more files from one directory to another directory. OPENFILES Displays files opened by remote users for a file share. PATH Displays or sets a search path for executable files. PAUSE Suspends processing of a batch file and displays a message. POPD Restores the previous value of the current directory saved by PUSHD. PRINT Prints a text file. PROMPT Changes the Windows command prompt. PUSHD Saves the current directory then changes it. RD Removes a directory. RECOVER Recovers readable information from a bad or defective disk. REM Records comments (remarks) in batch files or CONFIG.SYS. REN Renames a file or files. RENAME Renames a file or files. REPLACE Replaces files. RMDIR Removes a directory. ROBOCOPY Advanced utility to copy files and directory trees SET Displays, sets, or removes Windows environment variables. SETLOCAL Begins localization of environment changes in a batch file. SC Displays or configures services (background processes). SCHTASKS Schedules commands and programs to run on a computer. SHIFT Shifts the position of replaceable parameters in batch files. SHUTDOWN Allows proper local or remote shutdown of machine. SORT Sorts input. START Starts a separate window to run a specified program or command. SUBST Associates a path with a drive letter. SYSTEMINFO Displays machine specific properties and configuration. TASKLIST Displays all currently running tasks including services. TASKKILL Kill or stop a running process or application. TIME Displays or sets the system time. TITLE Sets the window title for a CMD.EXE session. TREE Graphically displays the directory structure of a drive or path. TYPE Displays the contents of a text file. VER Displays the Windows version. VERIFY Tells Windows whether to verify that your files are written correctly to a disk. VOL Displays a disk volume label and serial number. XCOPY Copies files and directory trees. WMIC Displays WMI information inside interactive command shell. To get more insight about a specific command use the /? option, e.g. the tree command gives: tree / ? Graphically displays the folder structure of a drive or path . TREE [ drive: ][ path ] [ /F ] [ /A ] / F Display the names of the files in each folder . / A Use ASCII instead of extended characters . Features # Microsoft Command Prompt is a command-line interpreter (CLI) for the Windows operating systems. A CLI is program intended primarily to read operating system instructions typed on a keyboard by the user. It is therefore addressed also as a command-line interface , to contrast it with graphical interfaces. As these interfaces (whether textual or graphical) shield the user from directly accessing to the operating system kernel, they are also said shells . Given the name of the Command Prompt executable file, cmd.exe , the Command Prompt is friendly named cmd . Given its OS piloting role, it is also said the console . Like other shells, cmd can read batch of instructions from a file. In this case the cmd shell acts as a language interpreter and the file content can be regarded as an actual program. When executing these batch programs, there is no intermediate compilation phase. They are typically read, interpreted and executed line by line. Since there is no compilation, there is no production of a separated executable file. For this reason the programs are denoted batch scripts or shell scripts . Note that the instructions entered interactively might have a slightly different syntax from those submitted as a script, but the general principle is that what can be entered from the command line can be also put in a file for later reuse. Hello World # Command Prompt batch scripts have extension .cmd or .bat , the latter for compatibility reasons. To create a hello-word-script, you first need a place where to type it. For simple scripts, also the Windows Notepad will do. If you are serious about shell scripting, you need more effective tools. There are anyway several free alternatives, such as Notepad++ . In your designated editor type: echo Hello World pause Save it as hello.cmd If you are using \"Notepad\" as an editor, you should pay much attention to the saved name, as Notepad tends to add always a .txt extension to your files, which means that the actual name of your file might be hello.cmd.txt . To avoid this, in the save dialog box: In the File name field enter the name in double quotes, e.g. \"hello.cmd\" In the Save as type field select All Files, instead of the default Text Document option. If the file has been saved properly, its icon should be similar to (Windows Vista): You may also consider to disable the option \"Hide extension for known file types\" in File Explorer folder view options. In this case, file names are always displayed with their extensions. To execute hello.cmd there are two possibilities. If you are using the Windows graphical shell, just double click on its icon. If you want to use the Command Prompt itself, you must first identify the directory where you saved hello.cmd . In this regard, if you open File Explorer with +E. In the windows listing files, you normally read the name of the directory path containing them. You can therefore identify the directory of hello.cmd . Windows directory names tend to be quite long and typing them is error prone. It is better if you select and copy the directory path in the clipboard for later pasting. Start the Command Prompt. You read a line similar to this. Microsoft Windows [Version ...] (c) ... Microsoft Corporation. All rights reserved. C:\\Users\\...> The version/year of Windows of course depends on yours. In the the final line, before > , you read the path of the directory which is current. You should make current the directory where your script is. For this reason enter the change directory command cd , using a line similar to the following: cd <dirpath> Instead of <dirpath> , paste the name of the directory you previously copied. To paste the directory path, in Windows 10, you just need to type Ctrl-C, as you would in an editor. For older systems you should be able to do this by right clicking in the cmd window. After entering the command, note that current path, before > , changes accordingly. You can now run your hello script by simply entering: hello Comments # The script prints an output similar to: C :\\ Users \\... > echo Hello World Hello World C :\\ Users \\... > pause Press any key to continue . . . The lines hosting the symbol > restate the script instructions as if you had entered interactively. This can be disabled writing: @echo off as the first line of your script. This might reduce the clutter, but you have less hints on what is going on, with respect to those script commands that do not give visible outputs. The last command, pause , prompts you to hit any key. When you do, you exit hello . If you run hello from the console, you don't really need it, because, when hello terminates its execution, cmd.exe remains open and you can to read hello output. When double-clicking in Explorer, you start cmd.exe for the time necessary to execute hello . When hello terminates, cmd.exe does the same and you have no possibility to read hello output. pause command prevents hello from exiting until you hit a key, which gives also the possibility to read the output. Finally, despite the name of the script is hello.cmd , it is not necessary to type the whole name, its hello stem is sufficient. This mechanism works for executables too, with extension .exe . What if there is a script hello.cmd and an executable hello.exe in the same directory? The former has priority in the Command Prompt, so hello.cmd will be executed. Navigating in cmd # One of the most common things you'll need to do in the command prompt is navigate your file system. To do this, we'll utilize the cd and dir keywords. Start by opening up a command prompt using one of the methods mentioned here . You most likely see something similar to what's below, where UserName is your user. C:\\Users\\UserName> Regardless of where in your file structure you are, if your system is like most, we can start with this command: cd C:\\ This will change your current directory to the C:\\ drive. Notice how the screen now looks like this C:\\> Next, run a dir so we can see anything in the C:\\ drive dir This will show you a list of files and folders with some information about them, similar to this: There's lots of good info here, but for basic navigation, we just care about the right-most column. Notice how we have a Users folder. That means we can run this cd Users Now if you run dir again, you'll see all the files and folders in your C:\\Users directory. Now, we didn't find what we wanted here, so let's go back to the parent folder. Rather than type the path to it, we can use .. to go up one folder like so cd .. Now we are back in C:\\ . If you want to go up multiple folders at once, you can put a backslash and another set of periods like so: cd ..\\.. , but we only needed one folder. Now we want to look in that Program Files folder. To avoid confusing the system, it's a good idea to put quotes around the directories, especially when there are spaces in the name. So this time, we'll use this command C:\\>cd \"Program Files\" Now you are in C:\\Program Files> and a dir command now will tell you anything that's in here. So, say we get tired of wandering around to find the folder and looked up exactly where we were needing to go. Turns out it's C:\\Windows\\Logs Rather than do a .. to Windows to Logs , we can just put the full path like so: cd \"C:\\Windows\\Logs\" And that's the basics of navigating the command prompt. You can now move through all your folders so you can run your other commands in the proper places. Opening a Command Prompt # The command prompt comes pre-installed on all Windows NT, Windows CE, OS/2 and eComStation operating systems, and exists as cmd.exe , typically located in C:\\Windows\\system32\\cmd.exe On Windows 7 the fastest ways to open the command prompt are: Press , type \"cmd\" and then press Enter. Press +R, type \"cmd\" then then press Enter. It can also be opened by navigating to the executable and double-clicking on it. In some cases you might need to run cmd with elevated permissions, in this case right click and select \"Run as administrator\". This can also be achieved by pressing Control+ Shift+Enter instead of Enter. Source","title":"Command Line - Getting started with CMD"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#must-know-commands","text":"","title":"Must Know Commands"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#assoc","text":"Most files in Windows are associated with a specific program that is assigned to open the file by default. At times, remembering these associations can become confusing. You can remind yourself by entering the command assoc to display a full list of file name extensions and program associations. You can also extend the command to change file associations. For example, assoc .txt= will change the file association for text files to whatever program you enter after the equal sign. The Assoc command itself will reveal both the extension names and program names, which will help you properly use this command. In Windows 10, you can view a more user-friendly interface that also lets you change file type associations on the spot. Head to Settings (Windows + I) > Apps > Default apps > Choose default app by file type .","title":"Assoc"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#cipher","text":"Deleting files on a mechanical hard drive doesn't really delete them at all. Instead, it marks the files as no longer accessible and the space they took up as free. The files remain recoverable until the system overwrites them with new data, which can take some time. The cipher command, however, wipes a directory by writing random data to it. To wipe your C drive, for example, you'd use the cipher /w:d command, which will wipe free space on the drive. The command does not overwrite undeleted data, so you will not wipe out files you need by running this command. You can use a host of other cipher commands, however, they are generally redundant with BitLocker enabled versions of Windows .","title":"Cipher"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#driverquery","text":"Drivers remain among the most important software installed on a PC. Improperly configured or missing drivers can cause all sorts of trouble, so its good to have access to a list of what's on your PC. That's exactly what the driverquery command does. You can extend it to driverquery -v to obtain more information, including the directory in which the driver is installed. For best use, I like to pipe the verbose output ( -v ) into a CSV ( -fo \"csv\" ) via: driverquery -v -fo \"csv\" >> My-Drivers.csv","title":"Driverquery"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#file-compare","text":"You can use this command to identify differences in text between two files. It's particularly useful for writers and programmers trying to find small changes between two versions of a file. Simply type fc and then the directory path and file name of the two files you want to compare. You can also extend the command in several ways. Typing /b compares only binary output, /c disregards the case of text in the comparison, and /l only compares ASCII text. So, for example, you could use the following: fc /l \"C:\\\\Program Files (x86)\\\\example1.doc\" \"C:\\\\Program Files (x86)\\\\example2.doc\" The above command compares ASCII text in two word documents.","title":"File Compare"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#ipconfig","text":"This command relays the IP address that your computer is currently using. However, if you're behind a router (like most computers today), you'll instead receive the local network address of the router. Still, ipconfig is useful because of its extensions. ipconfig /release followed by ipconfig /renew can force your Windows PC into asking for a new IP address, which is useful if your computer claims one isn't available. You can also use ipconfig /flushdns to refresh your DNS address. These commands are great if the Windows network troubleshooter chokes, which does happen on occasion.","title":"ipconfig"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#netstat","text":"Entering the command netstat -an will provide you with a list of currently open ports and related IP addresses. This command will also tell you what state the port is in; listening, established, or closed. This is a great command for when you're trying to troubleshoot devices connected to your PC or when you fear a Trojan infected your system and you're trying to locate a malicious connection.","title":"Netstat"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#ping","text":"Sometimes, you need to know whether or not packets are making it to a specific networked device. That's where ping comes in handy. Typing ping followed by an IP address or web domain will send a series of test packets to the specified address. If they arrive and are returned, you know the device is capable of communicating with your PC; if it fails, you know that there's something blocking communication between the device and your computer. This can help you decide if the root of the issue is an improper configuration or a failure of network hardware.","title":"Ping"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#pathping","text":"This is a more advanced version of ping that's useful if there are multiple routers between your PC and the device you're testing. Like ping, you use this command by typing pathping followed by the IP address, but unlike ping, pathping also relays some information about the route the test packets take.","title":"PathPing"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#tracert","text":"The tracert command is similar to pathping. Once again, type tracert followed by the IP address or domain you'd like to trace. You'll receive information about each step in the route between your PC and the target. Unlike pathping, however, tracert also tracks how much time (in milliseconds) each hop between servers or devices takes.","title":"Tracert"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#powercfg","text":"Powercfg is a very powerful command for managing and tracking how your computer uses energy. You can use the command powercfg hibernate on and powercfg hibernate off to manage hibernation, and you can also use the command powercfg /a to view the power-saving states currently available on your PC. Another useful command is powercfg /devicequery s1_supported , which displays a list of devices on your computer that support connected standby. When enabled, you can use these devices to bring your computer out of standby, even remotely. You can enable this by selecting the device in Device Manager , opening its properties, going to the Power Management tab, and then checking the Allow this device to wake the computer box. Powercfg /lastwake will show you what device last woke your PC from a sleep state. You can use this command to troubleshoot your PC if it seems to wake from sleep at random. You can use the powercfg /energy command to build a detailed power consumption report for your PC. The report saves to the directory indicated after the command finishes. This report will let you know of any system faults that might increase power consumption, like devices blocking certain sleep modes, or poorly configured to respond to your power management settings. Windows 8 added powercfg /batteryreport , which provides a detailed analysis of battery use, if applicable. Normally output to your Windows user directory, the report provides details about the time and length of charge and discharge cycles, lifetime average battery life, and estimated battery capacity.","title":"Powercfg"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#shutdown","text":"Windows 8 introduced the shutdown command that, you guessed it, shuts down your computer . This is, of course, redundant with the already easily accessed shutdown button, but what's not redundant is the shutdown /r /o command, which restarts your PC and launches the Advanced Start Options menu, which is where you can access Safe Mode and Windows recovery utilities. This is useful if you want to restart your computer for troubleshooting purposes.","title":"Shutdown"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#systeminfo","text":"This command will give you a detailed configuration overview of your computer. The list covers your operating system and hardware. For example, you can look up the original Windows installation date, the last boot time, your BIOS version, total and available memory, installed hotfixes, network card configurations, and more. Use systeminfo /s followed by the host name of a computer on your local network, to remotely grab the information for that system. This may require additional syntax elements for the domain, user name, and password, like this: systeminfo / s [ host_name ] / u [ domain ] \\ [ user_name ] / p [ user_password ]","title":"Systeminfo"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#system-file-checker","text":"System File Checker is an automatic scan and repair tool that focuses on Windows system files. You will need to run the command prompt with administrator privileges and enter the command sfc /scannow . If SFC finds any corrupt or missing files, it will automatically replace them using cached copies kept by Windows for this purpose alone. The command can require a half-hour to run on older notebooks.","title":"System File Checker"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#tasklist","text":"You can use the tasklist command to provide a current list of all tasks running on your PC. Though somewhat redundant with Task Manager, the command may sometimes find tasks hidden from view in that utility. There's also a wide range of modifiers. Tasklist -svc shows services related to each task, use tasklist -v to obtain more detail on each task, and tasklist -m will locate DLL files associated with active tasks. These commands are useful for advanced troubleshooting. Our reader Eric noted that you can \"get the name of the executable associated with the particular process ID you're interested in.\" The command for that operation is tasklist | find [process id].","title":"Tasklist"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#taskkill","text":"Tasks that appear in the tasklist command will have an executable and process ID (a four- or five-digit number) associated with them. You can force stop a program using taskkill -im followed by the executable's name, or taskkill -pid followed by the process ID. Again, this is a bit redundant with Task Manager, but you can use it to kill otherwise unresponsive or hidden programs.","title":"Taskkill"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#chkdsk","text":"Windows automatically marks your drive for a diagnostic chkdsk scan when symptoms indicate that a local drive has bad sectors, lost clusters, or other logical or physical errors. If you suspect your hard drive is failing, you can manually initiate a scan. The most basic command is chkdsk c: , which will immediately scan the C: drive, without a need to restart the computer. If you add parameters like /f, /r, /x, or /b, such as in chkdsk /f /r /x /b c: , chkdsk will also fix errors, recover data, dismount the drive, or clear the list of bad sectors, respectively. These actions require a reboot, as they can only run with Windows powered down. If you see chkdsk run at startup, let it do its thing. If it gets stuck, however, refer to the chkdsk troubleshooting article .","title":"Chkdsk"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#schtasks","text":"Schtasks is your command prompt access to the Task Scheduler, one of many underrated Windows administrative tools. While you can use the GUI to manage your scheduled tasks, the command prompt lets you copy&paste complex commands to set up multiple similar tasks without having to click through various options. Ultimately, it's much easier to use, once you've committed key parameters to memory. For example, you could schedule your computer to reboot at 11pm every Friday: schtasks /create /sc weekly /d FRI /tn \"auto reboot computer weekly\" /st 23:00 /tr \"shutdown -r -f -t 10\" To complement your weekly reboot, you could schedule tasks to launch specific programs on startup: schtasks /create /sc onstart /tn \"launch Chrome on startup\" /tr \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\Chrome.exe\" To duplicate the above command for different programs, just copy, paste, and modify it as needed.","title":"schtasks"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#command-and-conquer-your-windows-pc","text":"This article can only give you a taste of what's hidden within the Windows command line. When including all variables, there are literally hundreds of commands. Download Microsoft's command line reference guide (in Edge or Internet Explorer) for advanced support and troubleshooting. Tired of the command prompt? Time to try the new Windows Terminal !","title":"Command and Conquer Your Windows PC"},{"location":"_daily/2020-04-20/","text":"<< [[2021-04-19]] | 2021-04-21 >> Tuesday, April 2021 \u00b6 An aspirational diet will have you dreaming of success; but it's the attachment of expectations and tangible goals that feeds the desire, persistence, and fortitude required to make the win. \u2014 Lorii Myers \u2714\ufe0f Todo: \u00b6 [ ] Powwater: Implement new delivery fee pricing table from Ellie (see emails): Link to GitHub Issue [x] Adminportal: Add new data_prep R script, SQL creation file, and CSV lookup for the pricing table [ ] Add a new field to orders table showing the delivery fee tier associated with that order [ ] Add a new field to orders table for order type fees (i.e. Refill orders are X amount more than New orders) [ ] Create new tab in Adminportal that displays delivery fee tiers in a table: Columns: Tier (i.e. interval in Kilometers) New (Base) Fee Refill Fee (should be a surcharge - use shinyFeedback ) Swap Fee (should be a discount - use shinyFeedback ) Rows: Need to discuss this further with client - do we need alternative tables between vendors? If so should add vendor_uid to table. Add buttons to make table and data editable and allow addition of new tiers/removal of old tiers Display historical order route distance and time information as reference for pricing the tiers (see vendor's app) \ud83d\udcdd Notes: \u00b6","title":"2020 04 20"},{"location":"_daily/2020-04-20/#tuesday-april-2021","text":"An aspirational diet will have you dreaming of success; but it's the attachment of expectations and tangible goals that feeds the desire, persistence, and fortitude required to make the win. \u2014 Lorii Myers","title":"Tuesday, April 2021"},{"location":"_daily/2020-04-20/#todo","text":"[ ] Powwater: Implement new delivery fee pricing table from Ellie (see emails): Link to GitHub Issue [x] Adminportal: Add new data_prep R script, SQL creation file, and CSV lookup for the pricing table [ ] Add a new field to orders table showing the delivery fee tier associated with that order [ ] Add a new field to orders table for order type fees (i.e. Refill orders are X amount more than New orders) [ ] Create new tab in Adminportal that displays delivery fee tiers in a table: Columns: Tier (i.e. interval in Kilometers) New (Base) Fee Refill Fee (should be a surcharge - use shinyFeedback ) Swap Fee (should be a discount - use shinyFeedback ) Rows: Need to discuss this further with client - do we need alternative tables between vendors? If so should add vendor_uid to table. Add buttons to make table and data editable and allow addition of new tiers/removal of old tiers Display historical order route distance and time information as reference for pricing the tiers (see vendor's app)","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2020-04-20/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-21/","text":"<< [[2021-04-20]] | 2021-04-22 >> Wednesday, April 2021 \u00b6 If you're walking down the right path and you're willing to keep walking, eventually you'll make progress. \u2014 Barack Obama \u2714\ufe0f Todo: \u00b6 Leftover from Tuesday, April 2021: \u00b6 [ ] Finish delivery fee pricing table's implementation [ ] Add to adminportal for \"default\" powwater delivery fee tiers [ ] Add to vendor dashboard (non-editable for all vendors except Dutch Water) [ ] NOTE: Dutch Water - all deliveries are free of charge [ ] \ud83d\udcdd Notes: \u00b6","title":"2021 04 21"},{"location":"_daily/2021-04-21/#wednesday-april-2021","text":"If you're walking down the right path and you're willing to keep walking, eventually you'll make progress. \u2014 Barack Obama","title":"Wednesday, April 2021"},{"location":"_daily/2021-04-21/#todo","text":"","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-21/#leftover-from-tuesday-april-2021","text":"[ ] Finish delivery fee pricing table's implementation [ ] Add to adminportal for \"default\" powwater delivery fee tiers [ ] Add to vendor dashboard (non-editable for all vendors except Dutch Water) [ ] NOTE: Dutch Water - all deliveries are free of charge [ ]","title":"Leftover from Tuesday, April 2021:"},{"location":"_daily/2021-04-21/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-22/","text":"<< 2021-04-21 | 2021-04-23 >> Thursday, April 2021 \u00b6 Feeling grateful to or appreciative of someone or something in your life actually attracts more of the things that you appreciate and value into your life. \u2014 Christiane Northrup \u2714\ufe0f Todo: \u00b6 [ ] \ud83d\udcdd Notes: \u00b6","title":"2021 04 22"},{"location":"_daily/2021-04-22/#thursday-april-2021","text":"Feeling grateful to or appreciative of someone or something in your life actually attracts more of the things that you appreciate and value into your life. \u2014 Christiane Northrup","title":"Thursday, April 2021"},{"location":"_daily/2021-04-22/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-22/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-23/","text":"<< 2021-04-22 | 2021-04-24 >> Friday, April 2021 \u00b6 If you're not making mistakes, you're not taking risks, and that means you're not going anywhere. The key is to make mistakes faster than the competition, so you have more changes to learn and win. \u2014 John W. Holt, Jr. \u2714\ufe0f Todo: \u00b6 [ ] \ud83d\udcdd Notes: \u00b6","title":"2021 04 23"},{"location":"_daily/2021-04-23/#friday-april-2021","text":"If you're not making mistakes, you're not taking risks, and that means you're not going anywhere. The key is to make mistakes faster than the competition, so you have more changes to learn and win. \u2014 John W. Holt, Jr.","title":"Friday, April 2021"},{"location":"_daily/2021-04-23/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-23/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-24/","text":"<< 2021-04-23 | 2021-04-25 >> Saturday, April 2021 \u00b6 When a man is an upright contender, only real people are bound to be around; but if he is a downright pretender, then fake people surround and always abound to be found. \u2014 Anuj Somany \u2714\ufe0f Todo: \u00b6 [ ] \ud83d\udcdd Notes: \u00b6","title":"2021 04 24"},{"location":"_daily/2021-04-24/#saturday-april-2021","text":"When a man is an upright contender, only real people are bound to be around; but if he is a downright pretender, then fake people surround and always abound to be found. \u2014 Anuj Somany","title":"Saturday, April 2021"},{"location":"_daily/2021-04-24/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-24/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-25/","text":"<< 2021-04-24 | [[2021-04-26]] >> Sunday, April 2021 \u00b6 Quote of the Day #quotes \u00b6 Named must your fear be before banish it you can. \u2014 Yoda [[Saved Quotes]] \u2714\ufe0f Todo: \u00b6 [[Powwater API]] : \u00b6 Tags: #pow #api Tasks: [ ] Review all code in plumber.R and refactor accordingly - think MVP, speed, no unnecessary enhancements just because they seem cool and worth learning. [ ] Replace all [[R-Packages: DBI| DBI]] DBI::dbConnect connection conn objects with [[R-Packages: pool| Pool]] (i.e. pool::poolCreate , pool::poolCheckout , etc.) [ ] Review Transactions for Database ensuring are working properly though unit tests. [ ] Implement full test suite against API endpoints and authentication filters. [ ] Create a /customers/<customer_uid>/locations endpoint to GET (retrieve), POST (add new), and PUT (update existing) REST Methods. [ ] Limit all payloads sent between requests and responses to the minimum amount of data necessary and collaborate with Patrick on how to optimize these responses with additional restructuring \ud83d\udcdd Notes: \u00b6","title":"2021 04 25"},{"location":"_daily/2021-04-25/#sunday-april-2021","text":"","title":"Sunday, April 2021"},{"location":"_daily/2021-04-25/#quote-of-the-day-quotes","text":"Named must your fear be before banish it you can. \u2014 Yoda [[Saved Quotes]]","title":"Quote of the Day #quotes"},{"location":"_daily/2021-04-25/#todo","text":"","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-25/#powwater-api","text":"Tags: #pow #api Tasks: [ ] Review all code in plumber.R and refactor accordingly - think MVP, speed, no unnecessary enhancements just because they seem cool and worth learning. [ ] Replace all [[R-Packages: DBI| DBI]] DBI::dbConnect connection conn objects with [[R-Packages: pool| Pool]] (i.e. pool::poolCreate , pool::poolCheckout , etc.) [ ] Review Transactions for Database ensuring are working properly though unit tests. [ ] Implement full test suite against API endpoints and authentication filters. [ ] Create a /customers/<customer_uid>/locations endpoint to GET (retrieve), POST (add new), and PUT (update existing) REST Methods. [ ] Limit all payloads sent between requests and responses to the minimum amount of data necessary and collaborate with Patrick on how to optimize these responses with additional restructuring","title":"[[Powwater API]] :"},{"location":"_daily/2021-04-25/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-28/","text":"<< [[2021-04-27]] | 2021-04-29 >> Wednesday, April 2021 \u00b6 The determination to win is the better part of winning. \u2014 Daisaku Ikeda \u2714\ufe0f Todo: \u00b6 [ ] \ud83d\udcdd Notes: \u00b6","title":"2021 04 28"},{"location":"_daily/2021-04-28/#wednesday-april-2021","text":"The determination to win is the better part of winning. \u2014 Daisaku Ikeda","title":"Wednesday, April 2021"},{"location":"_daily/2021-04-28/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-28/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-29/","text":"<< 2021-04-28 | [[2021-04-30]] >> Thursday, April 2021 \u00b6 The best way to not feel hopeless is to get up and do something. Don't wait for good things to happen to you. If you go out and make some good things happen, you will fill the world with hope, you will fill yourself with hope. \u2014 Barack Obama \u2714\ufe0f Todo: \u00b6 [ ] Review powpolished authentication/registration/login process and database schema with Pat: Questions: \u00b6 How does accounts table and API use the polished_key and hashed_polished_key \ud83d\udcdd Notes: \u00b6","title":"2021 04 29"},{"location":"_daily/2021-04-29/#thursday-april-2021","text":"The best way to not feel hopeless is to get up and do something. Don't wait for good things to happen to you. If you go out and make some good things happen, you will fill the world with hope, you will fill yourself with hope. \u2014 Barack Obama","title":"Thursday, April 2021"},{"location":"_daily/2021-04-29/#todo","text":"[ ] Review powpolished authentication/registration/login process and database schema with Pat:","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-29/#questions","text":"How does accounts table and API use the polished_key and hashed_polished_key","title":"Questions:"},{"location":"_daily/2021-04-29/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-05-01/","text":"<< [[2021-04-30]] | 2021-05-02 >> Saturday, May 2021 \u00b6 At that point where you have decided to upgrade from aspiration to expectation and have begun to visualize an outcome, something incredibly important has happened, you have committed to the process of change. \u2014 Lorii Myers \u2714\ufe0f Todo: \u00b6 [ ] \ud83d\udcdd Notes: \u00b6","title":"2021 05 01"},{"location":"_daily/2021-05-01/#saturday-may-2021","text":"At that point where you have decided to upgrade from aspiration to expectation and have begun to visualize an outcome, something incredibly important has happened, you have committed to the process of change. \u2014 Lorii Myers","title":"Saturday, May 2021"},{"location":"_daily/2021-05-01/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-05-01/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-05-02/","text":"<< 2021-05-01 | [[2021-05-03]] >> Sunday, May 2021 \u00b6 Whether you think you can or think you can't, you're right.. \u2014 Henry Ford \u2714\ufe0f Todo: \u00b6 [ ] \ud83d\udcdd Notes: \u00b6","title":"2021 05 02"},{"location":"_daily/2021-05-02/#sunday-may-2021","text":"Whether you think you can or think you can't, you're right.. \u2014 Henry Ford","title":"Sunday, May 2021"},{"location":"_daily/2021-05-02/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-05-02/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_templates/daily/","text":"<< 2021-05-01 | [[2021-05-03]] >> Sunday, May 2021 \u00b6 Whether you think you can or think you can't, you're right.. \u2014 Henry Ford \u2714\ufe0f Todo: \u00b6 [ ] \ud83d\udcdd Notes: \u00b6","title":"Daily"},{"location":"_templates/daily/#sunday-may-2021","text":"Whether you think you can or think you can't, you're right.. \u2014 Henry Ford","title":"Sunday, May 2021"},{"location":"_templates/daily/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_templates/daily/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_templates/default/","text":"moc \u00b6 Links: Source:","title":"moc"},{"location":"_templates/default/#moc","text":"Links: Source:","title":"moc"},{"location":"_templates/moc/","text":"moc- Contents \u00b6 Contents: \u00b6 Backlinks: Sources:","title":"moc- Contents"},{"location":"_templates/moc/#moc-contents","text":"","title":"moc- Contents"},{"location":"_templates/moc/#contents","text":"Backlinks: Sources:","title":"Contents:"}]}