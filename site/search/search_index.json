{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u2691 Categories \u2691 Cloud Databases Docker Documentation Git Github Obsidian PKM Powwater Productivity Project Management Python R System Design Tools WSL Web Development Windows Daily Notes Documents \u2691 Changelog setup","title":"Home"},{"location":"#welcome","text":"","title":"Welcome"},{"location":"#categories","text":"Cloud Databases Docker Documentation Git Github Obsidian PKM Powwater Productivity Project Management Python R System Design Tools WSL Web Development Windows Daily Notes","title":"Categories"},{"location":"#documents","text":"Changelog setup","title":"Documents"},{"location":"Changelog/","text":"2021-05-05 at 00h02 \u00b7 Firefox Developer Edition 2021-05-05 at 00h02 \u00b7 Web Browsers 2021-05-05 at 00h02 \u00b7 APIs 2021-05-05 at 00h02 \u00b7 index 2021-05-05 at 00h02 \u00b7 README 2021-05-05 at 00h02 \u00b7 index 2021-05-04 at 23h58 \u00b7 Using Pool 2021-05-04 at 20h23 \u00b7 Setup Cloud SQL for PostgreSQL in Production 2021-05-04 at 20h04 \u00b7 Install gcloud SDK on Windows 2021-05-04 at 20h04 \u00b7 Install gcloud SDK on Ubuntu","title":"Changelog"},{"location":"setup/","text":"Setup \u2699\ufe0f \u2691 MkDocs \u2691 pip install mkdocs mkdocs build mkdocs serve mkdocs gh-deploy Plugins \u2691 Plugin Version mkdocs 1.1.2 mkdocs-autolinks-plugin 0.4.0 mkdocs-awesome-pages-plugin 2.5.0 mkdocs-git-revision-date-localized-plugin 0.9.2 mkdocs-minify-plugin 0.4.0 mkdocs-monorepo-plugin 0.4.14 mkdocs-roamlinks-plugin 0.1.3 Installations \u2691 $ pip install mkdocs / mkdocs-autolinks-plugin / mkdocs-awesome-pages-plugin / mkdocs-git-revision-date-localized-plugin / mkdocs-monorepo-plugin / mkdocs-roamlinks-plugin / mkdocs-minify-plugin $ pip list | grep mkdocs -* mkdocs 1 . 1 . 2 mkdocs-autolinks-plugin 0 . 4 . 0 mkdocs-awesome-pages-plugin 2 . 5 . 0 mkdocs-git-revision-date-localized-plugin 0 . 9 . 2 mkdocs-minify-plugin 0 . 4 . 0 mkdocs-monorepo-plugin 0 . 4 . 14 mkdocs-roamlinks-plugin 0 . 1 . 3 Configuration \u2691 plugins : - search - autolinks - roamlinks - awesome-pages - git-revision-date-localized : type : timeago - minify : minify_html : true markdown_extensions : - toc : permalink : \u2691 baselevel : 2 - codehilite : linenums : false guess_lang : true - footnotes - abbr - admonition - meta - def_list","title":"Setup"},{"location":"setup/#setup","text":"","title":"Setup \u2699\ufe0f"},{"location":"setup/#mkdocs","text":"pip install mkdocs mkdocs build mkdocs serve mkdocs gh-deploy","title":"MkDocs"},{"location":"setup/#plugins","text":"Plugin Version mkdocs 1.1.2 mkdocs-autolinks-plugin 0.4.0 mkdocs-awesome-pages-plugin 2.5.0 mkdocs-git-revision-date-localized-plugin 0.9.2 mkdocs-minify-plugin 0.4.0 mkdocs-monorepo-plugin 0.4.14 mkdocs-roamlinks-plugin 0.1.3","title":"Plugins"},{"location":"setup/#installations","text":"$ pip install mkdocs / mkdocs-autolinks-plugin / mkdocs-awesome-pages-plugin / mkdocs-git-revision-date-localized-plugin / mkdocs-monorepo-plugin / mkdocs-roamlinks-plugin / mkdocs-minify-plugin $ pip list | grep mkdocs -* mkdocs 1 . 1 . 2 mkdocs-autolinks-plugin 0 . 4 . 0 mkdocs-awesome-pages-plugin 2 . 5 . 0 mkdocs-git-revision-date-localized-plugin 0 . 9 . 2 mkdocs-minify-plugin 0 . 4 . 0 mkdocs-monorepo-plugin 0 . 4 . 14 mkdocs-roamlinks-plugin 0 . 1 . 3","title":"Installations"},{"location":"setup/#configuration","text":"plugins : - search - autolinks - roamlinks - awesome-pages - git-revision-date-localized : type : timeago - minify : minify_html : true markdown_extensions : - toc : permalink : \u2691 baselevel : 2 - codehilite : linenums : false guess_lang : true - footnotes - abbr - admonition - meta - def_list","title":"Configuration"},{"location":"Cloud/","text":"Cloud \u2691 Categories \u2691 Documents \u2691 Google Cloud Setup Notes","title":"Cloud"},{"location":"Cloud/#cloud","text":"","title":"Cloud"},{"location":"Cloud/#categories","text":"","title":"Categories"},{"location":"Cloud/#documents","text":"Google Cloud Setup Notes","title":"Documents"},{"location":"Cloud/Google%20Cloud%20Setup%20Notes/","text":"Google Cloud Setup Notes \u2691 Environment Setup and Configuration \u2691 In the Google Cloud Console , select or create a GCP project. Project Selector Page Ensure Billing is Enabled for the project. Enable the Cloud Run API . Install and Initialize gcloud SDK on local machine. On windows run, cinst gcloud if using Chocolatey. Update components via gcloud components update Authenticate GCP (two methods): Using a dedicated service account Links: Source:","title":"Google Cloud Setup Notes"},{"location":"Cloud/Google%20Cloud%20Setup%20Notes/#google-cloud-setup-notes","text":"","title":"Google Cloud Setup Notes"},{"location":"Cloud/Google%20Cloud%20Setup%20Notes/#environment-setup-and-configuration","text":"In the Google Cloud Console , select or create a GCP project. Project Selector Page Ensure Billing is Enabled for the project. Enable the Cloud Run API . Install and Initialize gcloud SDK on local machine. On windows run, cinst gcloud if using Chocolatey. Update components via gcloud components update Authenticate GCP (two methods): Using a dedicated service account Links: Source:","title":"Environment Setup and Configuration"},{"location":"Databases/","text":"Databases \u2691 Categories \u2691 Documents \u2691 Database GUIs Databases PostgreSQL Tools PostgreSQL Views vs. Materialized Views","title":"Databases"},{"location":"Databases/#databases","text":"","title":"Databases"},{"location":"Databases/#categories","text":"","title":"Categories"},{"location":"Databases/#documents","text":"Database GUIs Databases PostgreSQL Tools PostgreSQL Views vs. Materialized Views","title":"Documents"},{"location":"Databases/Database%20GUIs/","text":"Database GUIs \u2691 pgAdmin4 DBeaver Valentina Studio Beekeeper Studio DBTarzan DB Browser (SQLite) DataGrip MySQL Workbench PostGUI PostgreSQL Specific \u2691 \u2b50 = Recommended Adminer - Full-featured database management tool written in PHP. Beekeeper Studio - Free and open source SQL client with a modern UI and great Postgres support. Cross platform. \u2b50 DataGrip - IDE with advanced tool sets and good cross-platform experience (Commercial Software).\u2b50 Datazenit - Web-based PostgreSQL GUI (Commercial Software). DataRow - Cross-platform SQL Client for Amazon Redshift: Simple, Effortless, Extensible. DBeaver - Universal Database Manager with excellent support for PostgreSQL.\u2b50 dbglass - Cross-platform desktop client for PostgreSQL, built with Electron. Holistics - Online cross platform database management tool and SQL query reporting GUI with strong PostgreSQL support (Commercial Software). JackDB - Web-based SQL query interface (Commercial Software). Metabase - Simple dashboards, charts and query tool for PostgreSQL. Numeracy - Fast SQL editor with charts and dashboards for PostgreSQL (Commercial Software). OmniDB - Open Source Collaborative Environment For Database Management pgAdmin - PostgreSQL Administration and Management GUI.\u2b50 pgModeler - pgModeler is an open-source PostgreSQL Database Modeler. pgweb - Web-based PostgreSQL database browser written in Go.\u2b50 phpPgAdmin - The Premier Web Based Administration Tool for PostgreSQL. Postbird - PostgreSQL Client for macOS. PostgresCompare - Cross-platform database comparison and deployment tool (Commercial Software). Postico - Modern PostgreSQL Client for macOS (Commercial Software). PSequel - Clean and simple interface to perform common PostgreSQL tasks quickly (Commercial Software). SQL Tabs - Cross Platform Desktop Client for PostgreSQL written in JS. SQLPro for Postgres - Simple, powerful PostgreSQL manager for macOS (Commercial Software). temBoard - Web-based PostgreSQL GUI and monitoring. TablePlus - Native App which let you edit database and structure. High-end security ensured (Commercial Software). Valentina Studio - Cross-platform database administration tool (Free/Commercial)\u2b50 Links: Source:","title":"Database GUIs"},{"location":"Databases/Database%20GUIs/#database-guis","text":"pgAdmin4 DBeaver Valentina Studio Beekeeper Studio DBTarzan DB Browser (SQLite) DataGrip MySQL Workbench PostGUI","title":"Database GUIs"},{"location":"Databases/Database%20GUIs/#postgresql-specific","text":"\u2b50 = Recommended Adminer - Full-featured database management tool written in PHP. Beekeeper Studio - Free and open source SQL client with a modern UI and great Postgres support. Cross platform. \u2b50 DataGrip - IDE with advanced tool sets and good cross-platform experience (Commercial Software).\u2b50 Datazenit - Web-based PostgreSQL GUI (Commercial Software). DataRow - Cross-platform SQL Client for Amazon Redshift: Simple, Effortless, Extensible. DBeaver - Universal Database Manager with excellent support for PostgreSQL.\u2b50 dbglass - Cross-platform desktop client for PostgreSQL, built with Electron. Holistics - Online cross platform database management tool and SQL query reporting GUI with strong PostgreSQL support (Commercial Software). JackDB - Web-based SQL query interface (Commercial Software). Metabase - Simple dashboards, charts and query tool for PostgreSQL. Numeracy - Fast SQL editor with charts and dashboards for PostgreSQL (Commercial Software). OmniDB - Open Source Collaborative Environment For Database Management pgAdmin - PostgreSQL Administration and Management GUI.\u2b50 pgModeler - pgModeler is an open-source PostgreSQL Database Modeler. pgweb - Web-based PostgreSQL database browser written in Go.\u2b50 phpPgAdmin - The Premier Web Based Administration Tool for PostgreSQL. Postbird - PostgreSQL Client for macOS. PostgresCompare - Cross-platform database comparison and deployment tool (Commercial Software). Postico - Modern PostgreSQL Client for macOS (Commercial Software). PSequel - Clean and simple interface to perform common PostgreSQL tasks quickly (Commercial Software). SQL Tabs - Cross Platform Desktop Client for PostgreSQL written in JS. SQLPro for Postgres - Simple, powerful PostgreSQL manager for macOS (Commercial Software). temBoard - Web-based PostgreSQL GUI and monitoring. TablePlus - Native App which let you edit database and structure. High-end security ensured (Commercial Software). Valentina Studio - Cross-platform database administration tool (Free/Commercial)\u2b50 Links: Source:","title":"PostgreSQL Specific"},{"location":"Databases/Databases/","text":"Databases - MOC \u2691 Database Engines \u2691 PostgreSQL Database Design \u2691 Views vs. Materialized Views Tools \u2691 PostgreSQL Tools Backlinks: Sources:","title":"Databases - MOC"},{"location":"Databases/Databases/#databases-moc","text":"","title":"Databases - MOC"},{"location":"Databases/Databases/#database-engines","text":"PostgreSQL","title":"Database Engines"},{"location":"Databases/Databases/#database-design","text":"Views vs. Materialized Views","title":"Database Design"},{"location":"Databases/Databases/#tools","text":"PostgreSQL Tools Backlinks: Sources:","title":"Tools"},{"location":"Databases/PostgreSQL%20Tools/","text":"PostgreSQL Tools \u2691 PostgreSQL Engine \u2691 PostgreSQL: The world's most advanced open source database PostgreSQL: About PostgreSQL: Documentation: 13: PostgreSQL 13.2 Documentation PostgreSQL: Downloads PostgreSQL: Software Catalogue - Administration/development tools CLI \u2691 pgcli - Postgres CLI with autocompletion and syntax highlighting pgsh - Branch your PostgreSQL Database like Git psql - The built-in PostgreSQL CLI client psql2csv - Run a query in psql and output the result as CSV nancy - The Nancy CLI is a unified way to manage automated database experiments either in clouds or on-premise schemaspy - SchemaSpy is a JAVA JDBC-compliant tool for generating your database to HTML documentation, including Entity Relationship diagrams Monitoring \u2691 check_pgactivity - check_pgactivity is designed to monitor PostgreSQL clusters from Nagios. It offers many options to measure and monitor useful performance metrics. Check_postgres - Nagios check_postgres plugin for checking status of PostgreSQL databases. Instrumental - Real-time performance monitoring, including pre-made graphs for ease of setup (Commercial Software) libzbxpgsql - Comprehensive PostgreSQL monitoring module for Zabbix. PMM - Percona Monitoring and Management (PMM) is a Free and Open Source platform for monitoring and managing PostgreSQL, MySQL, and MongoDB. Pome - Pome stands for PostgreSQL Metrics. Pome is a PostgreSQL Metrics Dashboard to keep track of the health of your database. pgmetrics - pgmetrics is an open-source, zero-dependency, single-binary tool that can collect a lot of information and statistics from a running PostgreSQL server and display it in easy-to-read text format or export it as JSON and CSV for scripting. pg_view - Open-source command-line tool that shows global system stats, per-partition information, memory stats and other information. pgwatch2 - Flexible and easy to get started PostgreSQL metrics monitor focusing on Grafana dashboards. pgbench - Run a benchmark test on PostgreSQL. opm.io - Open PostgreSQL Monitoring is a free software suite designed to help you manage your PostgreSQL servers. It can gather stats, display dashboards and send warnings when something goes wrong. okmeter.io - Commercial SaaS agent-based monitoring with a very detailed PostgreSQL plugin. It automatically gathers 100s of stats, displays dashboards on every aspect and sends alerts when something goes wrong (Commercial Software). GUI \u2691 dbeaver - Valentina Studio - pgAdmin4 Dbeaver Beekeeper Studio Valentina Studio DBTarzan Developer Tools \u2691 pgHero pgSync postgres-ai/database-lab DBML, dbdocs.io, dbdiagram.io PostgREST postGIS WAL-G: wal-g/wal-g: Archival and Restoration for Postgres (github.com) Docker \u2691 postgres (docker.com) postgres/Dockerfile postgresai/extended-postgres (docker.com) docker pull postgresai/extended-postgres postgresai/sync-instance (docker.com) docker pull postgresai / sync-instances docker run \\ - -name sync_instance \\ - -env PGDATA =/ var / lib / postgresql / pgdata \\ - -env WALG_GS_PREFIX = \"gs://{BUCKET}/{SCOPE}\" \\ - -env GOOGLE_APPLICATION_CREDENTIALS = \"/etc/sa/credentials.json\" \\ - -volume { PATH_TO_CREDENTIALS } : / etc / sa / credentials . json \\ - -volume / var / lib / dblab / data : / var / lib / postgresql / pgdata : rshared \\ - -detach \\ postgresai / sync-instance : 13 postgrest/postgrest (docker.com) postgrestoauth/api (docker.com) PostgREST Documentation \u2014 PostgREST 7.0.1 documentation Extensions \u2691 - Table of Contents \u2014 pgRouting Manual (3.1) \u2691 Links: Source:","title":"PostgreSQL Tools"},{"location":"Databases/PostgreSQL%20Tools/#postgresql-tools","text":"","title":"PostgreSQL Tools"},{"location":"Databases/PostgreSQL%20Tools/#postgresql-engine","text":"PostgreSQL: The world's most advanced open source database PostgreSQL: About PostgreSQL: Documentation: 13: PostgreSQL 13.2 Documentation PostgreSQL: Downloads PostgreSQL: Software Catalogue - Administration/development tools","title":"PostgreSQL Engine"},{"location":"Databases/PostgreSQL%20Tools/#cli","text":"pgcli - Postgres CLI with autocompletion and syntax highlighting pgsh - Branch your PostgreSQL Database like Git psql - The built-in PostgreSQL CLI client psql2csv - Run a query in psql and output the result as CSV nancy - The Nancy CLI is a unified way to manage automated database experiments either in clouds or on-premise schemaspy - SchemaSpy is a JAVA JDBC-compliant tool for generating your database to HTML documentation, including Entity Relationship diagrams","title":"CLI"},{"location":"Databases/PostgreSQL%20Tools/#monitoring","text":"check_pgactivity - check_pgactivity is designed to monitor PostgreSQL clusters from Nagios. It offers many options to measure and monitor useful performance metrics. Check_postgres - Nagios check_postgres plugin for checking status of PostgreSQL databases. Instrumental - Real-time performance monitoring, including pre-made graphs for ease of setup (Commercial Software) libzbxpgsql - Comprehensive PostgreSQL monitoring module for Zabbix. PMM - Percona Monitoring and Management (PMM) is a Free and Open Source platform for monitoring and managing PostgreSQL, MySQL, and MongoDB. Pome - Pome stands for PostgreSQL Metrics. Pome is a PostgreSQL Metrics Dashboard to keep track of the health of your database. pgmetrics - pgmetrics is an open-source, zero-dependency, single-binary tool that can collect a lot of information and statistics from a running PostgreSQL server and display it in easy-to-read text format or export it as JSON and CSV for scripting. pg_view - Open-source command-line tool that shows global system stats, per-partition information, memory stats and other information. pgwatch2 - Flexible and easy to get started PostgreSQL metrics monitor focusing on Grafana dashboards. pgbench - Run a benchmark test on PostgreSQL. opm.io - Open PostgreSQL Monitoring is a free software suite designed to help you manage your PostgreSQL servers. It can gather stats, display dashboards and send warnings when something goes wrong. okmeter.io - Commercial SaaS agent-based monitoring with a very detailed PostgreSQL plugin. It automatically gathers 100s of stats, displays dashboards on every aspect and sends alerts when something goes wrong (Commercial Software).","title":"Monitoring"},{"location":"Databases/PostgreSQL%20Tools/#gui","text":"dbeaver - Valentina Studio - pgAdmin4 Dbeaver Beekeeper Studio Valentina Studio DBTarzan","title":"GUI"},{"location":"Databases/PostgreSQL%20Tools/#developer-tools","text":"pgHero pgSync postgres-ai/database-lab DBML, dbdocs.io, dbdiagram.io PostgREST postGIS WAL-G: wal-g/wal-g: Archival and Restoration for Postgres (github.com)","title":"Developer Tools"},{"location":"Databases/PostgreSQL%20Tools/#docker","text":"postgres (docker.com) postgres/Dockerfile postgresai/extended-postgres (docker.com) docker pull postgresai/extended-postgres postgresai/sync-instance (docker.com) docker pull postgresai / sync-instances docker run \\ - -name sync_instance \\ - -env PGDATA =/ var / lib / postgresql / pgdata \\ - -env WALG_GS_PREFIX = \"gs://{BUCKET}/{SCOPE}\" \\ - -env GOOGLE_APPLICATION_CREDENTIALS = \"/etc/sa/credentials.json\" \\ - -volume { PATH_TO_CREDENTIALS } : / etc / sa / credentials . json \\ - -volume / var / lib / dblab / data : / var / lib / postgresql / pgdata : rshared \\ - -detach \\ postgresai / sync-instance : 13 postgrest/postgrest (docker.com) postgrestoauth/api (docker.com) PostgREST Documentation \u2014 PostgREST 7.0.1 documentation","title":"Docker"},{"location":"Databases/PostgreSQL%20Tools/#extensions","text":"","title":"Extensions"},{"location":"Databases/PostgreSQL%20Tools/#-table-of-contents-pgrouting-manual-31","text":"Links: Source:","title":"- Table of Contents \u2014 pgRouting Manual (3.1)"},{"location":"Databases/PostgreSQL/","text":"PostgreSQL \u2691 PostgreSQL software, libraries, tools and resources. PostgreSQL , often simply Postgres, is an object-relational database (ORDBMS). PostgreSQL is ACID-compliant and transactional . (see more: wikipedia:PostgreSQL , PostgreSQL.org ). Contents \u2691 High-Availability Backups GUI Distributions CLI Server Monitoring Extensions Optimization Utilities Language bindings PaaS (PostgreSQL as a Service) Docker images Resources Tutorials Blogs Articles Documentation Newsletters Videos Community High-Availability \u2691 BDR - BiDirectional Replication - a multimaster replication system for PostgreSQL Patroni - Template for PostgreSQL HA with ZooKeeper or etcd. Stolon - PostgreSQL HA based on Consul or etcd, with Kubernetes integration. pglookout - Replication monitoring and failover daemon. repmgr - Open-source tool suite to manage replication and failover in a cluster of PostgreSQL servers. Slony-I - \"Master to multiple slaves\" replication system with cascading and failover. PAF - PostgreSQL Automatic Failover: High-Availibility for Postgres, based on Pacemaker and Corosync. SkyTools - Replication tools, including PgQ, a queuing system, and Londiste, a replication system a bit simpler to manage than Slony. Backups \u2691 Barman - Backup and Recovery Manager for PostgreSQL by 2ndQuadrant. OmniPITR - Advanced WAL File Management Tools for PostgreSQL. pg_probackup \u2013 A fork of pg_arman, improved by @PostgresPro, supports incremental backups, backups from replica, multithreaded backup and restore, and anonymous backup without archive command. pgBackRest - Reliable PostgreSQL Backup & Restore. pg_back - pg_back is a simple backup script pghoard - Backup and restore tool for cloud object stores (AWS S3, Azure, Google Cloud, OpenStack Swift). wal-e - Simple Continuous Archiving for PostgreSQL to S3, Azure, or Swift by Heroku. wal-g - The successor of WAL-E rewritten in Go. Currently supports cloud object storage services by AWS (S3), Google Cloud (GCS), Azure, as well as OpenStack Swift, MinIO, and file system storages. Supports block-level incremental backups, offloading backup tasks to a standby server, provides parallelization and throttling options. In addition to Postgres, WAL-G can be used for MySQL and MongoDB databases. pitrery - pitrery is a set of Bash scripts to manage Point In Time Recovery (PITR) backups for PostgreSQL. GUI \u2691 Adminer - Full-featured database management tool written in PHP. Beekeeper Studio - Free and open source SQL client with a modern UI and great Postgres support. Cross platform. DataGrip - IDE with advanced tool sets and good cross-platform experience (Commercial Software). Datazenit - Web-based PostgreSQL GUI (Commercial Software). DataRow - Cross-platform SQL Client for Amazon Redshift: Simple, Effortless, Extensible. DBeaver - Universal Database Manager with excellent support for PostgreSQL. dbglass - Cross-platform desktop client for PostgreSQL, built with Electron. Holistics - Online cross platform database management tool and SQL query reporting GUI with strong PostgreSQL support (Commercial Software). JackDB - Web-based SQL query interface (Commercial Software). Metabase - Simple dashboards, charts and query tool for PostgreSQL. Numeracy - Fast SQL editor with charts and dashboards for PostgreSQL (Commercial Software). OmniDB - Open Source Collaborative Environment For Database Management pgAdmin - PostgreSQL Administration and Management GUI. pgModeler - pgModeler is an open-source PostgreSQL Database Modeler. pgweb - Web-based PostgreSQL database browser written in Go. phpPgAdmin - The Premier Web Based Administration Tool for PostgreSQL. Postbird - PostgreSQL Client for macOS. PostgresCompare - Cross-platform database comparison and deployment tool (Commercial Software). Postico - Modern PostgreSQL Client for macOS (Commercial Software). PSequel - Clean and simple interface to perform common PostgreSQL tasks quickly (Commercial Software). SQL Tabs - Cross Platform Desktop Client for PostgreSQL written in JS. SQLPro for Postgres - Simple, powerful PostgreSQL manager for macOS (Commercial Software). temBoard - Web-based PostgreSQL GUI and monitoring. TablePlus - Native App which let you edit database and structure. High-end security ensured (Commercial Software). Valentina Studio - Cross-platform database administration tool (Free/Commercial) Distributions \u2691 Postgres.app - The Easiest Way to Get Started with PostgreSQL on macOS. PostgreSql.Binaries.Lite - Minimum set of Windows binaries of the PostgreSQL database. Also made available through NuGet. CLI \u2691 pgcli - Postgres CLI with autocompletion and syntax highlighting pgsh - Branch your PostgreSQL Database like Git psql - The built-in PostgreSQL CLI client psql2csv - Run a query in psql and output the result as CSV nancy - The Nancy CLI is a unified way to manage automated database experiments either in clouds or on-premise schemaspy - SchemaSpy is a JAVA JDBC-compliant tool for generating your database to HTML documentation, including Entity Relationship diagrams Server \u2691 Postgres-XL - Scalable Open Source PostgreSQL-based Database Cluster. AgensGraph - Powerful graph database based on the PostgreSQL. Greenplum Database - Open source fork of PostgreSQL for large data volumes. Monitoring \u2691 check_pgactivity - check_pgactivity is designed to monitor PostgreSQL clusters from Nagios. It offers many options to measure and monitor useful performance metrics. Check_postgres - Nagios check_postgres plugin for checking status of PostgreSQL databases. Instrumental - Real-time performance monitoring, including pre-made graphs for ease of setup (Commercial Software) libzbxpgsql - Comprehensive PostgreSQL monitoring module for Zabbix. PMM - Percona Monitoring and Management (PMM) is a Free and Open Source platform for monitoring and managing PostgreSQL, MySQL, and MongoDB. Pome - Pome stands for PostgreSQL Metrics. Pome is a PostgreSQL Metrics Dashboard to keep track of the health of your database. pgmetrics - pgmetrics is an open-source, zero-dependency, single-binary tool that can collect a lot of information and statistics from a running PostgreSQL server and display it in easy-to-read text format or export it as JSON and CSV for scripting. pg_view - Open-source command-line tool that shows global system stats, per-partition information, memory stats and other information. pgwatch2 - Flexible and easy to get started PostgreSQL metrics monitor focusing on Grafana dashboards. pgbench - Run a benchmark test on PostgreSQL. opm.io - Open PostgreSQL Monitoring is a free software suite designed to help you manage your PostgreSQL servers. It can gather stats, display dashboards and send warnings when something goes wrong. okmeter.io - Commercial SaaS agent-based monitoring with a very detailed PostgreSQL plugin. It automatically gathers 100s of stats, displays dashboards on every aspect and sends alerts when something goes wrong (Commercial Software). Extensions \u2691 Citus - Scalable PostgreSQL cluster for real-time workloads. cstore_fdw - Columnar store for analytics with PostgreSQL. cyanaudit - Cyan Audit provides in-database logging of all DML activity on a column-by-column basis. pg_cron - Run periodic jobs in PostgreSQL. pglogical - Extension that provides logical streaming replication. pg_partman - Partition management extension for PostgreSQL. pg_paxos - Basic implementation of Paxos and Paxos-based table replication for a cluster of PostgreSQL nodes. pg_shard - Extension to scale out real-time reads and writes. PGStrom - Extension to offload CPU intensive workloads to GPU. pgxn PostgreSQL Extension Network - central distribution point for many open-source PostgreSQL extensions PipelineDB - A PostgreSQL extension that runs SQL queries continuously on streams, incrementally storing results in tables. plpgsql_check - Extension that allows to check plpgsql source code. PostGIS - Spatial and Geographic objects for PostgreSQL. PG_Themis - Postgres binding as extension for crypto library Themis, providing various security services on PgSQL's side. zomboDB - Extension that enables efficient full-text searching via the use of indexes backed by Elasticsearch. pgMemento - Provides an audit trail for your data inside a PostgreSQL database using triggers and server-side functions written in PL/pgSQL. TimescaleDB - Open-source time-series database fully compatible with Postgres, distributed as extension pgTAP - Database testing framework for Postgres HypoPG - HypoPG provides hypothetical/virtual indexes feature. pgRouting - pgRouting extends the PostGIS/PostgreSQL geospatial database to provide geospatial routing and other network analysis functionality. Optimization \u2691 pg_flame - A flamegraph generator for query plans. PgHero - PostgreSQL insights made easy. pgMustard - A modern user interface for EXPLAIN , that also provides performance tips (Commercial Software). pgtune - PostgreSQL configuration wizard. pgtune - Online version of PostgreSQL configuration wizard. pgconfig.org - PostgreSQL Online Configuration Tool (also based on pgtune). PoWA - PostgreSQL Workload Analyzer gathers performance stats and provides real-time charts and graphs to help monitor and tune your PostgreSQL servers. pg_web_stats - Web UI to view pg_stat_statements. TimescaleDB Tune - a program for tuning a TimescaleDB database to perform its best based on the host's resources such as memory and number of CPUs. Utilities \u2691 apgdiff - Compares two database dump files and creates output with DDL statements that can be used to update old database schema to new one. ERAlchemy - ERAlchemy generates Entity Relation (ER) diagram from databases. Hasura GraphQL Engine - Blazing fast, instant realtime GraphQL APIs on Postgres with fine grained access control, also trigger webhooks on database events. ldap2pg - Synchronize roles and privileges from YML and LDAP. mysql-postgresql-converter - Lanyrd's MySQL to PostgreSQL conversion script. ora2pg - Perl module to export an Oracle database schema to a PostgreSQL compatible schema. pg_activity - top like application for PostgreSQL server activity monitoring. pg-formatter - A PostgreSQL SQL syntax beautifier (Node.js). pganalyze - PostgreSQL Performance Monitoring (Commercial Software). pgbadger - Fast PostgreSQL Log Analyzer. PgBouncer - Lightweight connection pooler for PostgreSQL. pgCenter - Provides convenient interface to various statistics, management task, reloading services, viewing log files and canceling or terminating database backends. pg_chameleon - Real time replica from MySQL to PostgreSQL with optional type override migration and migration capabilities. pgclimb - Export data from PostgreSQL into different data formats. pg_docs_bot - Browser extension to redirect PostgreSQL docs links to the current version. pgfutter - Import CSV and JSON into PostgreSQL the easy way. PGInsight - CLI tool to easily dig deep inside your PostgreSQL database. pg_insights - Convenient SQL for monitoring Postgres database health. pgloader - Loads data into PostgreSQL using the COPY streaming protocol, and does so with separate threads for reading and writing data. pgpool-II - Middleware that provides connection pooling, replication, load balancing and limiting exceeding connections. pgsync - Tool to sync PostgreSQL data to your local machine. PGXN client - Command line tool to interact with the PostgreSQL Extension Network postgresql-metrics - Tool that extracts and provides metrics for your PostgreSQL database. PostgREST - Serves a fully RESTful API from any existing PostgreSQL database. pREST - Serve a RESTful API from any PostgreSQL database (Golang) PostGraphile - Instant GraphQL API or GraphQL schema for your PostgreSQL database yoke - PostgreSQL high-availability cluster with auto-failover and automated cluster recovery. pglistend - A lightweight PostgresSQL LISTEN / NOTIFY daemon built on top of node-postgres . ZSON - PostgreSQL extension for transparent JSONB compression pg_bulkload - It's a high speed data loading utility for PostgreSQL. pg_migrate - Manage PostgreSQL codebases and make VCS simple. sqitch - Tool for managing versioned schema deployment pgmigrate - CLI tool to evolve schema migrations, developed by Yandex. pgcmp - Tool to compare database schemas, with capability to accept some persistent differences pg-differ - Tool for easy initialization / updating of the structure of PostgreSQL tables, migration alternative (Node.js). sqlcheck - Automatically detects common SQL anti-patterns. Such anti-patterns often slow down queries. Addressing them will, therefore, help accelerate queries. postgres-checkup - a new-generation diagnostics tool that allows users to collect deep analysis of the health of a Postgres database. ScaffoldHub.io - Generate fullstack PostgreSQL apps with Angular, Vue or React (Commercial Software). Language bindings \u2691 Common Lisp: Postmodern Clojure: clj-postgresql Elixir: postgrex Go: pq , pgx Haskell: postgresql-simple Java: PostgreSQL JDBC Driver .Net/.Net Core: Npgsql Node: node-postgres , pg-promise , pogi , slonik , postgres Perl: DBD-Pg PHP: Pomm , pecl/pq Python: psycopg2 , asyncpg Ruby: pg Rust: rust-postgresql Lua: luapgsql PaaS (PostgreSQL as a Service) \u2691 Aiven PostgreSQL - PostgreSQL as a service in AWS, Azure, DigitalOcean, Google Cloud and UpCloud; plans range from $19/month single node instances to large highly-available setups, free trial for two weeks. Amazon RDS for PostgreSQL - Amazon Relational Database Service (RDS) for PostgreSQL Azure Database for PostgreSQL - Azure Database for PostgreSQL provides fully managed, enterprise-ready community PostgreSQL database as a service. It provides builtin HA, elastic scaling and native integration with Azure ecosystem. Citus Cloud - Production grade scaled out PostgreSQL as a service enabling real-time workloads and sharding your multi-tenant apps. Compose - PostgreSQL as a service in AWS, Google Cloud Platform, and IBM Cloud; plans range from $17.5/month for 1GB storage and scale at $12/GB beyond that. Free trial for 30 days available. Database Labs - Get a production-ready cloud PostgreSQL server in minutes, from $20 a month Backups, monitoring, patches, and 24/7 tech support all included. DigitalOcean Managed Databases - Fully managed PostgreSQL databases. No free plan. Starting at $15/mo. Daily backups with point-in-time recovery. Standby nodes with auto-failover. ElephantSQL - Offers databases ranging from shared servers for smaller projects and proof of concepts, up to enterprise grade multi server setups. Has free plan for up to 5 DBs, 20 MB each. Google Cloud SQL for PostgreSQL - Fully-managed database service that makes it easy to set up, maintain, manage, and administer your PostgreSQL relational databases on Google Cloud Platform. Heroku Postgres - Plans from free to huge, operated by PostgreSQL experts. Does not require running your app on Heroku. Free plan includes 10,000 rows, 20 connections, up to two backups, and has PostGIS support. Scaleway Managed Database - Fully managed PostgreSQL databases with HA, scaling, and automated backups, hosted in the EU. Starting at \u20ac10 per month. Docker images \u2691 citusdata/citus - Citus official images with citus extensions. Based on the official Postgres container. mdillon/postgis - PostGIS 2.3 on Postgres 9. Based on the official Postgres container. postgres - Official postgres container (from Docker) Resources \u2691 Tutorials \u2691 Backup and recover a PostgreSQL DB using wal-e - Tutorial about setting up continuous archiving in PostgreSQL using wal-e. PG Casts - Free weekly PostgreSQL screencasts by Hashrocket. Postgres Guide - Guide designed as an aid for beginners and experienced users to find specific tips and explore tools available within PostgreSQL. PostgreSQL Exercises - Site to make it easy to learn PostgreSQL by doing exercises. tutorialspoint PostgreSQL tutorial - Very extensive collection of tutorials on PostgreSQL postgresDBSamples - A collection of sample postgres schemas PostgreSQL Primer for Busy People - A collection of the most common commands used in PostgreSQL pg-utils - Useful DBA tools by Data Egret Blogs \u2691 Planet PostgreSQL - Blog aggregation service for PostgreSQL. Andrew Dunstan's PostgreSQL and Technical blog Bruce Momjian's PostgreSQL blog Craig Kerstiens PostgreSQL posts - Set of posts on PostgreSQL cool features, tips and tricks. Database Soup - Josh Berkus' blog. Michael Paquier's blog Robert Haas' blog select * from depesz; - Hubert Lubaczewski's blog. Articles \u2691 What PostgreSQL has over other open source SQL databases: Part I What PostgreSQL has over other open source SQL databases: Part II the ultimate postgres vs mysql blog post Debugging PostgreSQL performance, the hard way Why use Postgres? Superfast CSV imports using PostgreSQL's COPY command Documentation \u2691 Wiki - user documentation, how-tos, and tips 'n' tricks Newsletters \u2691 Postgres Weekly - Weekly newsletter that contains articles, news, and repos relevant to PostgreSQL. Videos \u2691 Citus Data Youtube channel - Citus related videos EnterpriseDB Youtube channel - EnterpriseDB related videos Postgres Conference Youtube channel - Conference videos Scaling Postgres - Postgres video blog series by Creston Jamison Community \u2691 Mailing lists - Official mailing lists for Postgres for support, outreach, and more. One of the primary channels of communication in the Postgres community. Reddit - A reddit community for PostgreSQL users with over 12000 users Slack - Slack channel for Postgres with over 7000 users Telegram - Several groups for PostgreSQL in different langauges: Russian >4200 people, Brazilian Portuguese >2300 people, Indonesian ~1000 people, English >750 people #postgresql on Freenode - The most popular IRC channel about Postgres on Freenode with over 1000 users Links: Source:","title":"PostgreSQL"},{"location":"Databases/PostgreSQL/#postgresql","text":"PostgreSQL software, libraries, tools and resources. PostgreSQL , often simply Postgres, is an object-relational database (ORDBMS). PostgreSQL is ACID-compliant and transactional . (see more: wikipedia:PostgreSQL , PostgreSQL.org ).","title":"PostgreSQL"},{"location":"Databases/PostgreSQL/#contents","text":"High-Availability Backups GUI Distributions CLI Server Monitoring Extensions Optimization Utilities Language bindings PaaS (PostgreSQL as a Service) Docker images Resources Tutorials Blogs Articles Documentation Newsletters Videos Community","title":"Contents"},{"location":"Databases/PostgreSQL/#high-availability","text":"BDR - BiDirectional Replication - a multimaster replication system for PostgreSQL Patroni - Template for PostgreSQL HA with ZooKeeper or etcd. Stolon - PostgreSQL HA based on Consul or etcd, with Kubernetes integration. pglookout - Replication monitoring and failover daemon. repmgr - Open-source tool suite to manage replication and failover in a cluster of PostgreSQL servers. Slony-I - \"Master to multiple slaves\" replication system with cascading and failover. PAF - PostgreSQL Automatic Failover: High-Availibility for Postgres, based on Pacemaker and Corosync. SkyTools - Replication tools, including PgQ, a queuing system, and Londiste, a replication system a bit simpler to manage than Slony.","title":"High-Availability"},{"location":"Databases/PostgreSQL/#backups","text":"Barman - Backup and Recovery Manager for PostgreSQL by 2ndQuadrant. OmniPITR - Advanced WAL File Management Tools for PostgreSQL. pg_probackup \u2013 A fork of pg_arman, improved by @PostgresPro, supports incremental backups, backups from replica, multithreaded backup and restore, and anonymous backup without archive command. pgBackRest - Reliable PostgreSQL Backup & Restore. pg_back - pg_back is a simple backup script pghoard - Backup and restore tool for cloud object stores (AWS S3, Azure, Google Cloud, OpenStack Swift). wal-e - Simple Continuous Archiving for PostgreSQL to S3, Azure, or Swift by Heroku. wal-g - The successor of WAL-E rewritten in Go. Currently supports cloud object storage services by AWS (S3), Google Cloud (GCS), Azure, as well as OpenStack Swift, MinIO, and file system storages. Supports block-level incremental backups, offloading backup tasks to a standby server, provides parallelization and throttling options. In addition to Postgres, WAL-G can be used for MySQL and MongoDB databases. pitrery - pitrery is a set of Bash scripts to manage Point In Time Recovery (PITR) backups for PostgreSQL.","title":"Backups"},{"location":"Databases/PostgreSQL/#gui","text":"Adminer - Full-featured database management tool written in PHP. Beekeeper Studio - Free and open source SQL client with a modern UI and great Postgres support. Cross platform. DataGrip - IDE with advanced tool sets and good cross-platform experience (Commercial Software). Datazenit - Web-based PostgreSQL GUI (Commercial Software). DataRow - Cross-platform SQL Client for Amazon Redshift: Simple, Effortless, Extensible. DBeaver - Universal Database Manager with excellent support for PostgreSQL. dbglass - Cross-platform desktop client for PostgreSQL, built with Electron. Holistics - Online cross platform database management tool and SQL query reporting GUI with strong PostgreSQL support (Commercial Software). JackDB - Web-based SQL query interface (Commercial Software). Metabase - Simple dashboards, charts and query tool for PostgreSQL. Numeracy - Fast SQL editor with charts and dashboards for PostgreSQL (Commercial Software). OmniDB - Open Source Collaborative Environment For Database Management pgAdmin - PostgreSQL Administration and Management GUI. pgModeler - pgModeler is an open-source PostgreSQL Database Modeler. pgweb - Web-based PostgreSQL database browser written in Go. phpPgAdmin - The Premier Web Based Administration Tool for PostgreSQL. Postbird - PostgreSQL Client for macOS. PostgresCompare - Cross-platform database comparison and deployment tool (Commercial Software). Postico - Modern PostgreSQL Client for macOS (Commercial Software). PSequel - Clean and simple interface to perform common PostgreSQL tasks quickly (Commercial Software). SQL Tabs - Cross Platform Desktop Client for PostgreSQL written in JS. SQLPro for Postgres - Simple, powerful PostgreSQL manager for macOS (Commercial Software). temBoard - Web-based PostgreSQL GUI and monitoring. TablePlus - Native App which let you edit database and structure. High-end security ensured (Commercial Software). Valentina Studio - Cross-platform database administration tool (Free/Commercial)","title":"GUI"},{"location":"Databases/PostgreSQL/#distributions","text":"Postgres.app - The Easiest Way to Get Started with PostgreSQL on macOS. PostgreSql.Binaries.Lite - Minimum set of Windows binaries of the PostgreSQL database. Also made available through NuGet.","title":"Distributions"},{"location":"Databases/PostgreSQL/#cli","text":"pgcli - Postgres CLI with autocompletion and syntax highlighting pgsh - Branch your PostgreSQL Database like Git psql - The built-in PostgreSQL CLI client psql2csv - Run a query in psql and output the result as CSV nancy - The Nancy CLI is a unified way to manage automated database experiments either in clouds or on-premise schemaspy - SchemaSpy is a JAVA JDBC-compliant tool for generating your database to HTML documentation, including Entity Relationship diagrams","title":"CLI"},{"location":"Databases/PostgreSQL/#server","text":"Postgres-XL - Scalable Open Source PostgreSQL-based Database Cluster. AgensGraph - Powerful graph database based on the PostgreSQL. Greenplum Database - Open source fork of PostgreSQL for large data volumes.","title":"Server"},{"location":"Databases/PostgreSQL/#monitoring","text":"check_pgactivity - check_pgactivity is designed to monitor PostgreSQL clusters from Nagios. It offers many options to measure and monitor useful performance metrics. Check_postgres - Nagios check_postgres plugin for checking status of PostgreSQL databases. Instrumental - Real-time performance monitoring, including pre-made graphs for ease of setup (Commercial Software) libzbxpgsql - Comprehensive PostgreSQL monitoring module for Zabbix. PMM - Percona Monitoring and Management (PMM) is a Free and Open Source platform for monitoring and managing PostgreSQL, MySQL, and MongoDB. Pome - Pome stands for PostgreSQL Metrics. Pome is a PostgreSQL Metrics Dashboard to keep track of the health of your database. pgmetrics - pgmetrics is an open-source, zero-dependency, single-binary tool that can collect a lot of information and statistics from a running PostgreSQL server and display it in easy-to-read text format or export it as JSON and CSV for scripting. pg_view - Open-source command-line tool that shows global system stats, per-partition information, memory stats and other information. pgwatch2 - Flexible and easy to get started PostgreSQL metrics monitor focusing on Grafana dashboards. pgbench - Run a benchmark test on PostgreSQL. opm.io - Open PostgreSQL Monitoring is a free software suite designed to help you manage your PostgreSQL servers. It can gather stats, display dashboards and send warnings when something goes wrong. okmeter.io - Commercial SaaS agent-based monitoring with a very detailed PostgreSQL plugin. It automatically gathers 100s of stats, displays dashboards on every aspect and sends alerts when something goes wrong (Commercial Software).","title":"Monitoring"},{"location":"Databases/PostgreSQL/#extensions","text":"Citus - Scalable PostgreSQL cluster for real-time workloads. cstore_fdw - Columnar store for analytics with PostgreSQL. cyanaudit - Cyan Audit provides in-database logging of all DML activity on a column-by-column basis. pg_cron - Run periodic jobs in PostgreSQL. pglogical - Extension that provides logical streaming replication. pg_partman - Partition management extension for PostgreSQL. pg_paxos - Basic implementation of Paxos and Paxos-based table replication for a cluster of PostgreSQL nodes. pg_shard - Extension to scale out real-time reads and writes. PGStrom - Extension to offload CPU intensive workloads to GPU. pgxn PostgreSQL Extension Network - central distribution point for many open-source PostgreSQL extensions PipelineDB - A PostgreSQL extension that runs SQL queries continuously on streams, incrementally storing results in tables. plpgsql_check - Extension that allows to check plpgsql source code. PostGIS - Spatial and Geographic objects for PostgreSQL. PG_Themis - Postgres binding as extension for crypto library Themis, providing various security services on PgSQL's side. zomboDB - Extension that enables efficient full-text searching via the use of indexes backed by Elasticsearch. pgMemento - Provides an audit trail for your data inside a PostgreSQL database using triggers and server-side functions written in PL/pgSQL. TimescaleDB - Open-source time-series database fully compatible with Postgres, distributed as extension pgTAP - Database testing framework for Postgres HypoPG - HypoPG provides hypothetical/virtual indexes feature. pgRouting - pgRouting extends the PostGIS/PostgreSQL geospatial database to provide geospatial routing and other network analysis functionality.","title":"Extensions"},{"location":"Databases/PostgreSQL/#optimization","text":"pg_flame - A flamegraph generator for query plans. PgHero - PostgreSQL insights made easy. pgMustard - A modern user interface for EXPLAIN , that also provides performance tips (Commercial Software). pgtune - PostgreSQL configuration wizard. pgtune - Online version of PostgreSQL configuration wizard. pgconfig.org - PostgreSQL Online Configuration Tool (also based on pgtune). PoWA - PostgreSQL Workload Analyzer gathers performance stats and provides real-time charts and graphs to help monitor and tune your PostgreSQL servers. pg_web_stats - Web UI to view pg_stat_statements. TimescaleDB Tune - a program for tuning a TimescaleDB database to perform its best based on the host's resources such as memory and number of CPUs.","title":"Optimization"},{"location":"Databases/PostgreSQL/#utilities","text":"apgdiff - Compares two database dump files and creates output with DDL statements that can be used to update old database schema to new one. ERAlchemy - ERAlchemy generates Entity Relation (ER) diagram from databases. Hasura GraphQL Engine - Blazing fast, instant realtime GraphQL APIs on Postgres with fine grained access control, also trigger webhooks on database events. ldap2pg - Synchronize roles and privileges from YML and LDAP. mysql-postgresql-converter - Lanyrd's MySQL to PostgreSQL conversion script. ora2pg - Perl module to export an Oracle database schema to a PostgreSQL compatible schema. pg_activity - top like application for PostgreSQL server activity monitoring. pg-formatter - A PostgreSQL SQL syntax beautifier (Node.js). pganalyze - PostgreSQL Performance Monitoring (Commercial Software). pgbadger - Fast PostgreSQL Log Analyzer. PgBouncer - Lightweight connection pooler for PostgreSQL. pgCenter - Provides convenient interface to various statistics, management task, reloading services, viewing log files and canceling or terminating database backends. pg_chameleon - Real time replica from MySQL to PostgreSQL with optional type override migration and migration capabilities. pgclimb - Export data from PostgreSQL into different data formats. pg_docs_bot - Browser extension to redirect PostgreSQL docs links to the current version. pgfutter - Import CSV and JSON into PostgreSQL the easy way. PGInsight - CLI tool to easily dig deep inside your PostgreSQL database. pg_insights - Convenient SQL for monitoring Postgres database health. pgloader - Loads data into PostgreSQL using the COPY streaming protocol, and does so with separate threads for reading and writing data. pgpool-II - Middleware that provides connection pooling, replication, load balancing and limiting exceeding connections. pgsync - Tool to sync PostgreSQL data to your local machine. PGXN client - Command line tool to interact with the PostgreSQL Extension Network postgresql-metrics - Tool that extracts and provides metrics for your PostgreSQL database. PostgREST - Serves a fully RESTful API from any existing PostgreSQL database. pREST - Serve a RESTful API from any PostgreSQL database (Golang) PostGraphile - Instant GraphQL API or GraphQL schema for your PostgreSQL database yoke - PostgreSQL high-availability cluster with auto-failover and automated cluster recovery. pglistend - A lightweight PostgresSQL LISTEN / NOTIFY daemon built on top of node-postgres . ZSON - PostgreSQL extension for transparent JSONB compression pg_bulkload - It's a high speed data loading utility for PostgreSQL. pg_migrate - Manage PostgreSQL codebases and make VCS simple. sqitch - Tool for managing versioned schema deployment pgmigrate - CLI tool to evolve schema migrations, developed by Yandex. pgcmp - Tool to compare database schemas, with capability to accept some persistent differences pg-differ - Tool for easy initialization / updating of the structure of PostgreSQL tables, migration alternative (Node.js). sqlcheck - Automatically detects common SQL anti-patterns. Such anti-patterns often slow down queries. Addressing them will, therefore, help accelerate queries. postgres-checkup - a new-generation diagnostics tool that allows users to collect deep analysis of the health of a Postgres database. ScaffoldHub.io - Generate fullstack PostgreSQL apps with Angular, Vue or React (Commercial Software).","title":"Utilities"},{"location":"Databases/PostgreSQL/#language-bindings","text":"Common Lisp: Postmodern Clojure: clj-postgresql Elixir: postgrex Go: pq , pgx Haskell: postgresql-simple Java: PostgreSQL JDBC Driver .Net/.Net Core: Npgsql Node: node-postgres , pg-promise , pogi , slonik , postgres Perl: DBD-Pg PHP: Pomm , pecl/pq Python: psycopg2 , asyncpg Ruby: pg Rust: rust-postgresql Lua: luapgsql","title":"Language bindings"},{"location":"Databases/PostgreSQL/#paas-postgresql-as-a-service","text":"Aiven PostgreSQL - PostgreSQL as a service in AWS, Azure, DigitalOcean, Google Cloud and UpCloud; plans range from $19/month single node instances to large highly-available setups, free trial for two weeks. Amazon RDS for PostgreSQL - Amazon Relational Database Service (RDS) for PostgreSQL Azure Database for PostgreSQL - Azure Database for PostgreSQL provides fully managed, enterprise-ready community PostgreSQL database as a service. It provides builtin HA, elastic scaling and native integration with Azure ecosystem. Citus Cloud - Production grade scaled out PostgreSQL as a service enabling real-time workloads and sharding your multi-tenant apps. Compose - PostgreSQL as a service in AWS, Google Cloud Platform, and IBM Cloud; plans range from $17.5/month for 1GB storage and scale at $12/GB beyond that. Free trial for 30 days available. Database Labs - Get a production-ready cloud PostgreSQL server in minutes, from $20 a month Backups, monitoring, patches, and 24/7 tech support all included. DigitalOcean Managed Databases - Fully managed PostgreSQL databases. No free plan. Starting at $15/mo. Daily backups with point-in-time recovery. Standby nodes with auto-failover. ElephantSQL - Offers databases ranging from shared servers for smaller projects and proof of concepts, up to enterprise grade multi server setups. Has free plan for up to 5 DBs, 20 MB each. Google Cloud SQL for PostgreSQL - Fully-managed database service that makes it easy to set up, maintain, manage, and administer your PostgreSQL relational databases on Google Cloud Platform. Heroku Postgres - Plans from free to huge, operated by PostgreSQL experts. Does not require running your app on Heroku. Free plan includes 10,000 rows, 20 connections, up to two backups, and has PostGIS support. Scaleway Managed Database - Fully managed PostgreSQL databases with HA, scaling, and automated backups, hosted in the EU. Starting at \u20ac10 per month.","title":"PaaS (PostgreSQL as a Service)"},{"location":"Databases/PostgreSQL/#docker-images","text":"citusdata/citus - Citus official images with citus extensions. Based on the official Postgres container. mdillon/postgis - PostGIS 2.3 on Postgres 9. Based on the official Postgres container. postgres - Official postgres container (from Docker)","title":"Docker images"},{"location":"Databases/PostgreSQL/#resources","text":"","title":"Resources"},{"location":"Databases/PostgreSQL/#tutorials","text":"Backup and recover a PostgreSQL DB using wal-e - Tutorial about setting up continuous archiving in PostgreSQL using wal-e. PG Casts - Free weekly PostgreSQL screencasts by Hashrocket. Postgres Guide - Guide designed as an aid for beginners and experienced users to find specific tips and explore tools available within PostgreSQL. PostgreSQL Exercises - Site to make it easy to learn PostgreSQL by doing exercises. tutorialspoint PostgreSQL tutorial - Very extensive collection of tutorials on PostgreSQL postgresDBSamples - A collection of sample postgres schemas PostgreSQL Primer for Busy People - A collection of the most common commands used in PostgreSQL pg-utils - Useful DBA tools by Data Egret","title":"Tutorials"},{"location":"Databases/PostgreSQL/#blogs","text":"Planet PostgreSQL - Blog aggregation service for PostgreSQL. Andrew Dunstan's PostgreSQL and Technical blog Bruce Momjian's PostgreSQL blog Craig Kerstiens PostgreSQL posts - Set of posts on PostgreSQL cool features, tips and tricks. Database Soup - Josh Berkus' blog. Michael Paquier's blog Robert Haas' blog select * from depesz; - Hubert Lubaczewski's blog.","title":"Blogs"},{"location":"Databases/PostgreSQL/#articles","text":"What PostgreSQL has over other open source SQL databases: Part I What PostgreSQL has over other open source SQL databases: Part II the ultimate postgres vs mysql blog post Debugging PostgreSQL performance, the hard way Why use Postgres? Superfast CSV imports using PostgreSQL's COPY command","title":"Articles"},{"location":"Databases/PostgreSQL/#documentation","text":"Wiki - user documentation, how-tos, and tips 'n' tricks","title":"Documentation"},{"location":"Databases/PostgreSQL/#newsletters","text":"Postgres Weekly - Weekly newsletter that contains articles, news, and repos relevant to PostgreSQL.","title":"Newsletters"},{"location":"Databases/PostgreSQL/#videos","text":"Citus Data Youtube channel - Citus related videos EnterpriseDB Youtube channel - EnterpriseDB related videos Postgres Conference Youtube channel - Conference videos Scaling Postgres - Postgres video blog series by Creston Jamison","title":"Videos"},{"location":"Databases/PostgreSQL/#community","text":"Mailing lists - Official mailing lists for Postgres for support, outreach, and more. One of the primary channels of communication in the Postgres community. Reddit - A reddit community for PostgreSQL users with over 12000 users Slack - Slack channel for Postgres with over 7000 users Telegram - Several groups for PostgreSQL in different langauges: Russian >4200 people, Brazilian Portuguese >2300 people, Indonesian ~1000 people, English >750 people #postgresql on Freenode - The most popular IRC channel about Postgres on Freenode with over 1000 users Links: Source:","title":"Community"},{"location":"Databases/Views%20vs.%20Materialized%20Views/","text":"Views vs. Materialized Views \u2691 What are the major differences between a view and materialized view, and why should you use one over the other? Views \u2691 Normal views have their own advantages and disadvantages in comparison to materialized views . A view is created with the Create View SQL command and contains all data obtained from the supplied view query expression. A primary advantage of views is that you can query them in the same manner as you would any normal database schema's table and retrieve the latest updated calculated results. The key here is that views always compute every time they are queried or accessed in any way; which may be seen as both a pro and con. Another side-note is that in the circumstance you make any type of update to the content in a View, it will always be \u201cpushed back\u201d and updated in the original table. Likewise, the reverse is also true: any changes that are made to the original base table are instantly reflected in the View. What this means, however, is that the performance of a View will always be slower than that of a materialized view. The major advantage is that a View doesn\u2019t actually require storage space. You can also have total control over which users can or cannot view sensitive information within the database itself. A materialized view, on the other hand, is a physical copy of those original base tables. Think of it more like a photograph of the original base table. The key difference is that a materialized view will not be updated every time it is interacted with. Links: Databases | PostgreSQL | System Design | [[Web Development]] Source: What are Materialized Views? A 5 Minute Introduction (educative.io)","title":"Views vs. Materialized Views"},{"location":"Databases/Views%20vs.%20Materialized%20Views/#views-vs-materialized-views","text":"What are the major differences between a view and materialized view, and why should you use one over the other?","title":"Views vs. Materialized Views"},{"location":"Databases/Views%20vs.%20Materialized%20Views/#views","text":"Normal views have their own advantages and disadvantages in comparison to materialized views . A view is created with the Create View SQL command and contains all data obtained from the supplied view query expression. A primary advantage of views is that you can query them in the same manner as you would any normal database schema's table and retrieve the latest updated calculated results. The key here is that views always compute every time they are queried or accessed in any way; which may be seen as both a pro and con. Another side-note is that in the circumstance you make any type of update to the content in a View, it will always be \u201cpushed back\u201d and updated in the original table. Likewise, the reverse is also true: any changes that are made to the original base table are instantly reflected in the View. What this means, however, is that the performance of a View will always be slower than that of a materialized view. The major advantage is that a View doesn\u2019t actually require storage space. You can also have total control over which users can or cannot view sensitive information within the database itself. A materialized view, on the other hand, is a physical copy of those original base tables. Think of it more like a photograph of the original base table. The key difference is that a materialized view will not be updated every time it is interacted with. Links: Databases | PostgreSQL | System Design | [[Web Development]] Source: What are Materialized Views? A 5 Minute Introduction (educative.io)","title":"Views"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/","text":"Firefox Developer Edition \u2691 Really, just use Firefox for fuck sake. Features \u2691 The key killer feature of Firefox is containers . Settings \u2691 General: Enable Ctrl + Tab to cycle through tabs in recently used order: ![[_assets/Pasted image 20210501120642.png]] - Themes \u2691 Currently using the Matte Black (Red) Theme Extensions \u2691 Keeper Password Manager & Digital Vault Momentum OneTab Raindrop.io Drak Reader Non-Essential Extensions: Evernote Web Clipper Instapaper TamperMonkey Developer Tools \u2691 See 30 Tips Tricks with the Firefox Developer Tools Medium article You can save a snapshot of the network requests in your Network Monitor. It saves them as HAR or HTTP Archive format. You can also import HAR files and have them display in the Network Monitor so you can debug them. Links: Sources: - 30 Tips Tricks with the Firefox Developer Tools - Mozilla Github Organization Account Home Page - Calling all web developers: here\u2019s why you should be using Firefox - Firefox is the best browser for web-developers","title":"Firefox Developer Edition"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/#firefox-developer-edition","text":"Really, just use Firefox for fuck sake.","title":"Firefox Developer Edition"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/#features","text":"The key killer feature of Firefox is containers .","title":"Features"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/#settings","text":"General: Enable Ctrl + Tab to cycle through tabs in recently used order: ![[_assets/Pasted image 20210501120642.png]] -","title":"Settings"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/#themes","text":"Currently using the Matte Black (Red) Theme","title":"Themes"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/#extensions","text":"Keeper Password Manager & Digital Vault Momentum OneTab Raindrop.io Drak Reader Non-Essential Extensions: Evernote Web Clipper Instapaper TamperMonkey","title":"Extensions"},{"location":"Developer%20Tools/Web%20Browsers/Firefox%20Developer%20Edition/#developer-tools","text":"See 30 Tips Tricks with the Firefox Developer Tools Medium article You can save a snapshot of the network requests in your Network Monitor. It saves them as HAR or HTTP Archive format. You can also import HAR files and have them display in the Network Monitor so you can debug them. Links: Sources: - 30 Tips Tricks with the Firefox Developer Tools - Mozilla Github Organization Account Home Page - Calling all web developers: here\u2019s why you should be using Firefox - Firefox is the best browser for web-developers","title":"Developer Tools"},{"location":"Developer%20Tools/Web%20Browsers/Web%20Browsers/","text":"Web Browsers \u2691 ![[_assets/Pasted image 20210501123611.png]] ![[_assets/Pasted image 20210501123655.png]] ![[_assets/Pasted image 20210501123722.png]] Currently my primary web browsers are: - [Microsoft Edge Canary] - [Mozilla Firefox Developer Edition] Links: Source:","title":"Web Browsers"},{"location":"Developer%20Tools/Web%20Browsers/Web%20Browsers/#web-browsers","text":"![[_assets/Pasted image 20210501123611.png]] ![[_assets/Pasted image 20210501123655.png]] ![[_assets/Pasted image 20210501123722.png]] Currently my primary web browsers are: - [Microsoft Edge Canary] - [Mozilla Firefox Developer Edition] Links: Source:","title":"Web Browsers"},{"location":"Docker/","text":"Docker \u2691 Categories \u2691 Documents \u2691 Docker Compose Reference Docs Docker","title":"Docker"},{"location":"Docker/#docker","text":"","title":"Docker"},{"location":"Docker/#categories","text":"","title":"Categories"},{"location":"Docker/#documents","text":"Docker Compose Reference Docs Docker","title":"Documents"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/","text":"Docker Compose Reference Docs \u2691 Overview of docker-compose CLI \u2691 Estimated reading time: 5 minutes This page provides the usage information for the docker-compose Command. Command options overview and help \u2691 You can also see this information by running docker-compose --help from the command line. Define and run multi - container applications with Docker . Usage : docker - compose [ -f <arg>... ] [ --profile <name>... ] [ options ] [ COMMAND ] [ ARGS... ] docker - compose - h |-- help Options : - f , -- file FILE Specify an alternate compose file ( default : docker - compose . yml ) - p , -- project - name NAME Specify an alternate project name ( default : directory name ) -- profile NAME Specify a profile to enable -- verbose Show more output -- log - level LEVEL Set log level ( DEBUG , INFO , WARNING , ERROR , CRITICAL ) -- no - ansi Do not print ANSI control characters - v , -- version Print version and exit - H , -- host HOST Daemon socket to connect to -- tls Use TLS ; implied by -- tlsverify -- tlscacert CA_PATH Trust certs signed only by this CA -- tlscert CLIENT_CERT_PATH Path to TLS certificate file -- tlskey TLS_KEY_PATH Path to TLS key file -- tlsverify Use TLS and verify the remote -- skip - hostname - check Don 't check the daemon' s hostname against the name specified in the client certificate -- project - directory PATH Specify an alternate working directory ( default : the path of the Compose file ) -- compatibility If set , Compose will attempt to convert deploy keys in v3 files to their non - Swarm equivalent Commands : build Build or rebuild services bundle Generate a Docker bundle from the Compose file config Validate and view the Compose file create Create services down Stop and remove containers , networks , images , and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one - off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker - Compose version information You can use Docker Compose binary, docker-compose [-f <arg>...] [options] [COMMAND] [ARGS...] , to build and manage multiple services in Docker containers. Use -f to specify name and path of one or more Compose files \u2691 Use the -f flag to specify the location of a Compose configuration file. Specifying multiple Compose files \u2691 You can supply multiple -f configuration files. When you supply multiple files, Compose combines them into a single configuration. Compose builds the configuration in the order you supply the files. Subsequent files override and add to their predecessors. For example, consider this command line: $ docker-compose -f docker-compose.yml -f docker-compose.admin.yml run backup_db The docker-compose.yml file might specify a webapp service. webapp : image : examples / web ports : - \"8000:8000\" volumes : - \"/data\" If the docker-compose.admin.yml also specifies this same service, any matching fields override the previous file. New values, add to the webapp service configuration. webapp : build : . environment : - DEBUG = 1 When you use multiple Compose files, all paths in the files are relative to the first configuration file specified with -f . You can use the --project-directory option to override this base path. Use a -f with - (dash) as the filename to read the configuration from stdin . When stdin is used all paths in the configuration are relative to the current working directory. The -f flag is optional. If you don\u2019t provide this flag on the command line, Compose traverses the working directory and its parent directories looking for a docker-compose.yml and a docker-compose.override.yml file. You must supply at least the docker-compose.yml file. If both files are present on the same directory level, Compose combines the two files into a single configuration. The configuration in the docker-compose.override.yml file is applied over and in addition to the values in the docker-compose.yml file. Specifying a path to a single Compose file \u2691 You can use the -f flag to specify a path to a Compose file that is not located in the current directory, either from the command line or by setting up a COMPOSE_FILE environment variable in your shell or in an environment file. For an example of using the -f option at the command line, suppose you are running the Compose Rails sample , and have a docker-compose.yml file in a directory called sandbox/rails . You can use a command like docker-compose pull to get the postgres image for the db service from anywhere by using the -f flag as follows: docker-compose -f ~/sandbox/rails/docker-compose.yml pull db Here\u2019s the full example: $ docker - compose - f ~/ sandbox / rails / docker - compose . yml pull db Pulling db ( postgres : latest ) ... latest : Pulling from library / postgres ef0380f84d05 : Pull complete 50 cf91dc1db8 : Pull complete d3add4cd115c : Pull complete 467830 d8a616 : Pull complete 089 b9db7dc57 : Pull complete 6 fba0a36935c : Pull complete 81 ef0e73c953 : Pull complete 338 a6c4894dc : Pull complete 15853 f32f67c : Pull complete 044 c83d92898 : Pull complete 17301519 f133 : Pull complete dcca70822752 : Pull complete cecf11b8ccf3 : Pull complete Digest : sha256 : 1364924 c753d5ff7e2260cd34dc4ba05ebd40ee8193391220be0f9901d4e1651 Status : Downloaded newer image for postgres : latest Use -p to specify a project name \u2691 Each configuration has a project name. If you supply a -p flag, you can specify a project name. If you don\u2019t specify the flag, Compose uses the current directory name. See also the COMPOSE_PROJECT_NAME environment variable . Use --profile to specify one or more active profiles \u2691 Calling docker-compose --profile frontend up will start the services with the profile frontend and services without specified profiles. You can also enable multiple profiles, e.g. with docker-compose --profile frontend --profile debug up the profiles frontend and debug will be enabled. See also Using profiles with Compose and the COMPOSE_PROFILES environment variable . Set up environment variables \u2691 You can set environment variables for various docker-compose options, including the -f and -p flags. For example, the COMPOSE_FILE environment variable relates to the -f flag, and COMPOSE_PROJECT_NAME environment variable relates to the -p flag. Also, you can set some of these variables in an environment file . Where to go next \u2691 CLI environment variables Declare default environment variables in file fig , composition , compose , docker , orchestration , cli , reference , docker-compose Links: Source:","title":"Docker Compose Reference Docs"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#docker-compose-reference-docs","text":"","title":"Docker Compose Reference Docs"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#overview-of-docker-compose-cli","text":"Estimated reading time: 5 minutes This page provides the usage information for the docker-compose Command.","title":"Overview of docker-compose CLI"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#command-options-overview-and-help","text":"You can also see this information by running docker-compose --help from the command line. Define and run multi - container applications with Docker . Usage : docker - compose [ -f <arg>... ] [ --profile <name>... ] [ options ] [ COMMAND ] [ ARGS... ] docker - compose - h |-- help Options : - f , -- file FILE Specify an alternate compose file ( default : docker - compose . yml ) - p , -- project - name NAME Specify an alternate project name ( default : directory name ) -- profile NAME Specify a profile to enable -- verbose Show more output -- log - level LEVEL Set log level ( DEBUG , INFO , WARNING , ERROR , CRITICAL ) -- no - ansi Do not print ANSI control characters - v , -- version Print version and exit - H , -- host HOST Daemon socket to connect to -- tls Use TLS ; implied by -- tlsverify -- tlscacert CA_PATH Trust certs signed only by this CA -- tlscert CLIENT_CERT_PATH Path to TLS certificate file -- tlskey TLS_KEY_PATH Path to TLS key file -- tlsverify Use TLS and verify the remote -- skip - hostname - check Don 't check the daemon' s hostname against the name specified in the client certificate -- project - directory PATH Specify an alternate working directory ( default : the path of the Compose file ) -- compatibility If set , Compose will attempt to convert deploy keys in v3 files to their non - Swarm equivalent Commands : build Build or rebuild services bundle Generate a Docker bundle from the Compose file config Validate and view the Compose file create Create services down Stop and remove containers , networks , images , and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one - off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker - Compose version information You can use Docker Compose binary, docker-compose [-f <arg>...] [options] [COMMAND] [ARGS...] , to build and manage multiple services in Docker containers.","title":"Command options overview and help"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#use-f-to-specify-name-and-path-of-one-or-more-compose-files","text":"Use the -f flag to specify the location of a Compose configuration file.","title":"Use -f to specify name and path of one or more Compose files"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#specifying-multiple-compose-files","text":"You can supply multiple -f configuration files. When you supply multiple files, Compose combines them into a single configuration. Compose builds the configuration in the order you supply the files. Subsequent files override and add to their predecessors. For example, consider this command line: $ docker-compose -f docker-compose.yml -f docker-compose.admin.yml run backup_db The docker-compose.yml file might specify a webapp service. webapp : image : examples / web ports : - \"8000:8000\" volumes : - \"/data\" If the docker-compose.admin.yml also specifies this same service, any matching fields override the previous file. New values, add to the webapp service configuration. webapp : build : . environment : - DEBUG = 1 When you use multiple Compose files, all paths in the files are relative to the first configuration file specified with -f . You can use the --project-directory option to override this base path. Use a -f with - (dash) as the filename to read the configuration from stdin . When stdin is used all paths in the configuration are relative to the current working directory. The -f flag is optional. If you don\u2019t provide this flag on the command line, Compose traverses the working directory and its parent directories looking for a docker-compose.yml and a docker-compose.override.yml file. You must supply at least the docker-compose.yml file. If both files are present on the same directory level, Compose combines the two files into a single configuration. The configuration in the docker-compose.override.yml file is applied over and in addition to the values in the docker-compose.yml file.","title":"Specifying multiple Compose files"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#specifying-a-path-to-a-single-compose-file","text":"You can use the -f flag to specify a path to a Compose file that is not located in the current directory, either from the command line or by setting up a COMPOSE_FILE environment variable in your shell or in an environment file. For an example of using the -f option at the command line, suppose you are running the Compose Rails sample , and have a docker-compose.yml file in a directory called sandbox/rails . You can use a command like docker-compose pull to get the postgres image for the db service from anywhere by using the -f flag as follows: docker-compose -f ~/sandbox/rails/docker-compose.yml pull db Here\u2019s the full example: $ docker - compose - f ~/ sandbox / rails / docker - compose . yml pull db Pulling db ( postgres : latest ) ... latest : Pulling from library / postgres ef0380f84d05 : Pull complete 50 cf91dc1db8 : Pull complete d3add4cd115c : Pull complete 467830 d8a616 : Pull complete 089 b9db7dc57 : Pull complete 6 fba0a36935c : Pull complete 81 ef0e73c953 : Pull complete 338 a6c4894dc : Pull complete 15853 f32f67c : Pull complete 044 c83d92898 : Pull complete 17301519 f133 : Pull complete dcca70822752 : Pull complete cecf11b8ccf3 : Pull complete Digest : sha256 : 1364924 c753d5ff7e2260cd34dc4ba05ebd40ee8193391220be0f9901d4e1651 Status : Downloaded newer image for postgres : latest","title":"Specifying a path to a single Compose file"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#use-p-to-specify-a-project-name","text":"Each configuration has a project name. If you supply a -p flag, you can specify a project name. If you don\u2019t specify the flag, Compose uses the current directory name. See also the COMPOSE_PROJECT_NAME environment variable .","title":"Use -p to specify a project name"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#use-profile-to-specify-one-or-more-active-profiles","text":"Calling docker-compose --profile frontend up will start the services with the profile frontend and services without specified profiles. You can also enable multiple profiles, e.g. with docker-compose --profile frontend --profile debug up the profiles frontend and debug will be enabled. See also Using profiles with Compose and the COMPOSE_PROFILES environment variable .","title":"Use --profile to specify one or more active profiles"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#set-up-environment-variables","text":"You can set environment variables for various docker-compose options, including the -f and -p flags. For example, the COMPOSE_FILE environment variable relates to the -f flag, and COMPOSE_PROJECT_NAME environment variable relates to the -p flag. Also, you can set some of these variables in an environment file .","title":"Set up environment variables"},{"location":"Docker/Docker%20Compose%20Reference%20Docs/#where-to-go-next","text":"CLI environment variables Declare default environment variables in file fig , composition , compose , docker , orchestration , cli , reference , docker-compose Links: Source:","title":"Where to go next"},{"location":"Docker/Docker/","text":"Docker Best Practices \u2691 Utilize multistage builds and set DOCKER_BUILDKIT=1 environment variable to allow them to build in parallel . You can also copy files between stages in multistage builds from previous layers Split long RUN commands into multiple lines per statement and alphabetize the order of the arguments apt-get notes: don't run apt-get upgrade or dist-upgrade since that will be the job of the base image keep apt-get update and apt-get install together RUN apt-get update && apt-get install -y \\ package-bar \\ package-baz \\ package-foo Build an Image from a Github Repo (without a dockerfile) docker build -t myimage : latest -f - https : // github . com / docker-library / hello-world . git << EOF FROM busybox COPY hello . c . EOF Use ENV to update the path: ENV PATH /usr/local/nginx/bin:$PATH Use ENTRYPOINT for the main executable, with default flags provided by CMD ENTRYPOINT [\"s3cmd\"] CMD [\"--help\"] so this will show help menu: docker run s3cmd and this will do whatever the params say: docker run s3cmd ls s3://mybucket it's very common to create a script docker-entrypoint.sh as the ENTRYPOINT COPY ./docker-entrypoint.sh / ENTRYPOINT [\"/docker-entrypoint.sh\"] CMD [\"postgres\"] docker-entrypoint.sh: #!/bin/bash # exit if any commands return non-zero set -e # if first param is 'postgres' if [ \" $1 \" = 'postgres' ] ; then # assign postgres to the PGDATA directory chown -R postgres \" $PGDATA \" # if string is empty if [ -z \" $( ls -A \" $PGDATA \" ) \" ] ; then # gosu is like sudo without certain annoying TTY features gosu postgres initdb fi # $@ is all of the parameters passed through (e.g. $1 $2 ...) # exec will replace the currently executing process with a new one exec gosu postgres \" $@ \" fi exec \" $@ \" example invocations: docker run postgres : runs postres docker run postgres postgres --help : run Postgres and pass parameters to the server docker run --rm -it postgres bash : start a totally different tool, such as Bash use VOLUME for any mutable or user-servicable parts of your image avoid using sudo - gosu is a better option if you can run a service without root, use the USER command to change to the user You can create a user and set group with something like: RUN groupadd -r postgres && useradd --no-log-init -r -g postgres postgres. use absolute paths for your WORKDIR Caching only RUN, COPY, and ADD statments are cached COPY and ADD will perform a checksum on the corresponding file contents RUN will only attempt to match by the command name Once 1 cache layer is invalidated, everything dockerfile statement after it is run dynamically (not cached) So if you copy your source code in and it has changed, everything else after that is rebuilt Some people inject a specifically cache-busting layer: docker build --build-arg CACHE_BUST=$(date +%s) . ARG CACHE_BUST RUN echo \"command with external dependencies\" Dockerfile vs Docker-Compose From somebody on Docker Team Dockerfiles are the recipe for building images and should add all the binaries/other files you need to make your service work. There are a couple of exceptions to this: secrets (i.e.: credentials), configs (i.e.: configuration files), and application state data (e.g.: your database data). Note that secrets and configs are read only. Compose files are used to describe how a set of services are deployed and interact. The Compose format is used not only for a single engine (i.e.: docker-compose) but also for orchestrated environments like Swarm and Kubernetes. The goal of the Compose format is to make it easy to write an application and test it locally, then deploy it to an orchestrated environment with little or no changes. This goal limits what we can change in the format because of fundamental differences like how each environemtn handles volumes and data storage. Links: Source: Best practices for writing Dockerfiles | Docker Documentation","title":"Docker Best Practices"},{"location":"Docker/Docker/#docker-best-practices","text":"Utilize multistage builds and set DOCKER_BUILDKIT=1 environment variable to allow them to build in parallel . You can also copy files between stages in multistage builds from previous layers Split long RUN commands into multiple lines per statement and alphabetize the order of the arguments apt-get notes: don't run apt-get upgrade or dist-upgrade since that will be the job of the base image keep apt-get update and apt-get install together RUN apt-get update && apt-get install -y \\ package-bar \\ package-baz \\ package-foo Build an Image from a Github Repo (without a dockerfile) docker build -t myimage : latest -f - https : // github . com / docker-library / hello-world . git << EOF FROM busybox COPY hello . c . EOF Use ENV to update the path: ENV PATH /usr/local/nginx/bin:$PATH Use ENTRYPOINT for the main executable, with default flags provided by CMD ENTRYPOINT [\"s3cmd\"] CMD [\"--help\"] so this will show help menu: docker run s3cmd and this will do whatever the params say: docker run s3cmd ls s3://mybucket it's very common to create a script docker-entrypoint.sh as the ENTRYPOINT COPY ./docker-entrypoint.sh / ENTRYPOINT [\"/docker-entrypoint.sh\"] CMD [\"postgres\"] docker-entrypoint.sh: #!/bin/bash # exit if any commands return non-zero set -e # if first param is 'postgres' if [ \" $1 \" = 'postgres' ] ; then # assign postgres to the PGDATA directory chown -R postgres \" $PGDATA \" # if string is empty if [ -z \" $( ls -A \" $PGDATA \" ) \" ] ; then # gosu is like sudo without certain annoying TTY features gosu postgres initdb fi # $@ is all of the parameters passed through (e.g. $1 $2 ...) # exec will replace the currently executing process with a new one exec gosu postgres \" $@ \" fi exec \" $@ \" example invocations: docker run postgres : runs postres docker run postgres postgres --help : run Postgres and pass parameters to the server docker run --rm -it postgres bash : start a totally different tool, such as Bash use VOLUME for any mutable or user-servicable parts of your image avoid using sudo - gosu is a better option if you can run a service without root, use the USER command to change to the user You can create a user and set group with something like: RUN groupadd -r postgres && useradd --no-log-init -r -g postgres postgres. use absolute paths for your WORKDIR Caching only RUN, COPY, and ADD statments are cached COPY and ADD will perform a checksum on the corresponding file contents RUN will only attempt to match by the command name Once 1 cache layer is invalidated, everything dockerfile statement after it is run dynamically (not cached) So if you copy your source code in and it has changed, everything else after that is rebuilt Some people inject a specifically cache-busting layer: docker build --build-arg CACHE_BUST=$(date +%s) . ARG CACHE_BUST RUN echo \"command with external dependencies\" Dockerfile vs Docker-Compose From somebody on Docker Team Dockerfiles are the recipe for building images and should add all the binaries/other files you need to make your service work. There are a couple of exceptions to this: secrets (i.e.: credentials), configs (i.e.: configuration files), and application state data (e.g.: your database data). Note that secrets and configs are read only. Compose files are used to describe how a set of services are deployed and interact. The Compose format is used not only for a single engine (i.e.: docker-compose) but also for orchestrated environments like Swarm and Kubernetes. The goal of the Compose format is to make it easy to write an application and test it locally, then deploy it to an orchestrated environment with little or no changes. This goal limits what we can change in the format because of fundamental differences like how each environemtn handles volumes and data storage. Links: Source: Best practices for writing Dockerfiles | Docker Documentation","title":"Docker Best Practices"},{"location":"Documentation/","text":"Documentation \u2691 Categories \u2691 Documents \u2691 MkDocs","title":"Documentation"},{"location":"Documentation/#documentation","text":"","title":"Documentation"},{"location":"Documentation/#categories","text":"","title":"Categories"},{"location":"Documentation/#documents","text":"MkDocs","title":"Documents"},{"location":"Documentation/MkDocs/","text":"MkDocs \u2691 Installation \u2691 Requirements \u2691 Python pip Setup \u2691 Ensure pip is installed and in PATH : py -m pip - -version Links: Source:","title":"MkDocs"},{"location":"Documentation/MkDocs/#mkdocs","text":"","title":"MkDocs"},{"location":"Documentation/MkDocs/#installation","text":"","title":"Installation"},{"location":"Documentation/MkDocs/#requirements","text":"Python pip","title":"Requirements"},{"location":"Documentation/MkDocs/#setup","text":"Ensure pip is installed and in PATH : py -m pip - -version Links: Source:","title":"Setup"},{"location":"GCP/Google%20Cloud%20APIs/","text":"Google Cloud APIs \u2691 Links: Google Cloud APIs | [[GCP/Google Cloud Setup Notes]] Source: Google APIs Explorer | Google Developers","title":"Google Cloud APIs"},{"location":"GCP/Google%20Cloud%20APIs/#google-cloud-apis","text":"Links: Google Cloud APIs | [[GCP/Google Cloud Setup Notes]] Source: Google APIs Explorer | Google Developers","title":"Google Cloud APIs"},{"location":"GCP/Google%20Cloud%20Setup%20Notes/","text":"Google Cloud Setup Notes \u2691 Reference \u2691 How-to Guides | Cloud SDK Documentation | Google Cloud Quickstart: Getting started with Cloud SDK | Cloud SDK Documentation (google.com) Installing Google Cloud SDK | Cloud SDK Documentation The gcloud command-line tool cheat sheet | Cloud SDK Documentation (google.com) gcloud command-line tool overview | Cloud SDK Documentation (google.com) Initializing Cloud SDK | Cloud SDK Documentation | Google Cloud Managing SDK Components | Cloud SDK Documentation | Google Cloud Managing SDK Properties | Cloud SDK Documentation | Google Cloud Scripting gcloud tool commands | Cloud SDK Documentation (google.com) Quickstart | Secret Manager Documentation | Google Cloud Environment Setup and Configuration \u2691 In the Google Cloud Console , select or create a GCP project. Project Selector Page Ensure Billing is Enabled for the project. Enable the Cloud Run API . Install and Initialize gcloud SDK on local machine. On windows run, cinst gcloud if using Chocolatey. Update components via gcloud components update Authenticate GCP (two methods): Using a dedicated service account Links: Install gcloud SDK on Ubuntu Source: How-to Guides | Cloud SDK Documentation | Google Cloud","title":"Google Cloud Setup Notes"},{"location":"GCP/Google%20Cloud%20Setup%20Notes/#google-cloud-setup-notes","text":"","title":"Google Cloud Setup Notes"},{"location":"GCP/Google%20Cloud%20Setup%20Notes/#reference","text":"How-to Guides | Cloud SDK Documentation | Google Cloud Quickstart: Getting started with Cloud SDK | Cloud SDK Documentation (google.com) Installing Google Cloud SDK | Cloud SDK Documentation The gcloud command-line tool cheat sheet | Cloud SDK Documentation (google.com) gcloud command-line tool overview | Cloud SDK Documentation (google.com) Initializing Cloud SDK | Cloud SDK Documentation | Google Cloud Managing SDK Components | Cloud SDK Documentation | Google Cloud Managing SDK Properties | Cloud SDK Documentation | Google Cloud Scripting gcloud tool commands | Cloud SDK Documentation (google.com) Quickstart | Secret Manager Documentation | Google Cloud","title":"Reference"},{"location":"GCP/Google%20Cloud%20Setup%20Notes/#environment-setup-and-configuration","text":"In the Google Cloud Console , select or create a GCP project. Project Selector Page Ensure Billing is Enabled for the project. Enable the Cloud Run API . Install and Initialize gcloud SDK on local machine. On windows run, cinst gcloud if using Chocolatey. Update components via gcloud components update Authenticate GCP (two methods): Using a dedicated service account Links: Install gcloud SDK on Ubuntu Source: How-to Guides | Cloud SDK Documentation | Google Cloud","title":"Environment Setup and Configuration"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Ubuntu/","text":"Install gcloud SDK on Ubuntu \u2691 Quick Start \u2691 Follow the instruction below to install the Cloud SDK: Add the Cloud SDK distribution URI as a package source: echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list sudo apt-get install apt-transport-https ca-certificates curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - Update and install the Cloud SDK: sudo apt-get update && sudo apt-get install google-cloud-sdk Run gcloud init to initialize the SDK: gcloud init In Depth \u2691 Note: Depending on your setup, you can choose other installation methods: If you use Snap on your system, you can install Cloud SDK as a snap package If you're using an instance on Google Compute Engine, Cloud SDK is installed by default. Package contents Cloud SDK is available in package format for installation on Debian and Ubuntu systems. This package contains the gcloud , gcloud alpha , gcloud beta , gsutil , and bq commands only. It does not include kubectl or the App Engine extensions required to deploy an application using gcloud commands. If you want these components, you must install them separately as described later in this section. Prerequisites Before you install Cloud SDK, make sure that your operating system is one of the following: Ubuntu release that has not reached end-of-life Debian stable release from Wheezy forward Installation Add the Cloud SDK distribution URI as a package source: echo \"deb \\[signed-by=/usr/share/keyrings/cloud.google.gpg\\] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee \\- a /etc/apt/sources.list.d/google \\- cloud \\- sdk.list Make sure you have apt-transport-https installed: sudo apt \\- get install apt \\- transport \\- https ca \\- certificates gnupg Note: If your distribution does not support the signed-by option run this command instead: echo \"deb https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee \\- a /etc/apt/sources.list.d/google \\- cloud \\- sdk.list Note: Make sure you do not have duplicate entries for the cloud-sdk repo. Import the Google Cloud public key: curl https://packages.cloud.google.com/apt/doc/apt \\- key.gpg | sudo apt \\- key \\- -keyring /usr/share/keyrings/cloud.google.gpg add \\- Note: If you are unable to get latest updates due to an expired key, obtain the latest apt-get.gpg key file . Note: If your distribution's apt-key command does not support the --keyring argument run this command instead: curl https://packages.cloud.google.com/apt/doc/apt \\- key.gpg | sudo apt \\- key add \\- Update and install the Cloud SDK: sudo apt \\- get update && sudo apt \\- get install google \\- cloud \\- sdk For additional apt-get options, such as disabling prompts or dry runs, refer to the apt-get man pages . Docker Tip: If installing the Cloud SDK inside a Docker image, use a single RUN step instead: RUN echo \"deb \\[signed-by=/usr/share/keyrings/cloud.google.gpg\\] http://packages.cloud.google.com/apt cloud-sdk main\" | tee \\-a /etc/apt/sources.list.d/google\\-cloud\\-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt\\-key.gpg | apt\\-key \\--keyring /usr/share/keyrings/cloud.google.gpg add \\- && apt\\-get update \\-y && apt\\-get install google\\-cloud\\-sdk \\-y Optionally, install any of these additional components : google-cloud-sdk-app-engine-python google-cloud-sdk-app-engine-python-extras google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-bigtable-emulator google-cloud-sdk-cbt google-cloud-sdk-cloud-build-local google-cloud-sdk-datalab google-cloud-sdk-datastore-emulator google-cloud-sdk-firestore-emulator google-cloud-sdk-pubsub-emulator kubectl For example, the google-cloud-sdk-app-engine-java component can be installed as follows: sudo apt \\- get install google \\- cloud \\- sdk \\- app \\- engine \\- java Run gcloud init to get started: gcloud init Downgrading Cloud SDK versions If you'd like to revert to a specific version of Cloud SDK, where VERSION is of the form 123.0.0 , run: sudo apt-get update && sudo apt-get install google-cloud-sdk=123.0.0-0 The most recent ten releases will always be available in the repo. Optional: Install the latest Google Cloud Client Libraries \u2691 You can download Cloud Client Libraries for supported languages. Initializing the Cloud SDK \u2691 Use the gcloud init command to perform several common Cloud SDK setup tasks. These include authorizing the Cloud SDK tools to access Google Cloud using your user account credentials and setting up the default configuration. To initialize the Cloud SDK: Run the following at a command prompt: gcloud init Note: To prevent the command from launching a web browser, use gcloud init --console-only instead. To authorize without a web browser and non-interactively, create a service account with the appropriate scopes using the Google Cloud Console and use gcloud auth activate-service-account with the corresponding JSON key file. Accept the option to log in using your Google user account: To continue, you must log in. Would you like to log in (Y/n)? Y 3. In your browser, log in to your Google user account when prompted and click Allow to grant permission to access Google Cloud resources. At the command prompt, select a Google Cloud project from the list of those where you have Owner , Editor or Viewer permissions: Pick cloud project to use: [1] [my-project-1] [2] [my-project-2] ... Please enter your numeric choice: If you only have one project, gcloud init selects it for you. If you have access to more than 200 projects, you will be prompted to enter a project id, create a new project, or list projects. This account has a lot of projects! Listing them all can take a while. [1] Enter a project ID [2] Create a new project [3] List projects Please enter your numeric choice: Note: If you choose to create a project, you'll also need to enable billing on your project to use Google Cloud services. If you have the Google Compute Engine API enabled, gcloud init allows you to choose a default Compute Engine zone: Which compute zone would you like to use as project default? [1] [asia-east1-a] [2] [asia-east1-b] ... [14] Do not use default zone Please enter your numeric choice: gcloud init confirms that you have complete the setup steps successfully: gcloud has now been configured! You can use [gcloud config] to change more gcloud settings. Your active configuration is: [default] 6. (Optional) If you'd like a more streamlined screen reader experience, the gcloud command-line tool comes with an accessibility/screen_reader property. To enable this property, run: gcloud config set accessibility/screen_reader true For more details about the accessibility features that come with the gcloud command-line tool, refer to the Enabling accessibility features guide. Running core commands \u2691 Run these gcloud commands to view information about your Cloud SDK installation: To list accounts whose credentials are stored on the local system: gcloud auth list gcloud displays a list of credentialed accounts: Credentialed Accounts ACTIVE ACCOUNT * example-user-1@gmail.com example-user-2@gmail.com 2. To list the properties in your active Cloud SDK configuration: gcloud config list gcloud displays the list of properties: [core] account = example-user-1@gmail.com disable_usage_reporting = False project = example-project 3. To view information about your Cloud SDK installation and the active configuration: gcloud info gcloud displays a summary of information about your Cloud SDK installation. This includes information about your system, the installed components, the active user account and current project, and the properties in the active configuration. To view information about gcloud commands and other topics from the command line: gcloud help For example, to view the help for gcloud compute instances create : gcloud help compute instances create gcloud displays a help topic that contains a description of the command, a list of command flags and arguments, and examples of how to use it. What's next \u2691 Read the gcloud command-line tool guide for an overview of the gcloud command-line tool, including a quick introduction to key concepts, command conventions, and helpful tips. Read the gcloud command-line tool reference guide for detailed pages on each gcloud command, including descriptions, flags, and examples, that you can use to perform a variety of tasks on Google Cloud. Refer to the gcloud command-line tool cheat sheet for a list of commonly used commands and key concepts. Install additional components such as the App Engine emulators or kubectl using the Cloud SDK component manager . Links: [[GCP/Google Cloud Setup Notes]] Source: - Useful Google Cloud Platform Commands Cheat Sheet - Installing Google Cloud SDK | Cloud SDK Documentation","title":"Install gcloud SDK on Ubuntu"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Ubuntu/#install-gcloud-sdk-on-ubuntu","text":"","title":"Install gcloud SDK on Ubuntu"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Ubuntu/#quick-start","text":"Follow the instruction below to install the Cloud SDK: Add the Cloud SDK distribution URI as a package source: echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list sudo apt-get install apt-transport-https ca-certificates curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - Update and install the Cloud SDK: sudo apt-get update && sudo apt-get install google-cloud-sdk Run gcloud init to initialize the SDK: gcloud init","title":"Quick Start"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Ubuntu/#in-depth","text":"Note: Depending on your setup, you can choose other installation methods: If you use Snap on your system, you can install Cloud SDK as a snap package If you're using an instance on Google Compute Engine, Cloud SDK is installed by default. Package contents Cloud SDK is available in package format for installation on Debian and Ubuntu systems. This package contains the gcloud , gcloud alpha , gcloud beta , gsutil , and bq commands only. It does not include kubectl or the App Engine extensions required to deploy an application using gcloud commands. If you want these components, you must install them separately as described later in this section. Prerequisites Before you install Cloud SDK, make sure that your operating system is one of the following: Ubuntu release that has not reached end-of-life Debian stable release from Wheezy forward Installation Add the Cloud SDK distribution URI as a package source: echo \"deb \\[signed-by=/usr/share/keyrings/cloud.google.gpg\\] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee \\- a /etc/apt/sources.list.d/google \\- cloud \\- sdk.list Make sure you have apt-transport-https installed: sudo apt \\- get install apt \\- transport \\- https ca \\- certificates gnupg Note: If your distribution does not support the signed-by option run this command instead: echo \"deb https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee \\- a /etc/apt/sources.list.d/google \\- cloud \\- sdk.list Note: Make sure you do not have duplicate entries for the cloud-sdk repo. Import the Google Cloud public key: curl https://packages.cloud.google.com/apt/doc/apt \\- key.gpg | sudo apt \\- key \\- -keyring /usr/share/keyrings/cloud.google.gpg add \\- Note: If you are unable to get latest updates due to an expired key, obtain the latest apt-get.gpg key file . Note: If your distribution's apt-key command does not support the --keyring argument run this command instead: curl https://packages.cloud.google.com/apt/doc/apt \\- key.gpg | sudo apt \\- key add \\- Update and install the Cloud SDK: sudo apt \\- get update && sudo apt \\- get install google \\- cloud \\- sdk For additional apt-get options, such as disabling prompts or dry runs, refer to the apt-get man pages . Docker Tip: If installing the Cloud SDK inside a Docker image, use a single RUN step instead: RUN echo \"deb \\[signed-by=/usr/share/keyrings/cloud.google.gpg\\] http://packages.cloud.google.com/apt cloud-sdk main\" | tee \\-a /etc/apt/sources.list.d/google\\-cloud\\-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt\\-key.gpg | apt\\-key \\--keyring /usr/share/keyrings/cloud.google.gpg add \\- && apt\\-get update \\-y && apt\\-get install google\\-cloud\\-sdk \\-y Optionally, install any of these additional components : google-cloud-sdk-app-engine-python google-cloud-sdk-app-engine-python-extras google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-bigtable-emulator google-cloud-sdk-cbt google-cloud-sdk-cloud-build-local google-cloud-sdk-datalab google-cloud-sdk-datastore-emulator google-cloud-sdk-firestore-emulator google-cloud-sdk-pubsub-emulator kubectl For example, the google-cloud-sdk-app-engine-java component can be installed as follows: sudo apt \\- get install google \\- cloud \\- sdk \\- app \\- engine \\- java Run gcloud init to get started: gcloud init Downgrading Cloud SDK versions If you'd like to revert to a specific version of Cloud SDK, where VERSION is of the form 123.0.0 , run: sudo apt-get update && sudo apt-get install google-cloud-sdk=123.0.0-0 The most recent ten releases will always be available in the repo.","title":"In Depth"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Ubuntu/#optional-install-the-latest-google-cloud-client-libraries","text":"You can download Cloud Client Libraries for supported languages.","title":"Optional: Install the latest Google Cloud Client Libraries"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Ubuntu/#initializing-the-cloud-sdk","text":"Use the gcloud init command to perform several common Cloud SDK setup tasks. These include authorizing the Cloud SDK tools to access Google Cloud using your user account credentials and setting up the default configuration. To initialize the Cloud SDK: Run the following at a command prompt: gcloud init Note: To prevent the command from launching a web browser, use gcloud init --console-only instead. To authorize without a web browser and non-interactively, create a service account with the appropriate scopes using the Google Cloud Console and use gcloud auth activate-service-account with the corresponding JSON key file. Accept the option to log in using your Google user account: To continue, you must log in. Would you like to log in (Y/n)? Y 3. In your browser, log in to your Google user account when prompted and click Allow to grant permission to access Google Cloud resources. At the command prompt, select a Google Cloud project from the list of those where you have Owner , Editor or Viewer permissions: Pick cloud project to use: [1] [my-project-1] [2] [my-project-2] ... Please enter your numeric choice: If you only have one project, gcloud init selects it for you. If you have access to more than 200 projects, you will be prompted to enter a project id, create a new project, or list projects. This account has a lot of projects! Listing them all can take a while. [1] Enter a project ID [2] Create a new project [3] List projects Please enter your numeric choice: Note: If you choose to create a project, you'll also need to enable billing on your project to use Google Cloud services. If you have the Google Compute Engine API enabled, gcloud init allows you to choose a default Compute Engine zone: Which compute zone would you like to use as project default? [1] [asia-east1-a] [2] [asia-east1-b] ... [14] Do not use default zone Please enter your numeric choice: gcloud init confirms that you have complete the setup steps successfully: gcloud has now been configured! You can use [gcloud config] to change more gcloud settings. Your active configuration is: [default] 6. (Optional) If you'd like a more streamlined screen reader experience, the gcloud command-line tool comes with an accessibility/screen_reader property. To enable this property, run: gcloud config set accessibility/screen_reader true For more details about the accessibility features that come with the gcloud command-line tool, refer to the Enabling accessibility features guide.","title":"Initializing the Cloud SDK"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Ubuntu/#running-core-commands","text":"Run these gcloud commands to view information about your Cloud SDK installation: To list accounts whose credentials are stored on the local system: gcloud auth list gcloud displays a list of credentialed accounts: Credentialed Accounts ACTIVE ACCOUNT * example-user-1@gmail.com example-user-2@gmail.com 2. To list the properties in your active Cloud SDK configuration: gcloud config list gcloud displays the list of properties: [core] account = example-user-1@gmail.com disable_usage_reporting = False project = example-project 3. To view information about your Cloud SDK installation and the active configuration: gcloud info gcloud displays a summary of information about your Cloud SDK installation. This includes information about your system, the installed components, the active user account and current project, and the properties in the active configuration. To view information about gcloud commands and other topics from the command line: gcloud help For example, to view the help for gcloud compute instances create : gcloud help compute instances create gcloud displays a help topic that contains a description of the command, a list of command flags and arguments, and examples of how to use it.","title":"Running core commands"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Ubuntu/#whats-next","text":"Read the gcloud command-line tool guide for an overview of the gcloud command-line tool, including a quick introduction to key concepts, command conventions, and helpful tips. Read the gcloud command-line tool reference guide for detailed pages on each gcloud command, including descriptions, flags, and examples, that you can use to perform a variety of tasks on Google Cloud. Refer to the gcloud command-line tool cheat sheet for a list of commonly used commands and key concepts. Install additional components such as the App Engine emulators or kubectl using the Cloud SDK component manager . Links: [[GCP/Google Cloud Setup Notes]] Source: - Useful Google Cloud Platform Commands Cheat Sheet - Installing Google Cloud SDK | Cloud SDK Documentation","title":"What's next"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Windows/","text":"Install gcloud SDK on Windows \u2691 Download the Cloud SDK installer . Alternatively, open a PowerShell terminal and run the following PowerShell commands. ( New \\ -Object Net . WebClient ). DownloadFile ( \"https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe\" , \"$env:Temp\\\\GoogleCloudSDKInstaller.exe\" ) & $env:Temp \\\\ GoogleCloudSDKInstaller . exe Launch the installer and follow the prompts. The installer is signed by Google LLC. If you'd like to enable screen reader mode, select the Turn on screen reader mode option for a more streamlined screen reader experience. To read more about the Cloud SDK screen reader experience, refer to the Accessibility features guide . Cloud SDK requires Python; supported versions are Python 3 (preferred, 3.5 to 3.8) and Python 2 (2.7.9 or higher). The installer will install all necessary dependencies, including the needed Python version. While Cloud SDK currently uses Python 3 by default, you can use an existing Python installation if necessary by unchecking the option to Install Bundled Python. After installation has completed, the installer presents several options: Make sure that the following are selected: Start Google Cloud SDK Shell Run gcloud init The installer starts a terminal window and runs the gcloud init command. The default installation does not include the App Engine extensions required to deploy an application using gcloud commands. These components can be installed using the Cloud SDK component manager . Troubleshooting tips: If the Cloud SDK fails to run after installing version 274.0.0, please refer to this tracking bug for the latest workarounds. If your installation is unsuccessful due to the find command not being recognized, ensure your PATH environment variable is set to include the folder containing find . Usually, this is C:\\WINDOWS\\system32; . If you have just uninstalled Cloud SDK, you will need to reboot your system before installing Cloud SDK again. Optional: Install the latest Google Cloud Client Libraries \u2691 You can download Cloud Client Libraries for supported languages. Other installation options \u2691 Depending on your development needs, instead of the recommended installation , you can use an alternative method of installing Cloud SDK: Using Cloud SDK with scripts or Continuous Integration/Deployment? Download a versioned archive for a non-interactive installation of a specific version of Cloud SDK. Need to run Cloud SDK as a Docker image? Use the Cloud SDK Docker image for the latest release (or specific version) of Cloud SDK. Running Ubuntu and prefer automatic updates? Use a snap package to install the Cloud SDK. For Windows and macOS interactive installations, and all other use cases, run the interactive installer to install the latest release of Cloud SDK. What's in the box? \u2691 All of the installation methods above install the default Cloud SDK components, which include gcloud , gsutil and bq command-line tools. You can install additional components using the gcloud components install command, or by installing the appropriate deb or RPM packages. Managing an installation \u2691 After you have installed Cloud SDK, you can use commands in the gcloud components command group to manage your installation . This includes viewing installed components, adding and removing components, and upgrading to a new version (or downgrading to a specific version) of Cloud SDK. Note: Updating and removing components using gcloud components is disabled if you installed Cloud SDK using apt-get or yum . To manage the Cloud SDK in this case, continue using the package management tool used during installation. Older versions of Cloud SDK \u2691 If you'd need an older version of Cloud SDK to revert to, you can find all previous releases available to download from this archive . Links: Install gcloud SDK on Ubuntu | [[GCP/Google Cloud Setup Notes]] Source: Installing Google Cloud SDK (Windows) | Cloud SDK Documentation","title":"Install gcloud SDK on Windows"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Windows/#install-gcloud-sdk-on-windows","text":"Download the Cloud SDK installer . Alternatively, open a PowerShell terminal and run the following PowerShell commands. ( New \\ -Object Net . WebClient ). DownloadFile ( \"https://dl.google.com/dl/cloudsdk/channels/rapid/GoogleCloudSDKInstaller.exe\" , \"$env:Temp\\\\GoogleCloudSDKInstaller.exe\" ) & $env:Temp \\\\ GoogleCloudSDKInstaller . exe Launch the installer and follow the prompts. The installer is signed by Google LLC. If you'd like to enable screen reader mode, select the Turn on screen reader mode option for a more streamlined screen reader experience. To read more about the Cloud SDK screen reader experience, refer to the Accessibility features guide . Cloud SDK requires Python; supported versions are Python 3 (preferred, 3.5 to 3.8) and Python 2 (2.7.9 or higher). The installer will install all necessary dependencies, including the needed Python version. While Cloud SDK currently uses Python 3 by default, you can use an existing Python installation if necessary by unchecking the option to Install Bundled Python. After installation has completed, the installer presents several options: Make sure that the following are selected: Start Google Cloud SDK Shell Run gcloud init The installer starts a terminal window and runs the gcloud init command. The default installation does not include the App Engine extensions required to deploy an application using gcloud commands. These components can be installed using the Cloud SDK component manager . Troubleshooting tips: If the Cloud SDK fails to run after installing version 274.0.0, please refer to this tracking bug for the latest workarounds. If your installation is unsuccessful due to the find command not being recognized, ensure your PATH environment variable is set to include the folder containing find . Usually, this is C:\\WINDOWS\\system32; . If you have just uninstalled Cloud SDK, you will need to reboot your system before installing Cloud SDK again.","title":"Install gcloud SDK on Windows"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Windows/#optional-install-the-latest-google-cloud-client-libraries","text":"You can download Cloud Client Libraries for supported languages.","title":"Optional: Install the latest Google Cloud Client Libraries"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Windows/#other-installation-options","text":"Depending on your development needs, instead of the recommended installation , you can use an alternative method of installing Cloud SDK: Using Cloud SDK with scripts or Continuous Integration/Deployment? Download a versioned archive for a non-interactive installation of a specific version of Cloud SDK. Need to run Cloud SDK as a Docker image? Use the Cloud SDK Docker image for the latest release (or specific version) of Cloud SDK. Running Ubuntu and prefer automatic updates? Use a snap package to install the Cloud SDK. For Windows and macOS interactive installations, and all other use cases, run the interactive installer to install the latest release of Cloud SDK.","title":"Other installation options"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Windows/#whats-in-the-box","text":"All of the installation methods above install the default Cloud SDK components, which include gcloud , gsutil and bq command-line tools. You can install additional components using the gcloud components install command, or by installing the appropriate deb or RPM packages.","title":"What's in the box?"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Windows/#managing-an-installation","text":"After you have installed Cloud SDK, you can use commands in the gcloud components command group to manage your installation . This includes viewing installed components, adding and removing components, and upgrading to a new version (or downgrading to a specific version) of Cloud SDK. Note: Updating and removing components using gcloud components is disabled if you installed Cloud SDK using apt-get or yum . To manage the Cloud SDK in this case, continue using the package management tool used during installation.","title":"Managing an installation"},{"location":"GCP/Install%20gcloud%20SDK%20on%20Windows/#older-versions-of-cloud-sdk","text":"If you'd need an older version of Cloud SDK to revert to, you can find all previous releases available to download from this archive . Links: Install gcloud SDK on Ubuntu | [[GCP/Google Cloud Setup Notes]] Source: Installing Google Cloud SDK (Windows) | Cloud SDK Documentation","title":"Older versions of Cloud SDK"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/","text":"Setup Cloud SQL for PostgreSQL in Production \u2691 Deploying a Cloud SQL for PostgreSQL instance \u2691 You can set up a Cloud SQL for PostgreSQL instance in a few steps by using the Google Cloud Console or the gcloud command-line tool. Both methods are described here. Create the PostgreSQL instance: gcloud sql instances create postgresql01 - -cpu = 2 - -memory = 7680MB - -region = us-central1 - -zone = us-central1-a Assign a password for the PostgreSQL default user (syntax example): gcloud sql users set-password postgres - -instance < INSTANCE_NAME > - -password < PASSWORD > You can specify these additional options: Database version: One of the supported PostgreSQL versions . Storage type: Either SSD or HDD as the storage type. Storage capacity: The initial storage settings for the instance. Automatic storage increase: Cloud SQL automation for adding additional storage when free space runs low. High-availability: Cloud SQL high-availability. Automatic backups: The start-time window for backups. Point-in-time recovery: Point-in-time recovery and write-ahead logging. Maintenance window: A one-hour window when Cloud SQL can perform disruptive maintenance. Maintenance timing: The preferred timing for performing updates on the PostgreSQL instance. You can specify preview for earlier updates or production for later updates. Database flags: The PostgreSQL database flags for controlling settings and parameters. The following gcloud command creates a Cloud SQL for PostgreSQL instance with some additional options: gcloud sql instances create postgresql01 - -cpu = 2 - -memory = 7680MB - -region = us-central1 - -zone = us-central1-a \\ - -database-version = POSTGRES_12 - -storage-type = SSD - -storage-size = 100 - -storage-auto-increase \\ - -availability-type = regional - -backup-start-time = 23 : 30 - -enable-point-in-time-recovery - -maintenance-window-day = sun \\ - -maintenance-window-hour = 11 - -maintenance-release-channel = production - -database-flags max_connections = 100 For more information, see Creating instances . Instance selection \u2691 Instance selection or sizing involves selecting a machine type that can support your Oracle\u00ae workload on Cloud SQL for PostgreSQL. The instance types are divided into two main groups: Shared-core machines : Cost effective. Dedicated-core instances : Support multiple vCPUs and memory ratios. For more information about instance types, see Cloud SQL pricing . In order to size your instance, start by analyzing the resources allocated to and used by your source database. You can get the Oracle database resources settings from the V$OSSTAT system view or from an Oracle AWR report (see the following examples): Physical memory (total number of bytes of physical memory in the database server): SQL > SELECT ROUND ( MAX ( VALUE ) / 1024 / 1024 / 1024 ) AS MEM_SIZE_GB FROM V$OSSTAT WHERE STAT_NAME = 'PHYSICAL_MEMORY_BYTES' ; Allocated memory : SQL > SELECT NAME , VALUE , DISPLAY_VALUE FROM V$PARAMETER WHERE NAME LIKE '%sga%' OR NAME LIKE '%memory%' ; CPU cores (number of CPU cores available): SQL > SELECT VALUE FROM V$OSSTAT WHERE STAT_NAME = 'NUM_CPU_CORES' ; CPU cores (identified by an Oracle instance using the V$LICENSE view): SQL > SELECT CPU_CORE_COUNT_CURRENT FROM V$LICENSE ; Oracle AWR report resource example (An Oracle AWR report can provide additional insights about a specific Oracle instance's workload characteristics): When you have the resource information of your source database, we recommend choosing the closest matching Cloud SQL instance type and running some benchmarks. The results from your benchmarks help you finalize your instance selection. High-availability configuration \u2691 To implement a disaster recovery solution, similar to Oracle's Data Guard, Cloud SQL for PostgreSQL offers high-availability capabilities that provide automatic failover from the cluster's primary instance to its standby Instance. The standby instance is located in a different zone in the same region as the primary instance. The standby instance is kept in sync through synchronous replication between the persistent disks of the primary and standby instances. This method ensures that all data modifications to the primary are also applied to the standby. In the event of a primary failure, such as an unresponsive instance or zone-level failure, Cloud SQL performs an automatic failover. The primary instance is monitored by heartbeats, which occur in 1-second intervals, with a failover activating after approximately 60 seconds of no heartbeats received from the primary instance. At this point, the primary instance fails over to the standby, providing data access to applications or clients transparently, while the existing read-replicas remain operational. Note that unlike Active Data Guard, the standby instance is not open for reads while acting as a standby; with Cloud SQL, only read-replicas can be used to offload reads from the primary. You can enable the Cloud SQL for PostgreSQL high-availability (HA) feature when you create the instance or for an existing PostgreSQL instance. Here are the steps: Console gcloud Gcloud: Enable HA by setting the availability-type parameter to regional : gcloud sql instances create postgresql01 - -cpu = 2 - -memory = 7680MB - -region = us-central1 - -zone = us-central1-a - -availability-type = regional Check if an existing PostgreSQL instance has HA configured: gcloud sql instances describe < INSTANCE_NAME > If the output from this command includes availabilityType: REGIONAL , then HA is already enabled. If the output includes availabilityType: ZONAL , then HA is not configured and can be enabled using the patch command: gcloud sql instances patch INSTANCE \\ _NAME - -availability-type REGIONAL Initiate a failover test from the primary to the standby: gcloud sql instances failover < PRIMARY_INSTANCE_NAME > To fail back, run the same failover command on the new primary. Admin users and accounts \u2691 Two default PostgreSQL user accounts come with any Cloud SQL for PostgreSQL installation. These accounts are postgres and cloudsqlimportexport . postgres account \u2691 The postgres account is the administrator account and is equivalent to Oracle SYS or SYSTEM users under Cloud PaaS. Because Cloud SQL for PostgreSQL is a managed service, the postgres user, unlike Oracle SYS or SYSTEM users, is restricted from accessing certain system procedures and tables that require advanced privileges. The postgres user is part of the cloudsqlsuperuser role, and has the following attributes (privileges): CREATEROLE , CREATEDB , and LOGIN . It does not have the SUPERUSER or REPLICATION attributes. cloudsqlimportexport account \u2691 The cloudsqlimportexport account is created with the minimal set of privileges needed for CSV import/export operations. You have the option to create your own users to perform these operations, but if you don't, the default cloudsqlimportexport user is used. The cloudsqlimportexport user is a system user, and you cannot directly use it. Account management (add, delete, or change password) \u2691 Account management entails creating new user accounts, modifying the password of an existing account, and deleting an account that is no longer needed. You can perform these account operations through the Cloud Console, the gcloud tool, or the PostgreSQL client. List the existing user accounts: gcloud sql users list - -instance = postgresql01 The output is similar to the following: NAME HOSTPostgres Create the appuser user account, set the password, and delete appuser : gcloud sql users create appuser - -instance = postgresql01 - -password =< PASSWORD > gcloud sql users set-password appuser - -host =% - -instance = postgresql01 - -prompt-for-password gcloud sql users delete appuser - -instance = postgresql01 Monitoring and alerting \u2691 Cloud Logging is the main logging tool on Google Cloud. It is used for collecting and viewing a variety of monitoring logs for resources such as Cloud SQL for PostgreSQL. Cloud Logging lets you view logs for Cloud SQL for PostgreSQL filtered by event level (for example, Critical, Error, or Warning), event timeframe, and free text search, as in the following screenshot. PostgreSQL database instance monitoring \u2691 Oracle's main monitoring tools are Enterprise Manager and Grid/Cloud Control. These tools let you do real-time database instance monitoring at a database session and SQL statement level. Cloud SQL for PostgreSQL provides comparable monitoring capabilities through the Cloud Console. From there, you can get a summary view of your database instances, including CPU utilization, storage usage, memory usage, read/write operations, active connections, transactions per second, and ingress/egress bytes. Note that Google Cloud's operations suite provides additional monitoring metrics for Cloud SQL for PostgreSQL, such as auto-failover requests and replication lag between the primary and read-replicas. The following example graph shows a graph of transactions per second for the last 6 hours: Monitoring read-replicas \u2691 You can monitor read-replicas through the Cloud Console the same way that you monitor the primary instance. There are specific metrics for checking the replication status between the primary and read-replica instances. These metrics are used to populate the read-replica instance overview page in the Cloud Console. Alternatively, you can check the replication status from the command line: gcloud sql instances describe REPLICA \\ _NAME A third option is to check the replication status through a PostgreSQL client. The following PostgreSQL command checks the read-replica status: postgres => \\\\ x on Expanded display is on . postgres => select \\ * from pg \\ _stat \\ _replication ; - \\ [ RECORD 1 \\ ] ----+------------------------------------------- pid | 74733 usesysid | 16388 usename | cloudsqlreplica application \\ _name | PROJECT \\ _ID : REPLICA \\ _NAME client \\ _addr | REPLICA \\ _IP client \\ _hostname | client \\ _port | 41660 backend \\ _start | 2020 - 09 - 28 06 : 59 : 38 . 783981 + 00 backend \\ _xmin | state | streaming sent \\ _lsn | 0 / 2939 FFA8 write \\ _lsn | 0 / 2939 FFA8 flush \\ _lsn | 0 / 2939 FFA8 replay \\ _lsn | 0 / 2939 FFA8 write \\ _lag | flush \\ _lag | replay \\ _lag | sync \\ _priority | 0 sync \\ _state | async reply \\ _time | 2020 - 09 - 28 07 : 17 : 52 . 714969 + 00 postgres => PostgreSQL database monitoring \u2691 This section describes some additional monitoring tasks that are considered routine for a PostgreSQL DBA. Session monitoring \u2691 Oracle sessions are monitored by querying the dynamic performance views known as the \"V$\"views . The V$SESSION and the V$PROCESS views are commonly used to gain real-time insights about current database activity through SQL statements. You can monitor session activity in PostgreSQL in a similar manner, both through PostgreSQL commands and SQL statements. The PostgreSQL pg_stat_activity dynamic view provides detailed information on current database session activity: postgres => \\\\ x on postgres => select \\ * from pg \\ _stat \\ _activity where backend \\ _type = 'client backend' and usename != 'cloudsqladmin' ; - \\ [ RECORD 1 \\ ] ----+----------------------------------------------------------------------------------------------------- datid | 14052 datname | postgres pid | 74750 usesysid | 16389 usename | postgres application \\ _name | psql client \\ _addr | CLIENT \\ _IP client \\ _hostname | client \\ _port | 51904 backend \\ _start | 2020 - 09 - 28 07 : 01 : 30 . 214099 + 00 xact \\ _start | 2020 - 09 - 28 07 : 28 : 48 . 982115 + 00 query \\ _start | 2020 - 09 - 28 07 : 28 : 48 . 982115 + 00 state \\ _change | 2020 - 09 - 28 07 : 28 : 48 . 982117 + 00 wait \\ _event \\ _type | wait \\ _event | state | active backend \\ _xid | backend \\ _xmin | 88513 query | select \\ * from pg \\ _stat \\ _activity where backend \\ _type = 'client backend' and usename != 'cloudsqladmin' ; backend \\ _type | client backend postgres => Long transaction monitoring \u2691 In order to identify long running transactions that might lead to performance issues, query the pg_stat_activity dynamic view. You can identify long-running queries by applying appropriate filters on columns such as query_start and state . Locks monitoring \u2691 You can monitor database locks through the pg_locks dynamic view, which provides real-time information about any lock contention that can lead to performance issues. Alerting \u2691 You can use alerting in addition to monitoring and logging. You can also create alerts for conditions. Scaling \u2691 Cloud SQL for PostgreSQL supports both vertical and horizontal scaling options. You scale vertically by adding more resources to the Cloud SQL instance, such as increasing the instance-assigned number of CPUs and memory . Network throughput of your instance depends on the values you choose for CPU and memory. Cloud SQL supports up to 30 TB of storage space. Adding storage capacity generally increases throughput and disk IOPs of an instance. Note that network throughput of a Cloud SQL instance includes reads/writes of your data (disk throughput) as well as the content of queries, calculations, and other data not stored on your database. It's important to consider these factors when vertically scaling your Cloud SQL instance. You scale horizontally by creating read replicas . Read replicas let you scale your read workloads onto separate Cloud SQL instances without affecting the performance and availability of the primary instance. Backup and recovery \u2691 There are two database backup methods for Cloud SQL for PostgreSQL: on-demand and automated. You can do on-demand backups anytime and they are persisted until you delete them. Automated backups use a 4-hour backup window and are retained for 7 days. You can restore Cloud SQL for PostgreSQL database backups to the same instance, overwriting the existing data, or to a new instance. In addition, Cloud SQL for PostgreSQL lets you restore a PostgreSQL database to a specific point in time as long as point-in-time recovery is turned on and the automated backup option is enabled. Cloud SQL for PostgreSQL provides database cloning capabilities. The clone must be created from the primary instance (that is, it cannot be created from a replica). You can run database backups, restores, and clones from either the Cloud Console or the gcloud tool. Automation \u2691 You can use the Cloud SQL Admin API to completely automate administering a Cloud SQL for PostgreSQL instance. The Cloud SQL Admin API is a REST API for controlling different types of resources such as Instances, Databases, Users, Flags, Operations, SslCerts, Tiers, and BackupRuns. For more information, see the API documentation . What's next \u2691 Explore more about Cloud SQL for PostgreSQL user accounts. Learn more about Cloud SQL for PostgreSQL for Oracle users: Migrating Oracle users to Cloud SQL for PostgreSQL: Terminology and functionality Migrating Oracle users to Cloud SQL for PostgreSQL: Data types, users, and tables Migrating Oracle users to Cloud SQL for PostgreSQL: Queries, stored procedures, functions, and triggers Migrating Oracle users to Cloud SQL for PostgreSQL: Security, operations, monitoring, and logging Explore reference architectures, diagrams, tutorials, and best practices about Google Cloud. Take a look at our Cloud Architecture Center . Links: Google Cloud APIs | [[GCP/Google Cloud Setup Notes]] Source: Setting up Cloud SQL for PostgreSQL for production use | Solutions (google.com)","title":"Setup Cloud SQL for PostgreSQL in Production"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#setup-cloud-sql-for-postgresql-in-production","text":"","title":"Setup Cloud SQL for PostgreSQL in Production"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#deploying-a-cloud-sql-for-postgresql-instance","text":"You can set up a Cloud SQL for PostgreSQL instance in a few steps by using the Google Cloud Console or the gcloud command-line tool. Both methods are described here. Create the PostgreSQL instance: gcloud sql instances create postgresql01 - -cpu = 2 - -memory = 7680MB - -region = us-central1 - -zone = us-central1-a Assign a password for the PostgreSQL default user (syntax example): gcloud sql users set-password postgres - -instance < INSTANCE_NAME > - -password < PASSWORD > You can specify these additional options: Database version: One of the supported PostgreSQL versions . Storage type: Either SSD or HDD as the storage type. Storage capacity: The initial storage settings for the instance. Automatic storage increase: Cloud SQL automation for adding additional storage when free space runs low. High-availability: Cloud SQL high-availability. Automatic backups: The start-time window for backups. Point-in-time recovery: Point-in-time recovery and write-ahead logging. Maintenance window: A one-hour window when Cloud SQL can perform disruptive maintenance. Maintenance timing: The preferred timing for performing updates on the PostgreSQL instance. You can specify preview for earlier updates or production for later updates. Database flags: The PostgreSQL database flags for controlling settings and parameters. The following gcloud command creates a Cloud SQL for PostgreSQL instance with some additional options: gcloud sql instances create postgresql01 - -cpu = 2 - -memory = 7680MB - -region = us-central1 - -zone = us-central1-a \\ - -database-version = POSTGRES_12 - -storage-type = SSD - -storage-size = 100 - -storage-auto-increase \\ - -availability-type = regional - -backup-start-time = 23 : 30 - -enable-point-in-time-recovery - -maintenance-window-day = sun \\ - -maintenance-window-hour = 11 - -maintenance-release-channel = production - -database-flags max_connections = 100 For more information, see Creating instances .","title":"Deploying a Cloud SQL for PostgreSQL instance"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#instance-selection","text":"Instance selection or sizing involves selecting a machine type that can support your Oracle\u00ae workload on Cloud SQL for PostgreSQL. The instance types are divided into two main groups: Shared-core machines : Cost effective. Dedicated-core instances : Support multiple vCPUs and memory ratios. For more information about instance types, see Cloud SQL pricing . In order to size your instance, start by analyzing the resources allocated to and used by your source database. You can get the Oracle database resources settings from the V$OSSTAT system view or from an Oracle AWR report (see the following examples): Physical memory (total number of bytes of physical memory in the database server): SQL > SELECT ROUND ( MAX ( VALUE ) / 1024 / 1024 / 1024 ) AS MEM_SIZE_GB FROM V$OSSTAT WHERE STAT_NAME = 'PHYSICAL_MEMORY_BYTES' ; Allocated memory : SQL > SELECT NAME , VALUE , DISPLAY_VALUE FROM V$PARAMETER WHERE NAME LIKE '%sga%' OR NAME LIKE '%memory%' ; CPU cores (number of CPU cores available): SQL > SELECT VALUE FROM V$OSSTAT WHERE STAT_NAME = 'NUM_CPU_CORES' ; CPU cores (identified by an Oracle instance using the V$LICENSE view): SQL > SELECT CPU_CORE_COUNT_CURRENT FROM V$LICENSE ; Oracle AWR report resource example (An Oracle AWR report can provide additional insights about a specific Oracle instance's workload characteristics): When you have the resource information of your source database, we recommend choosing the closest matching Cloud SQL instance type and running some benchmarks. The results from your benchmarks help you finalize your instance selection.","title":"Instance selection"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#high-availability-configuration","text":"To implement a disaster recovery solution, similar to Oracle's Data Guard, Cloud SQL for PostgreSQL offers high-availability capabilities that provide automatic failover from the cluster's primary instance to its standby Instance. The standby instance is located in a different zone in the same region as the primary instance. The standby instance is kept in sync through synchronous replication between the persistent disks of the primary and standby instances. This method ensures that all data modifications to the primary are also applied to the standby. In the event of a primary failure, such as an unresponsive instance or zone-level failure, Cloud SQL performs an automatic failover. The primary instance is monitored by heartbeats, which occur in 1-second intervals, with a failover activating after approximately 60 seconds of no heartbeats received from the primary instance. At this point, the primary instance fails over to the standby, providing data access to applications or clients transparently, while the existing read-replicas remain operational. Note that unlike Active Data Guard, the standby instance is not open for reads while acting as a standby; with Cloud SQL, only read-replicas can be used to offload reads from the primary. You can enable the Cloud SQL for PostgreSQL high-availability (HA) feature when you create the instance or for an existing PostgreSQL instance. Here are the steps: Console gcloud Gcloud: Enable HA by setting the availability-type parameter to regional : gcloud sql instances create postgresql01 - -cpu = 2 - -memory = 7680MB - -region = us-central1 - -zone = us-central1-a - -availability-type = regional Check if an existing PostgreSQL instance has HA configured: gcloud sql instances describe < INSTANCE_NAME > If the output from this command includes availabilityType: REGIONAL , then HA is already enabled. If the output includes availabilityType: ZONAL , then HA is not configured and can be enabled using the patch command: gcloud sql instances patch INSTANCE \\ _NAME - -availability-type REGIONAL Initiate a failover test from the primary to the standby: gcloud sql instances failover < PRIMARY_INSTANCE_NAME > To fail back, run the same failover command on the new primary.","title":"High-availability configuration"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#admin-users-and-accounts","text":"Two default PostgreSQL user accounts come with any Cloud SQL for PostgreSQL installation. These accounts are postgres and cloudsqlimportexport .","title":"Admin users and accounts"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#postgres-account","text":"The postgres account is the administrator account and is equivalent to Oracle SYS or SYSTEM users under Cloud PaaS. Because Cloud SQL for PostgreSQL is a managed service, the postgres user, unlike Oracle SYS or SYSTEM users, is restricted from accessing certain system procedures and tables that require advanced privileges. The postgres user is part of the cloudsqlsuperuser role, and has the following attributes (privileges): CREATEROLE , CREATEDB , and LOGIN . It does not have the SUPERUSER or REPLICATION attributes.","title":"postgres account"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#cloudsqlimportexport-account","text":"The cloudsqlimportexport account is created with the minimal set of privileges needed for CSV import/export operations. You have the option to create your own users to perform these operations, but if you don't, the default cloudsqlimportexport user is used. The cloudsqlimportexport user is a system user, and you cannot directly use it.","title":"cloudsqlimportexport account"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#account-management-add-delete-or-change-password","text":"Account management entails creating new user accounts, modifying the password of an existing account, and deleting an account that is no longer needed. You can perform these account operations through the Cloud Console, the gcloud tool, or the PostgreSQL client. List the existing user accounts: gcloud sql users list - -instance = postgresql01 The output is similar to the following: NAME HOSTPostgres Create the appuser user account, set the password, and delete appuser : gcloud sql users create appuser - -instance = postgresql01 - -password =< PASSWORD > gcloud sql users set-password appuser - -host =% - -instance = postgresql01 - -prompt-for-password gcloud sql users delete appuser - -instance = postgresql01","title":"Account management (add, delete, or change password)"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#monitoring-and-alerting","text":"Cloud Logging is the main logging tool on Google Cloud. It is used for collecting and viewing a variety of monitoring logs for resources such as Cloud SQL for PostgreSQL. Cloud Logging lets you view logs for Cloud SQL for PostgreSQL filtered by event level (for example, Critical, Error, or Warning), event timeframe, and free text search, as in the following screenshot.","title":"Monitoring and alerting"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#postgresql-database-instance-monitoring","text":"Oracle's main monitoring tools are Enterprise Manager and Grid/Cloud Control. These tools let you do real-time database instance monitoring at a database session and SQL statement level. Cloud SQL for PostgreSQL provides comparable monitoring capabilities through the Cloud Console. From there, you can get a summary view of your database instances, including CPU utilization, storage usage, memory usage, read/write operations, active connections, transactions per second, and ingress/egress bytes. Note that Google Cloud's operations suite provides additional monitoring metrics for Cloud SQL for PostgreSQL, such as auto-failover requests and replication lag between the primary and read-replicas. The following example graph shows a graph of transactions per second for the last 6 hours:","title":"PostgreSQL database instance monitoring"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#monitoring-read-replicas","text":"You can monitor read-replicas through the Cloud Console the same way that you monitor the primary instance. There are specific metrics for checking the replication status between the primary and read-replica instances. These metrics are used to populate the read-replica instance overview page in the Cloud Console. Alternatively, you can check the replication status from the command line: gcloud sql instances describe REPLICA \\ _NAME A third option is to check the replication status through a PostgreSQL client. The following PostgreSQL command checks the read-replica status: postgres => \\\\ x on Expanded display is on . postgres => select \\ * from pg \\ _stat \\ _replication ; - \\ [ RECORD 1 \\ ] ----+------------------------------------------- pid | 74733 usesysid | 16388 usename | cloudsqlreplica application \\ _name | PROJECT \\ _ID : REPLICA \\ _NAME client \\ _addr | REPLICA \\ _IP client \\ _hostname | client \\ _port | 41660 backend \\ _start | 2020 - 09 - 28 06 : 59 : 38 . 783981 + 00 backend \\ _xmin | state | streaming sent \\ _lsn | 0 / 2939 FFA8 write \\ _lsn | 0 / 2939 FFA8 flush \\ _lsn | 0 / 2939 FFA8 replay \\ _lsn | 0 / 2939 FFA8 write \\ _lag | flush \\ _lag | replay \\ _lag | sync \\ _priority | 0 sync \\ _state | async reply \\ _time | 2020 - 09 - 28 07 : 17 : 52 . 714969 + 00 postgres =>","title":"Monitoring read-replicas"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#postgresql-database-monitoring","text":"This section describes some additional monitoring tasks that are considered routine for a PostgreSQL DBA.","title":"PostgreSQL database monitoring"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#session-monitoring","text":"Oracle sessions are monitored by querying the dynamic performance views known as the \"V$\"views . The V$SESSION and the V$PROCESS views are commonly used to gain real-time insights about current database activity through SQL statements. You can monitor session activity in PostgreSQL in a similar manner, both through PostgreSQL commands and SQL statements. The PostgreSQL pg_stat_activity dynamic view provides detailed information on current database session activity: postgres => \\\\ x on postgres => select \\ * from pg \\ _stat \\ _activity where backend \\ _type = 'client backend' and usename != 'cloudsqladmin' ; - \\ [ RECORD 1 \\ ] ----+----------------------------------------------------------------------------------------------------- datid | 14052 datname | postgres pid | 74750 usesysid | 16389 usename | postgres application \\ _name | psql client \\ _addr | CLIENT \\ _IP client \\ _hostname | client \\ _port | 51904 backend \\ _start | 2020 - 09 - 28 07 : 01 : 30 . 214099 + 00 xact \\ _start | 2020 - 09 - 28 07 : 28 : 48 . 982115 + 00 query \\ _start | 2020 - 09 - 28 07 : 28 : 48 . 982115 + 00 state \\ _change | 2020 - 09 - 28 07 : 28 : 48 . 982117 + 00 wait \\ _event \\ _type | wait \\ _event | state | active backend \\ _xid | backend \\ _xmin | 88513 query | select \\ * from pg \\ _stat \\ _activity where backend \\ _type = 'client backend' and usename != 'cloudsqladmin' ; backend \\ _type | client backend postgres =>","title":"Session monitoring"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#long-transaction-monitoring","text":"In order to identify long running transactions that might lead to performance issues, query the pg_stat_activity dynamic view. You can identify long-running queries by applying appropriate filters on columns such as query_start and state .","title":"Long transaction monitoring"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#locks-monitoring","text":"You can monitor database locks through the pg_locks dynamic view, which provides real-time information about any lock contention that can lead to performance issues.","title":"Locks monitoring"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#alerting","text":"You can use alerting in addition to monitoring and logging. You can also create alerts for conditions.","title":"Alerting"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#scaling","text":"Cloud SQL for PostgreSQL supports both vertical and horizontal scaling options. You scale vertically by adding more resources to the Cloud SQL instance, such as increasing the instance-assigned number of CPUs and memory . Network throughput of your instance depends on the values you choose for CPU and memory. Cloud SQL supports up to 30 TB of storage space. Adding storage capacity generally increases throughput and disk IOPs of an instance. Note that network throughput of a Cloud SQL instance includes reads/writes of your data (disk throughput) as well as the content of queries, calculations, and other data not stored on your database. It's important to consider these factors when vertically scaling your Cloud SQL instance. You scale horizontally by creating read replicas . Read replicas let you scale your read workloads onto separate Cloud SQL instances without affecting the performance and availability of the primary instance.","title":"Scaling"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#backup-and-recovery","text":"There are two database backup methods for Cloud SQL for PostgreSQL: on-demand and automated. You can do on-demand backups anytime and they are persisted until you delete them. Automated backups use a 4-hour backup window and are retained for 7 days. You can restore Cloud SQL for PostgreSQL database backups to the same instance, overwriting the existing data, or to a new instance. In addition, Cloud SQL for PostgreSQL lets you restore a PostgreSQL database to a specific point in time as long as point-in-time recovery is turned on and the automated backup option is enabled. Cloud SQL for PostgreSQL provides database cloning capabilities. The clone must be created from the primary instance (that is, it cannot be created from a replica). You can run database backups, restores, and clones from either the Cloud Console or the gcloud tool.","title":"Backup and recovery"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#automation","text":"You can use the Cloud SQL Admin API to completely automate administering a Cloud SQL for PostgreSQL instance. The Cloud SQL Admin API is a REST API for controlling different types of resources such as Instances, Databases, Users, Flags, Operations, SslCerts, Tiers, and BackupRuns. For more information, see the API documentation .","title":"Automation"},{"location":"GCP/Setup%20Cloud%20SQL%20for%20PostgreSQL%20in%20Production/#whats-next","text":"Explore more about Cloud SQL for PostgreSQL user accounts. Learn more about Cloud SQL for PostgreSQL for Oracle users: Migrating Oracle users to Cloud SQL for PostgreSQL: Terminology and functionality Migrating Oracle users to Cloud SQL for PostgreSQL: Data types, users, and tables Migrating Oracle users to Cloud SQL for PostgreSQL: Queries, stored procedures, functions, and triggers Migrating Oracle users to Cloud SQL for PostgreSQL: Security, operations, monitoring, and logging Explore reference architectures, diagrams, tutorials, and best practices about Google Cloud. Take a look at our Cloud Architecture Center . Links: Google Cloud APIs | [[GCP/Google Cloud Setup Notes]] Source: Setting up Cloud SQL for PostgreSQL for production use | Solutions (google.com)","title":"What's next"},{"location":"Git/","text":"Git \u2691 Categories \u2691 Documents \u2691 Git Tools","title":"Git"},{"location":"Git/#git","text":"","title":"Git"},{"location":"Git/#categories","text":"","title":"Categories"},{"location":"Git/#documents","text":"Git Tools","title":"Documents"},{"location":"Git/Git%20Tools/","text":"Git Tools \u2691 Links: Source:","title":"Git Tools"},{"location":"Git/Git%20Tools/#git-tools","text":"Links: Source:","title":"Git Tools"},{"location":"Github/","text":"Github \u2691 Categories \u2691 Documents \u2691 Github Actions for R","title":"Github"},{"location":"Github/#github","text":"","title":"Github"},{"location":"Github/#categories","text":"","title":"Categories"},{"location":"Github/#documents","text":"Github Actions for R","title":"Documents"},{"location":"Github/Github%20Actions%20for%20R/","text":"Github Actions for R \u2691 R CMD Check \u2691 check-full.yml # NOTE: This workflow is overkill for most R packages # check-standard.yaml is likely a better choice # usethis::use_github_action(\"check-standard\") will install it. # # For help debugging build failures open an issue on the RStudio community with the 'github-actions' tag. # https://community.rstudio.com/new-topic?category=Package%20development&tags=github-actions on : push : branches : - main - master pull_request : branches : - main - master name : R-CMD-check jobs : R-CMD-check : runs-on : ${{ matrix.config.os }} name : ${{ matrix.config.os }} (${{ matrix.config.r }}) strategy : fail-fast : false matrix : config : - { os : macOS-latest , r : 'release' } - { os : windows-latest , r : 'release' } - { os : windows-latest , r : '3.6' } - { os : ubuntu-18.04 , r : 'devel' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/bionic/latest\" , http-user-agent : \"R/4.0.0 (ubuntu-18.04) R (4.0.0 x86_64-pc-linux-gnu x86_64 linux-gnu) on GitHub Actions\" } - { os : ubuntu-18.04 , r : 'release' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/bionic/latest\" } - { os : ubuntu-18.04 , r : 'oldrel' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/bionic/latest\" } - { os : ubuntu-18.04 , r : '3.5' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/bionic/latest\" } - { os : ubuntu-18.04 , r : '3.4' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/bionic/latest\" } - { os : ubuntu-18.04 , r : '3.3' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/bionic/latest\" } env : RSPM : ${{ matrix.config.rspm }} GITHUB_PAT : ${{ secrets.GITHUB_TOKEN }} steps : - uses : actions/checkout@v2 - uses : r-lib/actions/setup-r@v1 id : install-r with : r-version : ${{ matrix.config.r }} http-user-agent : ${{ matrix.config.http-user-agent }} - uses : r-lib/actions/setup-pandoc@v1 - name : Install pak and query dependencies run : | install.packages(\"pak\", repos = \"https://r-lib.github.io/p/pak/dev/\") saveRDS(pak::pkg_deps(\"local::.\", dependencies = TRUE), \".github/r-depends.rds\") shell : Rscript {0} - name : Restore R package cache uses : actions/cache@v2 with : path : | ${{ env.R_LIBS_USER }}/* !${{ env.R_LIBS_USER }}/pak key : ${{ matrix.config.os }}-${{ steps.install-r.outputs.installed-r-version }}-1-${{ hashFiles('.github/r-depends.rds') }} restore-keys : ${{ matrix.config.os }}-${{ steps.install-r.outputs.installed-r-version }}-1- - name : Install system dependencies if : runner.os == 'Linux' run : | pak::local_system_requirements(execute = TRUE) pak::pkg_system_requirements(\"rcmdcheck\", execute = TRUE) shell : Rscript {0} - name : Install dependencies run : | pak::local_install_dev_deps(upgrade = TRUE) pak::pkg_install(\"rcmdcheck\") shell : Rscript {0} - name : Session info run : | options(width = 100) pkgs <- installed.packages()[, \"Package\"] sessioninfo::session_info(pkgs, include_base = TRUE) shell : Rscript {0} - name : Check env : _R_CHECK_CRAN_INCOMING_ : false run : | options(crayon.enabled = TRUE) rcmdcheck::rcmdcheck(args = c(\"--no-manual\", \"--as-cran\"), error_on = \"warning\", check_dir = \"check\") shell : Rscript {0} - name : Show testthat output if : always() run : find check -name 'testthat.Rout*' -exec cat '{}' \\; || true shell : bash - name : Upload check results if : failure() uses : actions/upload-artifact@main with : name : ${{ matrix.config.os }}-r${{ matrix.config.r }}-results path : check check-standard.yml # For help debugging build failures open an issue on the RStudio community with the 'github-actions' tag. # https://community.rstudio.com/new-topic?category=Package%20development&tags=github-actions on : push : branches : - main - master pull_request : branches : - main - master name : R-CMD-check jobs : R-CMD-check : runs-on : ${{ matrix.config.os }} name : ${{ matrix.config.os }} (${{ matrix.config.r }}) strategy : fail-fast : false matrix : config : - { os : windows-latest , r : 'release' } - { os : macOS-latest , r : 'release' } - { os : ubuntu-20.04 , r : 'release' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/focal/latest\" } - { os : ubuntu-20.04 , r : 'devel' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/focal/latest\" } env : R_REMOTES_NO_ERRORS_FROM_WARNINGS : true RSPM : ${{ matrix.config.rspm }} GITHUB_PAT : ${{ secrets.GITHUB_TOKEN }} steps : - uses : actions/checkout@v2 - uses : r-lib/actions/setup-r@v1 with : r-version : ${{ matrix.config.r }} - uses : r-lib/actions/setup-pandoc@v1 - name : Query dependencies run : | install.packages('remotes') saveRDS(remotes::dev_package_deps(dependencies = TRUE), \".github/depends.Rds\", version = 2) writeLines(sprintf(\"R-%i.%i\", getRversion()$major, getRversion()$minor), \".github/R-version\") shell : Rscript {0} - name : Restore R package cache if : runner.os != 'Windows' uses : actions/cache@v2 with : path : ${{ env.R_LIBS_USER }} key : ${{ runner.os }}-${{ hashFiles('.github/R-version') }}-1-${{ hashFiles('.github/depends.Rds') }} restore-keys : ${{ runner.os }}-${{ hashFiles('.github/R-version') }}-1- - name : Install system dependencies if : runner.os == 'Linux' run : | while read -r cmd do eval sudo $cmd done < <(Rscript -e 'writeLines(remotes::system_requirements(\"ubuntu\", \"20.04\"))') - name : Install dependencies run : | remotes::install_deps(dependencies = TRUE) remotes::install_cran(\"rcmdcheck\") shell : Rscript {0} - name : Check env : _R_CHECK_CRAN_INCOMING_REMOTE_ : false run : | options(crayon.enabled = TRUE) rcmdcheck::rcmdcheck(args = c(\"--no-manual\", \"--as-cran\"), error_on = \"warning\", check_dir = \"check\") shell : Rscript {0} - name : Upload check results if : failure() uses : actions/upload-artifact@main with : name : ${{ runner.os }}-r${{ matrix.config.r }}-results path : check Links: Source:","title":"Github Actions for R"},{"location":"Github/Github%20Actions%20for%20R/#github-actions-for-r","text":"","title":"Github Actions for R"},{"location":"Github/Github%20Actions%20for%20R/#r-cmd-check","text":"check-full.yml # NOTE: This workflow is overkill for most R packages # check-standard.yaml is likely a better choice # usethis::use_github_action(\"check-standard\") will install it. # # For help debugging build failures open an issue on the RStudio community with the 'github-actions' tag. # https://community.rstudio.com/new-topic?category=Package%20development&tags=github-actions on : push : branches : - main - master pull_request : branches : - main - master name : R-CMD-check jobs : R-CMD-check : runs-on : ${{ matrix.config.os }} name : ${{ matrix.config.os }} (${{ matrix.config.r }}) strategy : fail-fast : false matrix : config : - { os : macOS-latest , r : 'release' } - { os : windows-latest , r : 'release' } - { os : windows-latest , r : '3.6' } - { os : ubuntu-18.04 , r : 'devel' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/bionic/latest\" , http-user-agent : \"R/4.0.0 (ubuntu-18.04) R (4.0.0 x86_64-pc-linux-gnu x86_64 linux-gnu) on GitHub Actions\" } - { os : ubuntu-18.04 , r : 'release' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/bionic/latest\" } - { os : ubuntu-18.04 , r : 'oldrel' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/bionic/latest\" } - { os : ubuntu-18.04 , r : '3.5' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/bionic/latest\" } - { os : ubuntu-18.04 , r : '3.4' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/bionic/latest\" } - { os : ubuntu-18.04 , r : '3.3' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/bionic/latest\" } env : RSPM : ${{ matrix.config.rspm }} GITHUB_PAT : ${{ secrets.GITHUB_TOKEN }} steps : - uses : actions/checkout@v2 - uses : r-lib/actions/setup-r@v1 id : install-r with : r-version : ${{ matrix.config.r }} http-user-agent : ${{ matrix.config.http-user-agent }} - uses : r-lib/actions/setup-pandoc@v1 - name : Install pak and query dependencies run : | install.packages(\"pak\", repos = \"https://r-lib.github.io/p/pak/dev/\") saveRDS(pak::pkg_deps(\"local::.\", dependencies = TRUE), \".github/r-depends.rds\") shell : Rscript {0} - name : Restore R package cache uses : actions/cache@v2 with : path : | ${{ env.R_LIBS_USER }}/* !${{ env.R_LIBS_USER }}/pak key : ${{ matrix.config.os }}-${{ steps.install-r.outputs.installed-r-version }}-1-${{ hashFiles('.github/r-depends.rds') }} restore-keys : ${{ matrix.config.os }}-${{ steps.install-r.outputs.installed-r-version }}-1- - name : Install system dependencies if : runner.os == 'Linux' run : | pak::local_system_requirements(execute = TRUE) pak::pkg_system_requirements(\"rcmdcheck\", execute = TRUE) shell : Rscript {0} - name : Install dependencies run : | pak::local_install_dev_deps(upgrade = TRUE) pak::pkg_install(\"rcmdcheck\") shell : Rscript {0} - name : Session info run : | options(width = 100) pkgs <- installed.packages()[, \"Package\"] sessioninfo::session_info(pkgs, include_base = TRUE) shell : Rscript {0} - name : Check env : _R_CHECK_CRAN_INCOMING_ : false run : | options(crayon.enabled = TRUE) rcmdcheck::rcmdcheck(args = c(\"--no-manual\", \"--as-cran\"), error_on = \"warning\", check_dir = \"check\") shell : Rscript {0} - name : Show testthat output if : always() run : find check -name 'testthat.Rout*' -exec cat '{}' \\; || true shell : bash - name : Upload check results if : failure() uses : actions/upload-artifact@main with : name : ${{ matrix.config.os }}-r${{ matrix.config.r }}-results path : check check-standard.yml # For help debugging build failures open an issue on the RStudio community with the 'github-actions' tag. # https://community.rstudio.com/new-topic?category=Package%20development&tags=github-actions on : push : branches : - main - master pull_request : branches : - main - master name : R-CMD-check jobs : R-CMD-check : runs-on : ${{ matrix.config.os }} name : ${{ matrix.config.os }} (${{ matrix.config.r }}) strategy : fail-fast : false matrix : config : - { os : windows-latest , r : 'release' } - { os : macOS-latest , r : 'release' } - { os : ubuntu-20.04 , r : 'release' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/focal/latest\" } - { os : ubuntu-20.04 , r : 'devel' , rspm : \"https://packagemanager.rstudio.com/cran/__linux__/focal/latest\" } env : R_REMOTES_NO_ERRORS_FROM_WARNINGS : true RSPM : ${{ matrix.config.rspm }} GITHUB_PAT : ${{ secrets.GITHUB_TOKEN }} steps : - uses : actions/checkout@v2 - uses : r-lib/actions/setup-r@v1 with : r-version : ${{ matrix.config.r }} - uses : r-lib/actions/setup-pandoc@v1 - name : Query dependencies run : | install.packages('remotes') saveRDS(remotes::dev_package_deps(dependencies = TRUE), \".github/depends.Rds\", version = 2) writeLines(sprintf(\"R-%i.%i\", getRversion()$major, getRversion()$minor), \".github/R-version\") shell : Rscript {0} - name : Restore R package cache if : runner.os != 'Windows' uses : actions/cache@v2 with : path : ${{ env.R_LIBS_USER }} key : ${{ runner.os }}-${{ hashFiles('.github/R-version') }}-1-${{ hashFiles('.github/depends.Rds') }} restore-keys : ${{ runner.os }}-${{ hashFiles('.github/R-version') }}-1- - name : Install system dependencies if : runner.os == 'Linux' run : | while read -r cmd do eval sudo $cmd done < <(Rscript -e 'writeLines(remotes::system_requirements(\"ubuntu\", \"20.04\"))') - name : Install dependencies run : | remotes::install_deps(dependencies = TRUE) remotes::install_cran(\"rcmdcheck\") shell : Rscript {0} - name : Check env : _R_CHECK_CRAN_INCOMING_REMOTE_ : false run : | options(crayon.enabled = TRUE) rcmdcheck::rcmdcheck(args = c(\"--no-manual\", \"--as-cran\"), error_on = \"warning\", check_dir = \"check\") shell : Rscript {0} - name : Upload check results if : failure() uses : actions/upload-artifact@main with : name : ${{ runner.os }}-r${{ matrix.config.r }}-results path : check Links: Source:","title":"R CMD Check"},{"location":"Lists/Recurring%20Shopping%20List/","text":"Recurring Shopping List \u2691 Amazon \u2691 Links: Source:","title":"Recurring Shopping List"},{"location":"Lists/Recurring%20Shopping%20List/#recurring-shopping-list","text":"","title":"Recurring Shopping List"},{"location":"Lists/Recurring%20Shopping%20List/#amazon","text":"Links: Source:","title":"Amazon"},{"location":"Obsidian/","text":"Obsidian \u2691 Categories \u2691 Documents \u2691 Obsidian Git Plugin Notes Obsidian Publishing Workflow Templater Plugin Notes","title":"Obsidian"},{"location":"Obsidian/#obsidian","text":"","title":"Obsidian"},{"location":"Obsidian/#categories","text":"","title":"Categories"},{"location":"Obsidian/#documents","text":"Obsidian Git Plugin Notes Obsidian Publishing Workflow Templater Plugin Notes","title":"Documents"},{"location":"Obsidian/Mermaid%20Diagrams/","text":"Mermaid Diagrams \u2691 This is a brief walkthrough on how to create mermaid diagrams with Obsidian using Markdown. Obsidian uses Mermaid to render diagrams and charts. Mermaid also provides a helpful live editor . ```mermaid sequenceDiagram Alice->>+John: Hello John, how are you? Alice->>+John: John, can you hear me? John-->>-Alice: Hi Alice, I can hear you! John-->>-Alice: I feel great! ``` sequenceDiagram Alice ->>+ John : Hello John , how are you ? Alice ->>+ John : John , can you hear me ? John -->>- Alice : Hi Alice , I can hear you ! John -->>- Alice : I feel great ! Links: Source:","title":"Mermaid Diagrams"},{"location":"Obsidian/Mermaid%20Diagrams/#mermaid-diagrams","text":"This is a brief walkthrough on how to create mermaid diagrams with Obsidian using Markdown. Obsidian uses Mermaid to render diagrams and charts. Mermaid also provides a helpful live editor . ```mermaid sequenceDiagram Alice->>+John: Hello John, how are you? Alice->>+John: John, can you hear me? John-->>-Alice: Hi Alice, I can hear you! John-->>-Alice: I feel great! ``` sequenceDiagram Alice ->>+ John : Hello John , how are you ? Alice ->>+ John : John , can you hear me ? John -->>- Alice : Hi Alice , I can hear you ! John -->>- Alice : I feel great ! Links: Source:","title":"Mermaid Diagrams"},{"location":"Obsidian/Obsidian%20Git%20Plugin%20Notes/","text":"Obsidian Git Plugin Notes \u2691 Initialize with Git \u2691 git init gh repo create --private touch .gitignore notepad .gitignore git add * git commit -m \"init\" git push --set-upstream origin master Links: Obsidian Source: denolehov/obsidian-git: Backup your Obsidian.md vault with git (github.com) obsidian-git-tut-windows/README.md at main \u00b7 gitobsidiantutorial/obsidian-git-tut-windows (github.com) ssh agent - How can I run ssh-add automatically, without a password prompt? - Unix & Linux Stack Exchange Adding an existing project to GitHub using the command line - GitHub Docs","title":"Obsidian Git Plugin Notes"},{"location":"Obsidian/Obsidian%20Git%20Plugin%20Notes/#obsidian-git-plugin-notes","text":"","title":"Obsidian Git Plugin Notes"},{"location":"Obsidian/Obsidian%20Git%20Plugin%20Notes/#initialize-with-git","text":"git init gh repo create --private touch .gitignore notepad .gitignore git add * git commit -m \"init\" git push --set-upstream origin master Links: Obsidian Source: denolehov/obsidian-git: Backup your Obsidian.md vault with git (github.com) obsidian-git-tut-windows/README.md at main \u00b7 gitobsidiantutorial/obsidian-git-tut-windows (github.com) ssh agent - How can I run ssh-add automatically, without a password prompt? - Unix & Linux Stack Exchange Adding an existing project to GitHub using the command line - GitHub Docs","title":"Initialize with Git"},{"location":"Obsidian/Obsidian/","text":"Obsidian \u2691 Contents \u2691 Templater Obsidian Git Github \u2691 \u2b50= Recommended obsidian-md \u00b7 GitHub Topics Plugins & Tools \u2692\ufe0f \u2691 denolehov/obsidian-git: Backup your Obsidian.md vault with git (github.com) \u2b50 argenos/zotero-mdnotes: A Zotero plugin to export item metadata and notes as markdown files (github.com) liamcain/obsidian-calendar-plugin: Simple calendar widget for Obsidian. (github.com) \u2b50 tgrosinger/advanced-tables-obsidian: Improved table navigation, formatting, and manipulation in Obsidian.md (github.com) \u2b50 deathau/sliding-panes-obsidian: Andy Matuschak Mode as a plugin (github.com) SilentVoid13/Templater: A template plugin for obsidian (github.com) \u2b50 jamiebrynes7/obsidian-todoist-plugin: Materialize Todoist tasks in Obsidian notes (github.com) \u2b50 tgrosinger/slated-obsidian: Task management in Obsidian.md (github.com) mgmeyers/obsidian-kanban (github.com) \u2b50 argenos/nldates-obsidian: Work with dates in natural language in Obsidian (github.com) phibr0/obsidian-charts: Charts - Obsidian Plugin | Create editable, interactive and animated Charts in Obsidian (github.com) st3v3nmw/obsidian-spaced-repetition: Fight the forgetting curve & note aging by reviewing flashcards & notes using spaced repetition on Obsidian.md (github.com) mrjackphil/obsidian-text-expand: A simple text expand plugin for Obsidian.md (github.com) \u2b50 jplattel/obsidian-query-language: An Obsidian plugin allowing you to query your notes (github.com) liamcain/obsidian-periodic-notes: Create/manage your daily, weekly, and monthly notes in Obsidian (github.com) denolehov/obsidian-url-into-selection: Paste URLs into selected text \"notion style\" (github.com) \u2b50 darlal/obsidian-switcher-plus: Enhanced Quick Switcher plugin for Obsidian.md (github.com) ryanjamurphy/review-obsidian: Add the current note to a future daily note to remember to review it. (github.com) FHachez/obsidian-convert-url-to-iframe: Plugin for Obsidian.md to convert a selected URL to an iframe. (github.com) \u2b50 visini/obsidian-icons-plugin: Add icons to your Obsidian notes \u2013 Experimental Obsidian Plugin (github.com) \u2b50 Yeboster/autocomplete-obsidian: Obsidian plugin to provide text autocomplete (github.com) \u2b50 akosbalasko/yarle: Yarle - The ultimate converter of Evernote notes to Markdown (github.com) HEmile/juggl: An interactive, stylable and expandable graph view for Obsidian. Juggl is designed as an advanced 'local' graph view, where you can juggle all your thoughts with ease. (github.com) lynchjames/note-refactor-obsidian: Allows for text selections to be copied (refactored) into new notes and notes to be split into other notes. (github.com) zoni/obsidian-export: Rust library and CLI to export an Obsidian vault to regular Markdown (github.com) \u2b50 Liamballin/ObsidianBookmark: Chrome extension and nodejs server to allow web clipping to Obsidian. (github.com) \u2b50 obsidian-userland/publish: Open source Obsidian Publish alternative (github.com) \u2b50 djsudduth/keep-it-markdown: Convert Google Keep notes dynamically to markdown for Obsidian and Notion using the unofficial Keep API (github.com) \u2b50 pjeby/tag-wrangler: Rename, merge, toggle, and search tags from the Obsidian tag pane (github.com) argenos/hotkeysplus-obsidian: Adds hotkeys to toggle todos, ordered/unordered lists and blockquotes in Obsidian (github.com) pjeby/hot-reload: Automatically reload Obsidian plugins in development when their files are changed (github.com) kepano/obsidian-minimal-settings: Settings plugin to control colors and fonts in Minimal Theme (github.com) kepano/obsidian-hider: Hide Obsidian UI elements such as tooltips, status, titlebar and more (github.com) Vinzent03/find-unlinked-files: Find files, which are nowhere linked, so they are maybe lost in your vault. (github.com) tallguyjenks/Obsidian-For-Business: Using Obsidian.... For Business! (github.com) StefanoCecere/markdown_pandoc_book_template: a template to create pdf/ePub/html/docx books by Markdown via Pandoc (github.com) akaalias/obsidian-extract-pdf-highlights: Extract highlights, underlines and annotations from your PDFs into Obsidian (github.com) ryanjamurphy/workbench-obsidian: A plugin to help you collect working materials. (github.com) mgmeyers/obsidian-style-settings: Dynamically creates a user interface for adjusting theme, plugin, and snippet CSS variables (github.com) pyrochlore/obsidian-tracker: Track everything in daily notes (github.com) deathau/cm-show-whitespace-obsidian: A plugin for [Obsidian](https://obsidian.md) which shows whitespace in the editor. (github.com) DahaWong/obsidian-footlinks: Obsidian plugin that extracts urls from the main text to footer, offering a better reading/editing experience. (github.com) lukeleppan/better-word-count: Counts the words of selected text in the editor. (github.com) renehernandez/obsidian-readwise: Sync Readwise highlights into your obsidian vault (github.com) zephraph/obsidian-tools: An unofficial collection of tools that helps you build plugins for obsidian.md (github.com) avr/obsidian-reading-time (github.com) avirut/obsidian-metatemplates: Take advantage of YAML front-matter in generating notes from templates (for obsidian.md) (github.com) mrjackphil/obsidian-jump-to-link: Quick jump between links using hotkeys (github.com) mrjackphil/obsidian-crosslink-between-notes: This plugin adds a command which allows to add a link to the current note at the bottom of selected notes (github.com) danymat/Obsidian-Markdown-Parser: This repository will give you tools to parse and fetch useful informations of your notes in your Obsidian vault. (github.com) avirut/obsidian-query2table: Represent files returned by a query as a table of their YAML frontmatter (for obsidian.md) (github.com) aviskase/obsidian-link-indexer (github.com) hadynz/obsidian-kindle-plugin: Sync your Kindle notes and highlights directly into your Obsidian vault (github.com) erichalldev/obsidian-smart-random-note: A smart random note plugin for Obsidian (github.com) pjeby/pane-relief: Obsidian plugin for per-pane history, pane movement/navigation hotkeys, and more (github.com) HEmile/obsidian-search-on-internet: Add context menu items in Obsidian to search the internet. (github.com) meld-cp/obsidian-encrypt: Hide secrets in your Obsidian.md vault (github.com) THeK3nger/obsidian-plugin-template: Template for Obsidian.md Plugins (github.com) tgrosinger/recent-files-obsidian: Display a list of most recently opened files (github.com) joethei/obsidian-plantuml: Generate PlantUML Diagrams inside Obsidian.md (github.com) ryanjamurphy/vantage-obsidian: Vantage helps you build complex queries using Obsidian's native search tools. (github.com) trashhalo/obsidian-extract-url: Plugin to extract markdown out of urls (github.com) gavvvr/obsidian-imgur-plugin: Pastes images right to imgur.com (github.com) lukeleppan/obsidian-discordrpc: Update your Discord Status to show your friends what you are working on in Obsidian. With Discord Rich Presence. (github.com) dhruvik7/obsidian-daily-stats: Plugin to view your daily word count across all notes in your Obsidian.md vault. (github.com) ze-kel/DayOne-JSON-to-MD: Converts jsons from Day One app to Markdown. Intended for transferring from DayOne to Obsidian but should work with everything else. (github.com) whateverforever/zettelwarmer: CLI Tool for Zettlr/Obsidian to help you browse random notes. The older the note, the more likely it will be shown. (github.com) liamcain/obsidian-things-logbook: Sync your Things 3 Logbook with Obsidian (github.com) akaalias/obsidian-shuffle: Create custom and randomized writing prompts (github.com) kepano/obsidian-advanced-appearance: Change Obsidian colors, fonts and other cosmetic settings (github.com) phibr0/cycle-through-panes: Cycle through Panes - Obsidian Plugin (github.com) liamcain/obsidian-creases: Mark headings to be collapsed by default (github.com) pjeby/note-folder-autorename: Obsidian plugin to support folder-overview notes by keeping their folder in sync (github.com) rbrcsk/note-tools: A collection of my tools related to notetaking (github.com) jobindj/obsidian-mkdocs: Publish Obsidian Notes with MkDocs (github.com) \u2b50 Themes \ud83c\udfa2 \u2691 kepano/obsidian-minimal: Minimal theme for Obsidian (github.com) jplattel/obsidian-clipper: A Chrome extension that easily clips selections to Obsidian (github.com) deathau/Notation-for-Obsidian: A theme for Obsidian, inspired by and borrowing elements from Notion (github.com) dxcore35/Suddha-theme: Obsidian theme (github.com) Developers \u2691 - tallguyjenks (Bryan Jenks) (github.com) \u2691 Links: PKM Source:","title":"Obsidian"},{"location":"Obsidian/Obsidian/#obsidian","text":"","title":"Obsidian"},{"location":"Obsidian/Obsidian/#contents","text":"Templater Obsidian Git","title":"Contents"},{"location":"Obsidian/Obsidian/#github","text":"\u2b50= Recommended obsidian-md \u00b7 GitHub Topics","title":"Github"},{"location":"Obsidian/Obsidian/#plugins-tools","text":"denolehov/obsidian-git: Backup your Obsidian.md vault with git (github.com) \u2b50 argenos/zotero-mdnotes: A Zotero plugin to export item metadata and notes as markdown files (github.com) liamcain/obsidian-calendar-plugin: Simple calendar widget for Obsidian. (github.com) \u2b50 tgrosinger/advanced-tables-obsidian: Improved table navigation, formatting, and manipulation in Obsidian.md (github.com) \u2b50 deathau/sliding-panes-obsidian: Andy Matuschak Mode as a plugin (github.com) SilentVoid13/Templater: A template plugin for obsidian (github.com) \u2b50 jamiebrynes7/obsidian-todoist-plugin: Materialize Todoist tasks in Obsidian notes (github.com) \u2b50 tgrosinger/slated-obsidian: Task management in Obsidian.md (github.com) mgmeyers/obsidian-kanban (github.com) \u2b50 argenos/nldates-obsidian: Work with dates in natural language in Obsidian (github.com) phibr0/obsidian-charts: Charts - Obsidian Plugin | Create editable, interactive and animated Charts in Obsidian (github.com) st3v3nmw/obsidian-spaced-repetition: Fight the forgetting curve & note aging by reviewing flashcards & notes using spaced repetition on Obsidian.md (github.com) mrjackphil/obsidian-text-expand: A simple text expand plugin for Obsidian.md (github.com) \u2b50 jplattel/obsidian-query-language: An Obsidian plugin allowing you to query your notes (github.com) liamcain/obsidian-periodic-notes: Create/manage your daily, weekly, and monthly notes in Obsidian (github.com) denolehov/obsidian-url-into-selection: Paste URLs into selected text \"notion style\" (github.com) \u2b50 darlal/obsidian-switcher-plus: Enhanced Quick Switcher plugin for Obsidian.md (github.com) ryanjamurphy/review-obsidian: Add the current note to a future daily note to remember to review it. (github.com) FHachez/obsidian-convert-url-to-iframe: Plugin for Obsidian.md to convert a selected URL to an iframe. (github.com) \u2b50 visini/obsidian-icons-plugin: Add icons to your Obsidian notes \u2013 Experimental Obsidian Plugin (github.com) \u2b50 Yeboster/autocomplete-obsidian: Obsidian plugin to provide text autocomplete (github.com) \u2b50 akosbalasko/yarle: Yarle - The ultimate converter of Evernote notes to Markdown (github.com) HEmile/juggl: An interactive, stylable and expandable graph view for Obsidian. Juggl is designed as an advanced 'local' graph view, where you can juggle all your thoughts with ease. (github.com) lynchjames/note-refactor-obsidian: Allows for text selections to be copied (refactored) into new notes and notes to be split into other notes. (github.com) zoni/obsidian-export: Rust library and CLI to export an Obsidian vault to regular Markdown (github.com) \u2b50 Liamballin/ObsidianBookmark: Chrome extension and nodejs server to allow web clipping to Obsidian. (github.com) \u2b50 obsidian-userland/publish: Open source Obsidian Publish alternative (github.com) \u2b50 djsudduth/keep-it-markdown: Convert Google Keep notes dynamically to markdown for Obsidian and Notion using the unofficial Keep API (github.com) \u2b50 pjeby/tag-wrangler: Rename, merge, toggle, and search tags from the Obsidian tag pane (github.com) argenos/hotkeysplus-obsidian: Adds hotkeys to toggle todos, ordered/unordered lists and blockquotes in Obsidian (github.com) pjeby/hot-reload: Automatically reload Obsidian plugins in development when their files are changed (github.com) kepano/obsidian-minimal-settings: Settings plugin to control colors and fonts in Minimal Theme (github.com) kepano/obsidian-hider: Hide Obsidian UI elements such as tooltips, status, titlebar and more (github.com) Vinzent03/find-unlinked-files: Find files, which are nowhere linked, so they are maybe lost in your vault. (github.com) tallguyjenks/Obsidian-For-Business: Using Obsidian.... For Business! (github.com) StefanoCecere/markdown_pandoc_book_template: a template to create pdf/ePub/html/docx books by Markdown via Pandoc (github.com) akaalias/obsidian-extract-pdf-highlights: Extract highlights, underlines and annotations from your PDFs into Obsidian (github.com) ryanjamurphy/workbench-obsidian: A plugin to help you collect working materials. (github.com) mgmeyers/obsidian-style-settings: Dynamically creates a user interface for adjusting theme, plugin, and snippet CSS variables (github.com) pyrochlore/obsidian-tracker: Track everything in daily notes (github.com) deathau/cm-show-whitespace-obsidian: A plugin for [Obsidian](https://obsidian.md) which shows whitespace in the editor. (github.com) DahaWong/obsidian-footlinks: Obsidian plugin that extracts urls from the main text to footer, offering a better reading/editing experience. (github.com) lukeleppan/better-word-count: Counts the words of selected text in the editor. (github.com) renehernandez/obsidian-readwise: Sync Readwise highlights into your obsidian vault (github.com) zephraph/obsidian-tools: An unofficial collection of tools that helps you build plugins for obsidian.md (github.com) avr/obsidian-reading-time (github.com) avirut/obsidian-metatemplates: Take advantage of YAML front-matter in generating notes from templates (for obsidian.md) (github.com) mrjackphil/obsidian-jump-to-link: Quick jump between links using hotkeys (github.com) mrjackphil/obsidian-crosslink-between-notes: This plugin adds a command which allows to add a link to the current note at the bottom of selected notes (github.com) danymat/Obsidian-Markdown-Parser: This repository will give you tools to parse and fetch useful informations of your notes in your Obsidian vault. (github.com) avirut/obsidian-query2table: Represent files returned by a query as a table of their YAML frontmatter (for obsidian.md) (github.com) aviskase/obsidian-link-indexer (github.com) hadynz/obsidian-kindle-plugin: Sync your Kindle notes and highlights directly into your Obsidian vault (github.com) erichalldev/obsidian-smart-random-note: A smart random note plugin for Obsidian (github.com) pjeby/pane-relief: Obsidian plugin for per-pane history, pane movement/navigation hotkeys, and more (github.com) HEmile/obsidian-search-on-internet: Add context menu items in Obsidian to search the internet. (github.com) meld-cp/obsidian-encrypt: Hide secrets in your Obsidian.md vault (github.com) THeK3nger/obsidian-plugin-template: Template for Obsidian.md Plugins (github.com) tgrosinger/recent-files-obsidian: Display a list of most recently opened files (github.com) joethei/obsidian-plantuml: Generate PlantUML Diagrams inside Obsidian.md (github.com) ryanjamurphy/vantage-obsidian: Vantage helps you build complex queries using Obsidian's native search tools. (github.com) trashhalo/obsidian-extract-url: Plugin to extract markdown out of urls (github.com) gavvvr/obsidian-imgur-plugin: Pastes images right to imgur.com (github.com) lukeleppan/obsidian-discordrpc: Update your Discord Status to show your friends what you are working on in Obsidian. With Discord Rich Presence. (github.com) dhruvik7/obsidian-daily-stats: Plugin to view your daily word count across all notes in your Obsidian.md vault. (github.com) ze-kel/DayOne-JSON-to-MD: Converts jsons from Day One app to Markdown. Intended for transferring from DayOne to Obsidian but should work with everything else. (github.com) whateverforever/zettelwarmer: CLI Tool for Zettlr/Obsidian to help you browse random notes. The older the note, the more likely it will be shown. (github.com) liamcain/obsidian-things-logbook: Sync your Things 3 Logbook with Obsidian (github.com) akaalias/obsidian-shuffle: Create custom and randomized writing prompts (github.com) kepano/obsidian-advanced-appearance: Change Obsidian colors, fonts and other cosmetic settings (github.com) phibr0/cycle-through-panes: Cycle through Panes - Obsidian Plugin (github.com) liamcain/obsidian-creases: Mark headings to be collapsed by default (github.com) pjeby/note-folder-autorename: Obsidian plugin to support folder-overview notes by keeping their folder in sync (github.com) rbrcsk/note-tools: A collection of my tools related to notetaking (github.com) jobindj/obsidian-mkdocs: Publish Obsidian Notes with MkDocs (github.com) \u2b50","title":"Plugins &amp; Tools \u2692\ufe0f"},{"location":"Obsidian/Obsidian/#themes","text":"kepano/obsidian-minimal: Minimal theme for Obsidian (github.com) jplattel/obsidian-clipper: A Chrome extension that easily clips selections to Obsidian (github.com) deathau/Notation-for-Obsidian: A theme for Obsidian, inspired by and borrowing elements from Notion (github.com) dxcore35/Suddha-theme: Obsidian theme (github.com)","title":"Themes \ud83c\udfa2"},{"location":"Obsidian/Obsidian/#developers","text":"","title":"Developers"},{"location":"Obsidian/Obsidian/#-tallguyjenks-bryan-jenks-githubcom","text":"Links: PKM Source:","title":"- tallguyjenks (Bryan Jenks) (github.com)"},{"location":"Obsidian/Publishing%20Workflow/","text":"Publishing Workflow \u2691 Instead of paying for the built-in Obsidian Publish Feature this workflow utilizes the fast, simple, and nice looking MkDocs static sit generator to publish an Obsidian Vault. Initial Setup \u2691 Fork the Obsidian-MkDocs Github repo template from jobindj/obsidian-mkdocs Note: if your obsidian vault is already a git repository you may want to utilize git submodules instead of nesting git repo's. Clone the newly forked repo into your local obsidian vault Move any notes you want published into the <repo-name>/docs folder Commit and push changes to trigger the Github Action to publish your notes Example Code: # navigate to obsidian vault's directory cd < path / to / obsidian / vault > # add a git submodule for the mkdocs repo under a folder named '_published' git submodule add git @github . com : jimbrig / obsidian_published . git _published # move some notes into the _published/docs folder Configuration \u2691 Configure the published site's mkdocs.yml configuration file located in the root level of the MkDocs folder. See MkDocs Configuration Documentation for more details https://www.mkdocs.org/#adding-pages Links: Source:","title":"Publishing Workflow"},{"location":"Obsidian/Publishing%20Workflow/#publishing-workflow","text":"Instead of paying for the built-in Obsidian Publish Feature this workflow utilizes the fast, simple, and nice looking MkDocs static sit generator to publish an Obsidian Vault.","title":"Publishing Workflow"},{"location":"Obsidian/Publishing%20Workflow/#initial-setup","text":"Fork the Obsidian-MkDocs Github repo template from jobindj/obsidian-mkdocs Note: if your obsidian vault is already a git repository you may want to utilize git submodules instead of nesting git repo's. Clone the newly forked repo into your local obsidian vault Move any notes you want published into the <repo-name>/docs folder Commit and push changes to trigger the Github Action to publish your notes Example Code: # navigate to obsidian vault's directory cd < path / to / obsidian / vault > # add a git submodule for the mkdocs repo under a folder named '_published' git submodule add git @github . com : jimbrig / obsidian_published . git _published # move some notes into the _published/docs folder","title":"Initial Setup"},{"location":"Obsidian/Publishing%20Workflow/#configuration","text":"Configure the published site's mkdocs.yml configuration file located in the root level of the MkDocs folder. See MkDocs Configuration Documentation for more details https://www.mkdocs.org/#adding-pages Links: Source:","title":"Configuration"},{"location":"Obsidian/Templater%20Plugin%20Notes/","text":"Templater Plugin Notes \u2691 Source: Introduction | Templater (silentvoid13.github.io) Links: Personal Knowledge Management | Obsidian","title":"Templater Plugin Notes"},{"location":"Obsidian/Templater%20Plugin%20Notes/#templater-plugin-notes","text":"Source: Introduction | Templater (silentvoid13.github.io) Links: Personal Knowledge Management | Obsidian","title":"Templater Plugin Notes"},{"location":"PKM/","text":"PKM \u2691 Categories \u2691 Documents \u2691 PKM","title":"PKM"},{"location":"PKM/#pkm","text":"","title":"PKM"},{"location":"PKM/#categories","text":"","title":"Categories"},{"location":"PKM/#documents","text":"PKM","title":"Documents"},{"location":"PKM/PKM/","text":"","title":"PKM"},{"location":"Powwater/","text":"Powwater \u2691 Categories \u2691 Documents \u2691 Database Documentation","title":"Powwater"},{"location":"Powwater/#powwater","text":"","title":"Powwater"},{"location":"Powwater/#categories","text":"","title":"Categories"},{"location":"Powwater/#documents","text":"Database Documentation","title":"Documents"},{"location":"Powwater/Database%20Documentation/","text":"Database Documentation \u2691 Schema \u2691 Connecting \u2691 Running Locally in Docker Container \u2691 pgsync/pg_dump to retrieve SQL from remote hosted production database spin up docker container for postgres locally with correct credentials (password=p, port=5432, dbname = postgres, etc.) create copy of remote database's public schema in the newly created docker container connect to local container database instance from apps, API, etc. Resources: \u2691 Tools \u2691 Database Markup Language (DBML) dbdocs.io dbdiagram.io PostgreSQL psql pgcli GUI's \u2691 pgAdmin4 Valentina Studio DBeaver VSCode RStudio R Packages \u2691 DBI RPostgres RPostgreSQL Pool dbplyr dbplyr dbx connections sqlpetr sqldf and more... Links: PostgreSQL | PostgreSQL Tools | System Design | Databases Source: https://techdocs.powwater.org","title":"Database Documentation"},{"location":"Powwater/Database%20Documentation/#database-documentation","text":"","title":"Database Documentation"},{"location":"Powwater/Database%20Documentation/#schema","text":"","title":"Schema"},{"location":"Powwater/Database%20Documentation/#connecting","text":"","title":"Connecting"},{"location":"Powwater/Database%20Documentation/#running-locally-in-docker-container","text":"pgsync/pg_dump to retrieve SQL from remote hosted production database spin up docker container for postgres locally with correct credentials (password=p, port=5432, dbname = postgres, etc.) create copy of remote database's public schema in the newly created docker container connect to local container database instance from apps, API, etc.","title":"Running Locally in Docker Container"},{"location":"Powwater/Database%20Documentation/#resources","text":"","title":"Resources:"},{"location":"Powwater/Database%20Documentation/#tools","text":"Database Markup Language (DBML) dbdocs.io dbdiagram.io PostgreSQL psql pgcli","title":"Tools"},{"location":"Powwater/Database%20Documentation/#guis","text":"pgAdmin4 Valentina Studio DBeaver VSCode RStudio","title":"GUI's"},{"location":"Powwater/Database%20Documentation/#r-packages","text":"DBI RPostgres RPostgreSQL Pool dbplyr dbplyr dbx connections sqlpetr sqldf and more... Links: PostgreSQL | PostgreSQL Tools | System Design | Databases Source: https://techdocs.powwater.org","title":"R Packages"},{"location":"Productivity/","text":"Productivity \u2691 Categories \u2691 Documents \u2691 Notes on Finishing Projects Productivity Time Management","title":"Productivity"},{"location":"Productivity/#productivity","text":"","title":"Productivity"},{"location":"Productivity/#categories","text":"","title":"Categories"},{"location":"Productivity/#documents","text":"Notes on Finishing Projects Productivity Time Management","title":"Documents"},{"location":"Productivity/Notes%20on%20Finishing%20Projects/","text":"Notes on Finishing Projects \u2691 Set Limitations \u2691 Time : Force time constraints on yourself to avoid wasting useless time that does not reach you closer to your end [[desired outcome]]. Tools : Limit the number of possible tools at your disposal. There's never enough time to try them all out and is a perfect excuse for you to trick yourself into thinking you're being productive when you are just procrastinating . Undo's : This is a big one. Limit yourself to avoid resetting and undoing projects mid-development at all costs . Learn to live with imperfection. Links: Source:","title":"Notes on Finishing Projects"},{"location":"Productivity/Notes%20on%20Finishing%20Projects/#notes-on-finishing-projects","text":"","title":"Notes on Finishing Projects"},{"location":"Productivity/Notes%20on%20Finishing%20Projects/#set-limitations","text":"Time : Force time constraints on yourself to avoid wasting useless time that does not reach you closer to your end [[desired outcome]]. Tools : Limit the number of possible tools at your disposal. There's never enough time to try them all out and is a perfect excuse for you to trick yourself into thinking you're being productive when you are just procrastinating . Undo's : This is a big one. Limit yourself to avoid resetting and undoing projects mid-development at all costs . Learn to live with imperfection. Links: Source:","title":"Set Limitations"},{"location":"Productivity/Productivity/","text":"","title":"Productivity"},{"location":"Productivity/Time%20Management/","text":"Time Management \u2691 Time is merely an illusion - therefore I can procrastinate and it doesn't matter! Time Tracking \u2691 Use tools (internal, external, or simple manual tracking) to decipher what you are spending (and wasting your time) on. Simply gathering data is not enough, you should reflect on the collected times and perform a \"post-mortem\" on what you could be spending your time on. Time Tracking Tools \u2691 TMetric Toggl Clockify etc. Manual Tracking - at [[Tychobra]] we utilize an internal application we created ourselves called T3 for this, but anything works the key is to consistently and thorouly track you time regardless of the medium used to track it. Get Clear On Priorities \u2691 Get clear on your priorities. Batch Tasks and Time Blocks \u2691 Use your calendar, task manager coupled with labels, or a [[time block planner]] to block your time into chunks that contain similar tasks. Learn how to say NO \u2691 Links: Sources:","title":"Time Management"},{"location":"Productivity/Time%20Management/#time-management","text":"Time is merely an illusion - therefore I can procrastinate and it doesn't matter!","title":"Time Management"},{"location":"Productivity/Time%20Management/#time-tracking","text":"Use tools (internal, external, or simple manual tracking) to decipher what you are spending (and wasting your time) on. Simply gathering data is not enough, you should reflect on the collected times and perform a \"post-mortem\" on what you could be spending your time on.","title":"Time Tracking"},{"location":"Productivity/Time%20Management/#time-tracking-tools","text":"TMetric Toggl Clockify etc. Manual Tracking - at [[Tychobra]] we utilize an internal application we created ourselves called T3 for this, but anything works the key is to consistently and thorouly track you time regardless of the medium used to track it.","title":"Time Tracking Tools"},{"location":"Productivity/Time%20Management/#get-clear-on-priorities","text":"Get clear on your priorities.","title":"Get Clear On Priorities"},{"location":"Productivity/Time%20Management/#batch-tasks-and-time-blocks","text":"Use your calendar, task manager coupled with labels, or a [[time block planner]] to block your time into chunks that contain similar tasks.","title":"Batch Tasks and Time Blocks"},{"location":"Productivity/Time%20Management/#learn-how-to-say-no","text":"Links: Sources:","title":"Learn how to say NO"},{"location":"Project%20Management/","text":"Project Management \u2691 Categories \u2691 Documents \u2691 Ludicrously Complex Projects in Software Development Project Management Pipeline Project Management Taming a Chaotic Project","title":"Project Management"},{"location":"Project%20Management/#project-management","text":"","title":"Project Management"},{"location":"Project%20Management/#categories","text":"","title":"Categories"},{"location":"Project%20Management/#documents","text":"Ludicrously Complex Projects in Software Development Project Management Pipeline Project Management Taming a Chaotic Project","title":"Documents"},{"location":"Project%20Management/Ludicrously%20Complex%20Projects%20in%20Software%20Development/","text":"Ludicrously Complex Projects \u2691 Clear Ownership \u2691 Split out the project into separate buckets of work duties with leaders for each bucket: Administrative Technical Strategy and Planning Track Progress \u2691 Communicate and track project's progression through sharing a project timeline and use it as a single source of truth . Keep it updated to reflect reality. Make Effective Decisions \u2691 Brainstorm and be thoughtful by taking into account any and all long and short term implications. Optimize for efficiency Organize and Communicate Manage Dependencies \u2691 Anticipate bottlenecks \u2013 Make a table or diagram that maps out who your team relies on, and who relies on your team. Keep tabs on it \u2013 Assign one owner from each side who looks after each dependency. Make sure the dependency owners understand and communicate the impact of changes to all upstream and downstream teams. Links: Source: The Top 8 Tips for Managing Complex Software Projects (atlassian.com)","title":"Ludicrously Complex Projects"},{"location":"Project%20Management/Ludicrously%20Complex%20Projects%20in%20Software%20Development/#ludicrously-complex-projects","text":"","title":"Ludicrously Complex Projects"},{"location":"Project%20Management/Ludicrously%20Complex%20Projects%20in%20Software%20Development/#clear-ownership","text":"Split out the project into separate buckets of work duties with leaders for each bucket: Administrative Technical Strategy and Planning","title":"Clear Ownership"},{"location":"Project%20Management/Ludicrously%20Complex%20Projects%20in%20Software%20Development/#track-progress","text":"Communicate and track project's progression through sharing a project timeline and use it as a single source of truth . Keep it updated to reflect reality.","title":"Track Progress"},{"location":"Project%20Management/Ludicrously%20Complex%20Projects%20in%20Software%20Development/#make-effective-decisions","text":"Brainstorm and be thoughtful by taking into account any and all long and short term implications. Optimize for efficiency Organize and Communicate","title":"Make Effective Decisions"},{"location":"Project%20Management/Ludicrously%20Complex%20Projects%20in%20Software%20Development/#manage-dependencies","text":"Anticipate bottlenecks \u2013 Make a table or diagram that maps out who your team relies on, and who relies on your team. Keep tabs on it \u2013 Assign one owner from each side who looks after each dependency. Make sure the dependency owners understand and communicate the impact of changes to all upstream and downstream teams. Links: Source: The Top 8 Tips for Managing Complex Software Projects (atlassian.com)","title":"Manage Dependencies"},{"location":"Project%20Management/Project%20Management%20Pipeline/","text":"Project Management Pipeline \u2691 Having a good plan is the most important strategy for getting a project done . Step 1: Collection \u2691 Collect all work related items that are part of the project, including: Physical Handwritten Notes and Brainstorms Meeting/Call Agendas and Notes Emails/Correspondence with Client Open Tasks in Task Manager Open Github Issues Slack Messages Data received from client etc. After collecting, prioritize the pieces as tasks that are part of the project - if get stuck prioritizing ask yourself: Which one has the most immediate hard deadline? Which task will make the most positive effect if it is finished ASAP? Are any of these tasks dependent on another one? Am I dependent on another person to complete something else before starting? Is there a task that I must get off my plate to clear my mind and move forward? Step 2: Develop a Process \u2691 Next, outline and list out all the necessary steps to complete each task, asking these three questions: What are all the tasks and micro-tasks that must be done to complete this? Who needs to weigh in on, contribute to, perform a quality assurance check on, or sign off on the work? Is this the most efficient way to get from A to Z on this particular project? Step 3: Get Organized \u2691 Committing to being organized and finding a structure that fits the project's needs gives a fresh insight into what needs to get done, re-prioritized, or reorganized. Set a time each week, such as every Friday afternoon or Monday morning, to review work items. This not only keeps your mind fresh but also helps you see all the things that are part of a bigger project and vision. Change happens, so you\u2019re probably updating a lot of tasks in the course of a week. This review process will help you stay on top of your moving work Step 4: Just Do It \u2691 Now that you\u2019ve completed the first four steps, it\u2019s time to take action. Pull the trigger; press publish; deliver the final product. What do you do right now ? Based on David Allen\u2019s GTD methodology, consider these four things: Context. What can you do right now? Time available. What do you have time to do right now? Energy available. What are you able to accomplish right now? Priority. After answering the first three questions, start working on the highest priority item. Links: Project Management | PKM | Productivity Source: Five Steps to Getting a Project Done | LiquidPlanner","title":"Project Management Pipeline"},{"location":"Project%20Management/Project%20Management%20Pipeline/#project-management-pipeline","text":"Having a good plan is the most important strategy for getting a project done .","title":"Project Management Pipeline"},{"location":"Project%20Management/Project%20Management%20Pipeline/#step-1-collection","text":"Collect all work related items that are part of the project, including: Physical Handwritten Notes and Brainstorms Meeting/Call Agendas and Notes Emails/Correspondence with Client Open Tasks in Task Manager Open Github Issues Slack Messages Data received from client etc. After collecting, prioritize the pieces as tasks that are part of the project - if get stuck prioritizing ask yourself: Which one has the most immediate hard deadline? Which task will make the most positive effect if it is finished ASAP? Are any of these tasks dependent on another one? Am I dependent on another person to complete something else before starting? Is there a task that I must get off my plate to clear my mind and move forward?","title":"Step 1: Collection"},{"location":"Project%20Management/Project%20Management%20Pipeline/#step-2-develop-a-process","text":"Next, outline and list out all the necessary steps to complete each task, asking these three questions: What are all the tasks and micro-tasks that must be done to complete this? Who needs to weigh in on, contribute to, perform a quality assurance check on, or sign off on the work? Is this the most efficient way to get from A to Z on this particular project?","title":"Step 2: Develop a Process"},{"location":"Project%20Management/Project%20Management%20Pipeline/#step-3-get-organized","text":"Committing to being organized and finding a structure that fits the project's needs gives a fresh insight into what needs to get done, re-prioritized, or reorganized. Set a time each week, such as every Friday afternoon or Monday morning, to review work items. This not only keeps your mind fresh but also helps you see all the things that are part of a bigger project and vision. Change happens, so you\u2019re probably updating a lot of tasks in the course of a week. This review process will help you stay on top of your moving work","title":"Step 3: Get Organized"},{"location":"Project%20Management/Project%20Management%20Pipeline/#step-4-just-do-it","text":"Now that you\u2019ve completed the first four steps, it\u2019s time to take action. Pull the trigger; press publish; deliver the final product. What do you do right now ? Based on David Allen\u2019s GTD methodology, consider these four things: Context. What can you do right now? Time available. What do you have time to do right now? Energy available. What are you able to accomplish right now? Priority. After answering the first three questions, start working on the highest priority item. Links: Project Management | PKM | Productivity Source: Five Steps to Getting a Project Done | LiquidPlanner","title":"Step 4: Just Do It"},{"location":"Project%20Management/Project%20Management/","text":"","title":"Project Management"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/","text":"Taming a Chaotic Project \u2691 It happens to projects big and small: one day, you wake up to a mess in your workspace. Sometimes it\u2019s because of a colleague who can\u2019t load-balance, or a budget decision that left you with fewer resources. And in a certain sense, it doesn\u2019t matter why the project got off track \u2013 but it definitely matters how you untangle it. 1. Recognize Warning Signs and Act Fast \u2691 Project snags don\u2019t magically resolve themselves. The earlier you face these problems in the project\u2019s lifecycle, the more options you have to resolve them. Throughout the life of the project, get regular status updates. Encourage your team to forecast their remaining work, factoring in any project plan changes so that potential slippages are identified as soon as possible. Use collaborative project management software to keep everyone clear on where the project\u2014and their responsibilities\u2014stands at all times. 2. Find Out What\u2019s Gone Wrong \u2691 Don\u2019t rush into a fix without first identifying why. Without taking the time to understand the root cause of the problem, any proposed solution will be a shot in the dark. Start by talking to your team. Get their view on what\u2019s hot and what\u2019s not, and elicit their ideas for a solution. Your team\u2019s engagement and commitment to any new plan going forward will be the critical contributor to its success. And a heads-up: When the going gets tough, these information-gathering exercises can deteriorate into finger-pointing sessions so don\u2019t let that happen. Instead, press for accurate information on how to reprioritize and restructure tasks. Team members might be reticent when it comes to delivering bad news and may choose to do so in bits and pieces. Be clear that you need all of the bad news right now, otherwise you\u2019ll be re-structuring the plan every week as the news rolls in incrementally. Be sure to share what you find so that subsequent chaotic projects don\u2019t suffer the same issues. 3. Revisit the Original Plan \u2691 Don\u2019t forget why you\u2019re doing the project in the first place. Review the original business case and check in from time to time to verify that it\u2019s still valid. It\u2019s easy to focus on doing whatever\u2019s needed to hit that next deadline, but be aware of what that mono-focusing does to the overall project (and the consequences it has on any projects that follow). You might be able to hit that deadline by throwing additional resources at it, but if this plan trashes the overall budget and subsequent delivery schedules, then that\u2019s not the way to go. You want to play the long game here. Don\u2019t be afraid to consider the effects of killing the chaotic project and walking away. In reality, this is rarely an option as it can be devastating in terms of customer relations and company reputation, but you should always weigh it up. A dogged determination to see a project through is admirable, but if it makes you unable to deliver on other commitments, it can be catastrophic for your organization. Review the financial and your resources against the overall business plan. If the chaotic project isn\u2019t going to deliver what it set out to do, then look at how you can really get there at this point in the project. 4. Review Your Resources \u2691 When scheduling issues occur, it\u2019s easy (and common practice) to simply throw extra bodies at the chaotic project and grow the team. But this rarely yields a great result. Instead, when your project hits a speed bump, consider the following options to get your project back on track: Make sure the right people are assigned to the right tasks. You might have to do some re-delegating and re-allocating. See if you can spread portions of a meaty task among a larger team so that more tasks can be worked on in parallel rather than serially. Identify team members that you can shift from non-critical work to critical path activities. Focus on competency, not availability. Add resources with the right experience and skills so they can hit the ground running. 5. Look for new Solutions \u2691 Look at the project scope, and ask yourself: Is there anything planned that doesn\u2019t need to be here? Are there any activities or deliverables being added or gold-plated that could be dropped or scaled back without falling short of the original requirement? (A zero-tolerance approach to scope creep can often save the day). A note of caution: When re-planning or re-prioritizing, be wary of sacrificing quality for expediency. Time can often be saved by cutting test and validation activities, but this is a false economy as you\u2019ll end up doing the work later, post-delivery. And then it will be even costlier. Review the planned deliverables and activities and strip out the non-essentials. 6. Talk to your Client \u2691 None of us likes to share our woes with our clients, but if the relationship is sound, your client will work with you to find a solution. Get a dialogue going and develop a workable plan. Start by changing parts of the plan with your client, like the delivery schedule, or agreed-upon delivery phases, rather than overhauling the one big deadline. Who knows\u2014you might be able to extend the overall schedule but still get your client the key deliverables when they need them. 7. Review Work Processes \u2691 We\u2019re creatures of habit. This means we might be prone to keep working on a chaotic project in the same manner we always have because it\u2019s what we\u2019re used to. But when your project\u2019s struggling, it\u2019s time to find a new, creative approach. Start by talking to your team. Let them help you identify inefficiencies and bottlenecks, and together you can come up with smarter ways to get the work done. Another trick: Review stages are often a sticking point, so if documentation keeps getting bounced from one reviewer to another, schedule a meeting-based review. Get all the players in the room so issues can be ironed out swiftly in one session. 8. Check your Dependencies \u2691 It\u2019s easy to leave some project activities loosely-defined when you\u2019re in the first stages of planning. For example, to be on the safe side you might schedule dependencies serially (e.g., development only starts when a design is fully complete, etc.). But when you\u2019re further down the project road, it\u2019s time to see if some of these tasks and activities can be re-scheduled in a more parallel manner. A word of caution: Be careful of over-doing this fast-tracking. There\u2019s always a chance you\u2019ll have to rework things down the road, but it\u2019s always worth looking at. Also, the client can be the bottleneck if you\u2019re waiting on stakeholder participation in certain events, so communicate what you need and when you need it. Be clear on what impact that any delays will have on delivery. Don\u2019t take all the pain yourself. Identify anything that\u2019s obstructing progress on the project and work with the team to smooth out the bumps. 9. Time for Overtime? \u2691 This is often the first thought when schedules start to flounder. Overtime should be a last resort\u2014turned to when there are few to no other options left as a deadline looms. The strange truth about overtime is that it doesn\u2019t always yield higher levels of productivity. Instead, team members who work long hours over the course of time tend to pace themselves (either consciously or sub-consciously) accordingly. The result is that they end up applying themselves less during the non-overtime hours to save themselves for the overtime portion of their day. To combat this, try laying out a rough draft of a schedule that includes a small amount of overtime, and see if this delivers a schedule that works for the project. If it doesn\u2019t work on paper, it won\u2019t work in practice. 10. Keep Managing \u2691 It\u2019s much easier to manage a team when everything\u2019s going well. But a lot of best practices can go out the window when it\u2019s an all-hands-to-the-pumps situation to keep things afloat. As a project manager, keep talking to the team and make sure everyone is clear on what\u2019s expected of them. Roles and responsibilities can become woolly when a new plan is quickly put in place. Morale can also suffer when things go awry as team members feel they haven\u2019t delivered\u2014or, even if they have, people get discouraged when they\u2019re part of a potentially unsuccessful venture. So, don\u2019t forget to acknowledge what is going well. Check your own behavior. Leading by example, is the most crucial when times are tough. Links: Source: Ten Tips For Taming That Chaotic Project | LiquidPlanner","title":"Taming a Chaotic Project"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#taming-a-chaotic-project","text":"It happens to projects big and small: one day, you wake up to a mess in your workspace. Sometimes it\u2019s because of a colleague who can\u2019t load-balance, or a budget decision that left you with fewer resources. And in a certain sense, it doesn\u2019t matter why the project got off track \u2013 but it definitely matters how you untangle it.","title":"Taming a Chaotic Project"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#1-recognize-warning-signs-and-act-fast","text":"Project snags don\u2019t magically resolve themselves. The earlier you face these problems in the project\u2019s lifecycle, the more options you have to resolve them. Throughout the life of the project, get regular status updates. Encourage your team to forecast their remaining work, factoring in any project plan changes so that potential slippages are identified as soon as possible. Use collaborative project management software to keep everyone clear on where the project\u2014and their responsibilities\u2014stands at all times.","title":"1. Recognize Warning Signs and Act Fast"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#2-find-out-whats-gone-wrong","text":"Don\u2019t rush into a fix without first identifying why. Without taking the time to understand the root cause of the problem, any proposed solution will be a shot in the dark. Start by talking to your team. Get their view on what\u2019s hot and what\u2019s not, and elicit their ideas for a solution. Your team\u2019s engagement and commitment to any new plan going forward will be the critical contributor to its success. And a heads-up: When the going gets tough, these information-gathering exercises can deteriorate into finger-pointing sessions so don\u2019t let that happen. Instead, press for accurate information on how to reprioritize and restructure tasks. Team members might be reticent when it comes to delivering bad news and may choose to do so in bits and pieces. Be clear that you need all of the bad news right now, otherwise you\u2019ll be re-structuring the plan every week as the news rolls in incrementally. Be sure to share what you find so that subsequent chaotic projects don\u2019t suffer the same issues.","title":"2. Find Out What\u2019s Gone Wrong"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#3-revisit-the-original-plan","text":"Don\u2019t forget why you\u2019re doing the project in the first place. Review the original business case and check in from time to time to verify that it\u2019s still valid. It\u2019s easy to focus on doing whatever\u2019s needed to hit that next deadline, but be aware of what that mono-focusing does to the overall project (and the consequences it has on any projects that follow). You might be able to hit that deadline by throwing additional resources at it, but if this plan trashes the overall budget and subsequent delivery schedules, then that\u2019s not the way to go. You want to play the long game here. Don\u2019t be afraid to consider the effects of killing the chaotic project and walking away. In reality, this is rarely an option as it can be devastating in terms of customer relations and company reputation, but you should always weigh it up. A dogged determination to see a project through is admirable, but if it makes you unable to deliver on other commitments, it can be catastrophic for your organization. Review the financial and your resources against the overall business plan. If the chaotic project isn\u2019t going to deliver what it set out to do, then look at how you can really get there at this point in the project.","title":"3. Revisit the Original Plan"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#4-review-your-resources","text":"When scheduling issues occur, it\u2019s easy (and common practice) to simply throw extra bodies at the chaotic project and grow the team. But this rarely yields a great result. Instead, when your project hits a speed bump, consider the following options to get your project back on track: Make sure the right people are assigned to the right tasks. You might have to do some re-delegating and re-allocating. See if you can spread portions of a meaty task among a larger team so that more tasks can be worked on in parallel rather than serially. Identify team members that you can shift from non-critical work to critical path activities. Focus on competency, not availability. Add resources with the right experience and skills so they can hit the ground running.","title":"4. Review Your Resources"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#5-look-for-new-solutions","text":"Look at the project scope, and ask yourself: Is there anything planned that doesn\u2019t need to be here? Are there any activities or deliverables being added or gold-plated that could be dropped or scaled back without falling short of the original requirement? (A zero-tolerance approach to scope creep can often save the day). A note of caution: When re-planning or re-prioritizing, be wary of sacrificing quality for expediency. Time can often be saved by cutting test and validation activities, but this is a false economy as you\u2019ll end up doing the work later, post-delivery. And then it will be even costlier. Review the planned deliverables and activities and strip out the non-essentials.","title":"5. Look for new Solutions"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#6-talk-to-your-client","text":"None of us likes to share our woes with our clients, but if the relationship is sound, your client will work with you to find a solution. Get a dialogue going and develop a workable plan. Start by changing parts of the plan with your client, like the delivery schedule, or agreed-upon delivery phases, rather than overhauling the one big deadline. Who knows\u2014you might be able to extend the overall schedule but still get your client the key deliverables when they need them.","title":"6. Talk to your Client"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#7-review-work-processes","text":"We\u2019re creatures of habit. This means we might be prone to keep working on a chaotic project in the same manner we always have because it\u2019s what we\u2019re used to. But when your project\u2019s struggling, it\u2019s time to find a new, creative approach. Start by talking to your team. Let them help you identify inefficiencies and bottlenecks, and together you can come up with smarter ways to get the work done. Another trick: Review stages are often a sticking point, so if documentation keeps getting bounced from one reviewer to another, schedule a meeting-based review. Get all the players in the room so issues can be ironed out swiftly in one session.","title":"7. Review Work Processes"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#8-check-your-dependencies","text":"It\u2019s easy to leave some project activities loosely-defined when you\u2019re in the first stages of planning. For example, to be on the safe side you might schedule dependencies serially (e.g., development only starts when a design is fully complete, etc.). But when you\u2019re further down the project road, it\u2019s time to see if some of these tasks and activities can be re-scheduled in a more parallel manner. A word of caution: Be careful of over-doing this fast-tracking. There\u2019s always a chance you\u2019ll have to rework things down the road, but it\u2019s always worth looking at. Also, the client can be the bottleneck if you\u2019re waiting on stakeholder participation in certain events, so communicate what you need and when you need it. Be clear on what impact that any delays will have on delivery. Don\u2019t take all the pain yourself. Identify anything that\u2019s obstructing progress on the project and work with the team to smooth out the bumps.","title":"8. Check your Dependencies"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#9-time-for-overtime","text":"This is often the first thought when schedules start to flounder. Overtime should be a last resort\u2014turned to when there are few to no other options left as a deadline looms. The strange truth about overtime is that it doesn\u2019t always yield higher levels of productivity. Instead, team members who work long hours over the course of time tend to pace themselves (either consciously or sub-consciously) accordingly. The result is that they end up applying themselves less during the non-overtime hours to save themselves for the overtime portion of their day. To combat this, try laying out a rough draft of a schedule that includes a small amount of overtime, and see if this delivers a schedule that works for the project. If it doesn\u2019t work on paper, it won\u2019t work in practice.","title":"9. Time for Overtime?"},{"location":"Project%20Management/Taming%20a%20Chaotic%20Project/#10-keep-managing","text":"It\u2019s much easier to manage a team when everything\u2019s going well. But a lot of best practices can go out the window when it\u2019s an all-hands-to-the-pumps situation to keep things afloat. As a project manager, keep talking to the team and make sure everyone is clear on what\u2019s expected of them. Roles and responsibilities can become woolly when a new plan is quickly put in place. Morale can also suffer when things go awry as team members feel they haven\u2019t delivered\u2014or, even if they have, people get discouraged when they\u2019re part of a potentially unsuccessful venture. So, don\u2019t forget to acknowledge what is going well. Check your own behavior. Leading by example, is the most crucial when times are tough. Links: Source: Ten Tips For Taming That Chaotic Project | LiquidPlanner","title":"10. Keep Managing"},{"location":"Python/","text":"Python \u2691 Categories \u2691 Documents \u2691 Python","title":"Python"},{"location":"Python/#python","text":"","title":"Python"},{"location":"Python/#categories","text":"","title":"Categories"},{"location":"Python/#documents","text":"Python","title":"Documents"},{"location":"Python/Python/","text":"Python \u2691 Installation \u2691 Links: Source:","title":"Python"},{"location":"Python/Python/#python","text":"","title":"Python"},{"location":"Python/Python/#installation","text":"Links: Source:","title":"Installation"},{"location":"R/","text":"R \u2691 Categories \u2691 Documents \u2691 Base Package Hidden Gems in R Data Validation Packages in R Databases with R Resources EDA Packages in R Plumber Logging Plumber REST APIs in R Plumber Resources R Books R Development R Miscellaneous Notes RStudio Configuration Notes Shiny Apps as Packages in R Tools Package Hidden Gems in R Useful Packages in R List Utils Package Hidden Gems in R","title":"R"},{"location":"R/#r","text":"","title":"R"},{"location":"R/#categories","text":"","title":"Categories"},{"location":"R/#documents","text":"Base Package Hidden Gems in R Data Validation Packages in R Databases with R Resources EDA Packages in R Plumber Logging Plumber REST APIs in R Plumber Resources R Books R Development R Miscellaneous Notes RStudio Configuration Notes Shiny Apps as Packages in R Tools Package Hidden Gems in R Useful Packages in R List Utils Package Hidden Gems in R","title":"Documents"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/","text":"Base Package Hidden Gems in R \ud83d\udc8e \u2691 autoload Family \u2691 Related: delayedAssign and library These are amazingly underutilized functions from base R. Description: \u2691 autoload creates a promise-to-evaluate autoloader and stores it with name name in the .AutoloadEnv environment. When R attempts to evaluate name , autoloader is run, the package is loaded and name is re-evaluated in the new package's environment ! The result is that R behaves as if package was loaded but it does not occupy memory. .Autoloaded contains the names of the packages for which autoloading has been promised. Usage \u2691 autoload : autoload(name, package, reset = FALSE, ...) autoloader : autoloader(name, package, ...) .AutoloadEnv : see below .Autoloaded : see below Arguments \u2691 name - string giving the name of an object package - string giving the name of a package containing the object reset - logical: for internal use by autoloader ... - other arguments to library Examples \u2691 require ( stats ) autoload ( \"interpSpline\" , \"splines\" ) search () ls ( \"Autoloads\" ) . Autoloaded x <- sort ( stats :: rnorm ( 12 )) y <- x ^ 2 is <- interpSpline ( x , y ) search () ## now has splines detach ( \"package:splines\" ) search () is2 <- interpSpline ( x , y + x ) search () ## and again detach ( \"package:splines\" ) My Take \u2691 I find autoload extremely useful for incorporating functions from packages in my [[.Rprofile| Rprofile ]]. For example, I like to utilized magrittr's %>% pipe and usually do not want to have to library it in for ad-hoc analysis on the fly; therefore by including autoload(\"%>%\", \"magrittr\") in my .Rprofile , I have complete access to %>% without magrittr cluttering up my search path or namespaced environment (especially useful on Windows). With that snippet included in my .Rprofile I can use %>% later on in the same R session and magrittr will not get attached until I use it. Other ideas for autoload useful functions would be: dplyr 's suite: mutate , filter , select , etc. lubdridate 's ymd and other date utility functions fs 's file navigation functions. ... Reduce \u2691 Reduce function (note the capital \u201cR\u201d). Reduce takes a list or vector as input, and reduces it down to a single element. It works by applying a function to the first two elements of the vector or list, and then applying the same function to that result with the third element. This new result gets passed with the fourth element into the function and so on until a single object remains. If the input is a vector, the result will be a single number or character. On the other hand, inputting a list can have interesting results. A list of data frames can be reduced down to a single data frame, a list of vectors can be collapsed into a matrix, and so on. A simple, though not entirely useful, example of how this works is like so: test <- 1 : 10 result <- Reduce ( sum , test ) Here, result will equal 55 , which happens to be the sum of the vector test i.e. the sum of the integers 1 through 10. Reduce solves for this by first applying the sum function to 1 and 2 (the first two elements in test). This equals 3, which then gets summed with the next element in the vector, 3. This total of 6 gets added to 4, which equals 10, and so on. The process can be seen below. $1 + 2 = 3$ $3 + 3 = 6$ $6 + 4 = 10$ $10 + 5 = 15$ $15 + 6 = 21$ $21 + 7 = 28$ $28 + 8 = 36$ $36 + 9 = 45$ $45 + 10 = 55$ Now, how about something a little more useful? What if you had a list of vectors and you wanted to combine them into a matrix? test <- list ( 1 : 3 , 4 : 6 , 7 : 9 , 10 : 12 , 13 : 15 , 16 : 18 ) matrix_result <- Reduce ( rbind , test ) In this case, we have a list of six three-element vectors. Reduce applies rbind to the first two vectors, 1:3 and 4:6 initially. This creates a 2 x 3 matrix, where the first row is 1:3, and the second row is 4:6. 1 2 3 4 5 6 Then, the above result is combined (via rbind ) to the next vector in the list, 7:9. 1 2 3 4 5 6 7 8 9 This process continues, as you can see below: 1 2 3 4 5 6 7 8 9 10 11 12 Next: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Finally: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Thus, the final result is a single object \u2014 but in this case, is a 6 x 3 matrix because rbind collapsed all of the vectors of the list, test, into a single matrix. Similarly, you could run this example using cbind instead of rbind and that would collapse the vectors column-wise, rather than row-wise. Another example where Reduce comes in handy might be if you want to combine a collection of data frames into a single one. state_data <- list ( FL = data.frame ( state = c ( \"FL\" , \"FL\" , \"FL\" ), city = c ( \"Miami\" , \"Jacksonville\" , \"Saint Augustine\" )), NY = data.frame ( state = c ( \"NY\" , \"NY\" , \"NY\" ), city = c ( \"NYC\" , \"Buffalo\" , \"Rochester\" )), MD = data.frame ( state = c ( \"MD\" , \"MD\" , \"MD\" ), city = c ( \"Baltimore\" , \"Annapolis\" , \"Ocean City\" )) ) combined <- data.frame ( Reduce ( rbind , state_data )) Filter \u2691 The Filter function does basically what it sounds like \u2014 it applies a filter to a vector, list, or data frame (which is actually a type of list). It takes two main inputs, a function that applies the filter, and the object for which the filter applies. Here\u2019s a simple example: test <- 1 : 10 less_than_5 <- Filter ( function ( x ) x < 5 , test ) This, once again, creates a vector of the first 10 positive integers. The Filter function applies function(x) x < 5 to each element, x , in the vector, test . In other words, it checks each element, x , for the Boolean expression, x < 5 . If an element is not less than 5, it gets filtered out. So you might be thinking\u2026can\u2019t this be done like this? less_than_5 <- test[test < 5] \u2026and the answer is\u2026yes. It can be done that way. Filter is more useful as a function in cases involving data frames or lists. Suppose, for instance, you want to remove all constant columns from a data frame. This is something that may be done when preprocessing data prior to modeling, as a constant attribute isn\u2019t particular useful. This is can be done in one line using Filter df <- data . frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( 1 , 1 , 1 ), d = c ( 3 , 4 , 5 )) without_constants <- Filter ( function ( x ) length ( unique ( x )) > 1 , df ) Alternatively, using dplyr\u2019s n_distinct function, which counts the number of distinct elements in a vector, you could do this: library ( dplyr ) df <- data . frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( 1 , 1 , 1 ), d = c ( 3 , 4 , 5 )) without_constants <- Filter ( function ( x ) n_distinct ( x ) > 1 , df ) In the example, we create a data frame with four columns \u2014 two of them are constant. Filter tests whether there is more than one unique value in each column. If there is only one unique value, then we know the column is constant, and it gets filtered out. Each element x is a vector, or column, in the data frame. If you wanted to just drop all columns that are all NAs, you could make a minor tweak like this: df <- data.frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( 1 , 1 , 1 ), d = c ( NA , NA , NA )) without_nas <- Filter ( function ( x ) ! all ( is.na ( x )), df ) Filter can also be used on a regular list as well. Suppose you have a list of vectors, where some of the vectors are characters, while others are numeric. If want to filter out all of the non-numeric vectors, you could call Filter : sample_list <- list ( a = c ( 1 , 2 , 3 ), b = c ( \"is\" , \"a\" , \"character\" ), c = c ( 4 , 5 , 6 ), d = c ( \"is\" , \"another\" , \"character\" )) only_numeric <- Filter ( function ( x ) is.numeric ( x ), sample_list ) rapply \u2691 The rapply function is part of the apply family of functions in R. It has a few different uses, but one of my favorite applications for it is to apply a function to columns of a data frame that belong to a specific class, or have a particular data type. Let\u2019s say you want to get the sum of all of the numeric columns. df <- data.frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( \"r\" , \"is\" , \"awesome\" ), d = c ( 3 , 4 , 5 ), e = c ( \"some\" , \"other\" , \"character\" )) summed_columns <- rapply ( df , sum , class = \"numeric\" ) Similar to sapply or lapply , rapply takes a list / vector / data frame as input, along with a function to be applied. However, it can also take a \u201cclass\u201d parameter, which allows us to specify what class of object we want our function to be used for. rapply can also be used to recursively apply functions to nested lists (see examples from its documentation here ). rep \u2691 The last function I want to mention for this post is the rep function. This can be used to repeat a value as many times as you want. So if you want to create a vector of 1000 5\u2019s, it could be done like this: rep(5, 1000) Here\u2019s a couple other examples: rep(\"a\", 500) rep(\"repeat this\", 100) If you pass a vector with more than one element to rep , the entire vector gets repeated the number of times you specify. rep(c(1,2,3), 100) The above code will create a vector with 300 elements \u2014 the number of elements in c(1,2,3) times 100, repeating 1, 2, 3 over and over. Links: Source:","title":"Base Package Hidden Gems in R \ud83d\udc8e"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#base-package-hidden-gems-in-r","text":"","title":"Base Package Hidden Gems in R \ud83d\udc8e"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#autoload-family","text":"Related: delayedAssign and library These are amazingly underutilized functions from base R.","title":"autoload Family"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#description","text":"autoload creates a promise-to-evaluate autoloader and stores it with name name in the .AutoloadEnv environment. When R attempts to evaluate name , autoloader is run, the package is loaded and name is re-evaluated in the new package's environment ! The result is that R behaves as if package was loaded but it does not occupy memory. .Autoloaded contains the names of the packages for which autoloading has been promised.","title":"Description:"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#usage","text":"autoload : autoload(name, package, reset = FALSE, ...) autoloader : autoloader(name, package, ...) .AutoloadEnv : see below .Autoloaded : see below","title":"Usage"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#arguments","text":"name - string giving the name of an object package - string giving the name of a package containing the object reset - logical: for internal use by autoloader ... - other arguments to library","title":"Arguments"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#examples","text":"require ( stats ) autoload ( \"interpSpline\" , \"splines\" ) search () ls ( \"Autoloads\" ) . Autoloaded x <- sort ( stats :: rnorm ( 12 )) y <- x ^ 2 is <- interpSpline ( x , y ) search () ## now has splines detach ( \"package:splines\" ) search () is2 <- interpSpline ( x , y + x ) search () ## and again detach ( \"package:splines\" )","title":"Examples"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#my-take","text":"I find autoload extremely useful for incorporating functions from packages in my [[.Rprofile| Rprofile ]]. For example, I like to utilized magrittr's %>% pipe and usually do not want to have to library it in for ad-hoc analysis on the fly; therefore by including autoload(\"%>%\", \"magrittr\") in my .Rprofile , I have complete access to %>% without magrittr cluttering up my search path or namespaced environment (especially useful on Windows). With that snippet included in my .Rprofile I can use %>% later on in the same R session and magrittr will not get attached until I use it. Other ideas for autoload useful functions would be: dplyr 's suite: mutate , filter , select , etc. lubdridate 's ymd and other date utility functions fs 's file navigation functions. ...","title":"My Take"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#reduce","text":"Reduce function (note the capital \u201cR\u201d). Reduce takes a list or vector as input, and reduces it down to a single element. It works by applying a function to the first two elements of the vector or list, and then applying the same function to that result with the third element. This new result gets passed with the fourth element into the function and so on until a single object remains. If the input is a vector, the result will be a single number or character. On the other hand, inputting a list can have interesting results. A list of data frames can be reduced down to a single data frame, a list of vectors can be collapsed into a matrix, and so on. A simple, though not entirely useful, example of how this works is like so: test <- 1 : 10 result <- Reduce ( sum , test ) Here, result will equal 55 , which happens to be the sum of the vector test i.e. the sum of the integers 1 through 10. Reduce solves for this by first applying the sum function to 1 and 2 (the first two elements in test). This equals 3, which then gets summed with the next element in the vector, 3. This total of 6 gets added to 4, which equals 10, and so on. The process can be seen below. $1 + 2 = 3$ $3 + 3 = 6$ $6 + 4 = 10$ $10 + 5 = 15$ $15 + 6 = 21$ $21 + 7 = 28$ $28 + 8 = 36$ $36 + 9 = 45$ $45 + 10 = 55$ Now, how about something a little more useful? What if you had a list of vectors and you wanted to combine them into a matrix? test <- list ( 1 : 3 , 4 : 6 , 7 : 9 , 10 : 12 , 13 : 15 , 16 : 18 ) matrix_result <- Reduce ( rbind , test ) In this case, we have a list of six three-element vectors. Reduce applies rbind to the first two vectors, 1:3 and 4:6 initially. This creates a 2 x 3 matrix, where the first row is 1:3, and the second row is 4:6. 1 2 3 4 5 6 Then, the above result is combined (via rbind ) to the next vector in the list, 7:9. 1 2 3 4 5 6 7 8 9 This process continues, as you can see below: 1 2 3 4 5 6 7 8 9 10 11 12 Next: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Finally: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Thus, the final result is a single object \u2014 but in this case, is a 6 x 3 matrix because rbind collapsed all of the vectors of the list, test, into a single matrix. Similarly, you could run this example using cbind instead of rbind and that would collapse the vectors column-wise, rather than row-wise. Another example where Reduce comes in handy might be if you want to combine a collection of data frames into a single one. state_data <- list ( FL = data.frame ( state = c ( \"FL\" , \"FL\" , \"FL\" ), city = c ( \"Miami\" , \"Jacksonville\" , \"Saint Augustine\" )), NY = data.frame ( state = c ( \"NY\" , \"NY\" , \"NY\" ), city = c ( \"NYC\" , \"Buffalo\" , \"Rochester\" )), MD = data.frame ( state = c ( \"MD\" , \"MD\" , \"MD\" ), city = c ( \"Baltimore\" , \"Annapolis\" , \"Ocean City\" )) ) combined <- data.frame ( Reduce ( rbind , state_data ))","title":"Reduce"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#filter","text":"The Filter function does basically what it sounds like \u2014 it applies a filter to a vector, list, or data frame (which is actually a type of list). It takes two main inputs, a function that applies the filter, and the object for which the filter applies. Here\u2019s a simple example: test <- 1 : 10 less_than_5 <- Filter ( function ( x ) x < 5 , test ) This, once again, creates a vector of the first 10 positive integers. The Filter function applies function(x) x < 5 to each element, x , in the vector, test . In other words, it checks each element, x , for the Boolean expression, x < 5 . If an element is not less than 5, it gets filtered out. So you might be thinking\u2026can\u2019t this be done like this? less_than_5 <- test[test < 5] \u2026and the answer is\u2026yes. It can be done that way. Filter is more useful as a function in cases involving data frames or lists. Suppose, for instance, you want to remove all constant columns from a data frame. This is something that may be done when preprocessing data prior to modeling, as a constant attribute isn\u2019t particular useful. This is can be done in one line using Filter df <- data . frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( 1 , 1 , 1 ), d = c ( 3 , 4 , 5 )) without_constants <- Filter ( function ( x ) length ( unique ( x )) > 1 , df ) Alternatively, using dplyr\u2019s n_distinct function, which counts the number of distinct elements in a vector, you could do this: library ( dplyr ) df <- data . frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( 1 , 1 , 1 ), d = c ( 3 , 4 , 5 )) without_constants <- Filter ( function ( x ) n_distinct ( x ) > 1 , df ) In the example, we create a data frame with four columns \u2014 two of them are constant. Filter tests whether there is more than one unique value in each column. If there is only one unique value, then we know the column is constant, and it gets filtered out. Each element x is a vector, or column, in the data frame. If you wanted to just drop all columns that are all NAs, you could make a minor tweak like this: df <- data.frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( 1 , 1 , 1 ), d = c ( NA , NA , NA )) without_nas <- Filter ( function ( x ) ! all ( is.na ( x )), df ) Filter can also be used on a regular list as well. Suppose you have a list of vectors, where some of the vectors are characters, while others are numeric. If want to filter out all of the non-numeric vectors, you could call Filter : sample_list <- list ( a = c ( 1 , 2 , 3 ), b = c ( \"is\" , \"a\" , \"character\" ), c = c ( 4 , 5 , 6 ), d = c ( \"is\" , \"another\" , \"character\" )) only_numeric <- Filter ( function ( x ) is.numeric ( x ), sample_list )","title":"Filter"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#rapply","text":"The rapply function is part of the apply family of functions in R. It has a few different uses, but one of my favorite applications for it is to apply a function to columns of a data frame that belong to a specific class, or have a particular data type. Let\u2019s say you want to get the sum of all of the numeric columns. df <- data.frame ( a = c ( 2 , 2 , 2 ), b = c ( 1 , 2 , 3 ), c = c ( \"r\" , \"is\" , \"awesome\" ), d = c ( 3 , 4 , 5 ), e = c ( \"some\" , \"other\" , \"character\" )) summed_columns <- rapply ( df , sum , class = \"numeric\" ) Similar to sapply or lapply , rapply takes a list / vector / data frame as input, along with a function to be applied. However, it can also take a \u201cclass\u201d parameter, which allows us to specify what class of object we want our function to be used for. rapply can also be used to recursively apply functions to nested lists (see examples from its documentation here ).","title":"rapply"},{"location":"R/Base%20Package%20Hidden%20Gems%20in%20R/#rep","text":"The last function I want to mention for this post is the rep function. This can be used to repeat a value as many times as you want. So if you want to create a vector of 1000 5\u2019s, it could be done like this: rep(5, 1000) Here\u2019s a couple other examples: rep(\"a\", 500) rep(\"repeat this\", 100) If you pass a vector with more than one element to rep , the entire vector gets repeated the number of times you specify. rep(c(1,2,3), 100) The above code will create a vector with 300 elements \u2014 the number of elements in c(1,2,3) times 100, repeating 1, 2, 3 over and over. Links: Source:","title":"rep"},{"location":"R/Data%20Validation%20Packages%20in%20R/","text":"Data Validation Packages and Tools in R \u2691 Packages \u2691 pointblank assertr assertthat checkmate ruler ensurer tester sealr validateIt More leaned towards validation: naniar skimr validate Miscellaneous/Related rstudio/shinyvalidate shinyFeedback rjsonvalidate validator lumberjack : Track changes in data with ease Lumberjack-App : The lumberjack-app is a Shiny Application that predicts tree volume in cubic feet from a linear model using data from the \"trees\" dataset in the R datasets package","title":"Data Validation Packages and Tools in R"},{"location":"R/Data%20Validation%20Packages%20in%20R/#data-validation-packages-and-tools-in-r","text":"","title":"Data Validation Packages and Tools in R"},{"location":"R/Data%20Validation%20Packages%20in%20R/#packages","text":"pointblank assertr assertthat checkmate ruler ensurer tester sealr validateIt More leaned towards validation: naniar skimr validate Miscellaneous/Related rstudio/shinyvalidate shinyFeedback rjsonvalidate validator lumberjack : Track changes in data with ease Lumberjack-App : The lumberjack-app is a Shiny Application that predicts tree volume in cubic feet from a linear model using data from the \"trees\" dataset in the R datasets package","title":"Packages"},{"location":"R/Databases%20with%20R%20Resources/","text":"Databases with R Resources \u2691 Resources: \u2691 Databases using R (rstudio.com) A Comprehensive Introduction to Working with Databases using R - Rsquared Academy Blog R Packages \u2691 r-dbi/DBI r-dbi/RPostgres (DBI Driver) & tomoakin/RPostgreSQL (ODBC Driver) rstudio/pool data-cleaning/validatedb r-dbi/dblog [dbx] [databaser] [threadr] & [dbr] [DatabaseConnector] [ShinyPostgreSQL] [dittodb] Links: Source:","title":"Databases with R Resources"},{"location":"R/Databases%20with%20R%20Resources/#databases-with-r-resources","text":"","title":"Databases with R Resources"},{"location":"R/Databases%20with%20R%20Resources/#resources","text":"Databases using R (rstudio.com) A Comprehensive Introduction to Working with Databases using R - Rsquared Academy Blog","title":"Resources:"},{"location":"R/Databases%20with%20R%20Resources/#r-packages","text":"r-dbi/DBI r-dbi/RPostgres (DBI Driver) & tomoakin/RPostgreSQL (ODBC Driver) rstudio/pool data-cleaning/validatedb r-dbi/dblog [dbx] [databaser] [threadr] & [dbr] [DatabaseConnector] [ShinyPostgreSQL] [dittodb] Links: Source:","title":"R Packages"},{"location":"R/EDA%20Packages%20in%20R/","text":"EDA Packages in R \u2691 Exploratory Data Analysis | R for Data Science (had.co.nz) dataMaid (CRAN package) - automated checks of data validity. DataExplorer (CRAN package) - automated data exploration (including univariate and bivariate plots, PCA) and treatment. funModeling (CRAN package) - automated EDA, simple feature engineering and outlier detection. SmartEDA ( CRAN | Github ) - automated generation of descriptive statistics and uni- and bivariate plots, parallel coordinate plots. Details can be found in a dedicated paper . autoEDA (GitHub package) - automated EDA with uni- and bivariate plots. An article with an introduction can be found on LinkedIn . visdat (CRAN package) - 6 exploratory/diagnostic plots for initial data analysis. dlookr (CRAN package) - tools for data quality diagnosis, basic exploration and feature transformations. xray (CRAN package) - first look at the data - distributions and anomalies. More in the blog post . arsenal (CRAN package) - statistical summaries (models and exploration) and quick reporting. RtutoR (CRAN package) - learning material with a automatic reports module. More at R-Bloggers . exploreR (CRAN package) - exploration based on univariate linear regression. summarytools (CRAN package) - table to summarise datasets and perform simple uni- and bivariate analyses. inspectdf (CRAN package) - tools for column-wise exploration and comparison of data frames. Examples are provided in a README of the GitHub repo . explore (CRAN package) - interactive Shiny app for comprehensive dataset exploration (including uni- and bivariate relationships, correlation analysis and simple modeling with decision trees) and stand-alone function for quick exploration. Examples are given in a vignette . skimr (CRAN package) - well formatted summaries of data frames, vector and matrices. Examples are provided in a vignette . janitor (CRAN package) - a tools for fast data cleaning. All functionalities are introduced in the vignette . autoplotly (CRAN package) - a library for fast visualization of statistical results supported by ggfortify. Details can be found in the vignette or JOSS paper brinton (CRAN package) - packages for quick exploration and visualization. Details can be found in the documentation . AEDA (GitHub package) - summary statistics, correlation analysis, cluster analysis, PCA & other projections. automatic-data-explorer (GitHub package) - basic EDA and creating Markdown reports from multiple R scripts. xda (GitHub package) - basic data summaries. modeler (GitHub package) - tools for exploration and pre-processing. IEDA (GitHub package) - EDA simplified through interactive visualization. dfvis (GitHub package) - ggplot2 based implementation of tabplot. Domain-specific packages \u2691 compMS2Miner: An Automatable Metabolite Identification, Visualization, and Data-Sharing R Package for High-Resolution LC-MS Data Sets RBioPlot (GitHub package) - automated data analysis and visualization for molecular biology. Details can be found in the paper at NCBI . ExPanDaR - package for interactive data visualization. Designed for longitudinal data, but can be also used with other types of data after setting an artificial time variable. Shiny apps with examples are provided on the github website of the package . brolgar (GitHub package) - tools to assist in longitudinal data analysis POMA (Bioconductor package) - structured, reproducible and easy-to-use workflow for the visualization, pre-processing, exploratory data analysis and statistical analysis of mass spectrometry data. POMA R/Shiny version available here . featuretoolsR (CRAN package) - R port to Python library for automated feature engineering. vtreat (CRAN package) - data treatment (pre-processing) that includes dealing with missing data and large categorical variables. Details can be found in the paper about vtreat . report - automated modeling report generation. FactoInvestigate (CRAN package) - has an automatic reporting module which selects best plots that summarise different projection techniques. gtsummary (GitHub package) - presentation-ready tables summarizing data sets, regression models, and more. clean (CRAN package) - fast data cleaning. finalfit (CRAN package) - tables and plots to quickly visualize regression results. modelsummary (GitHub package) - summary tables for regression models. Python libraries \u2691 General Packages \u2691 DataPrep (pip library) - data preparation library with an EDA package. Dora (pip library) - data cleaning, featuring engineering and simple modeling tools. statsModels (pip library) - collection of statistical tools, including EDA. TPOT (pip library) - autoML tool with feature engineering module. HoloViews (pip library) - automated visualization based on short data annotations. lens (pip library) - fast calculation of summary statistics and correlations. Presentation about the library . pandas-profiling - popular library for quick data summaries and correlation analysis. speedML (pip library) - large library for ML with module dedicated to fast EDA. edaviz - Python library for fast data exploration that provides functions for dataset overviews, bivariate plots and finding good predictors. (Free version only works for small datasets). AutoViz - Python library for automated visualization. ExploriPy - Python library for various EDA tasks. pandas-summary - simple extension to pandas.describe. sweetviz - visualizations for automated EDA. Related packages \u2691 featuretools - library for automated feature engineering. pyvtreat - Python version of the R's vtreat package. autoimpute - easier handling of missing values. Auto_TS - automated time series modeling. Stata packages \u2691 eda - a package that produces a pdf report with all permutations of univariate and bivariate visualizations and tables. Notably, three-dimensional displays are also possible. Web services \u2691 DIVE - MIT's tools for data exploration that tries to choose best (most informative) visualizations. Automatic Statistician - tool for automated EDA and modeling. Several Shiny apps by R Squared Computing, including visulizer and descriptr . Standalone software \u2691 auto-eda - automatic EDA with SQL. elycite - tools for exploration and modelling available (locally) as an web application. Designed for NLP problems. Papers and short articles \u2691 Methods and tools for autoEDA \u2691 Interactive Data Exploration with \u201cBig Data Tukey Plots\u201d - automated visualization of big data. Extracting Top-K Insights from Multi-dimensional Data . Agency plus Automation: Designing Artificial Intelligence into Interactive Systems The Landscape of R Packages for Automated Exploratory Data Analysis Issues in Automating Exploratory Data Analysis Automating anomaly detection for exploratory data analytics Task-Oriented Optimal Sequencing of Visualization Charts A Rank-by-Feature Framework for Interactive Exploration of Multidimensional Data - A paper that describe many measures that can be used to sort 1d and 2d data displays. Towards a benefit-based optimizer for Interactive Data Analysis Spotfire: an information exploration environment AlphaClean: Automatic Generation ofData Cleaning Pipelines Testing MS Excel's autoEDA tool Visualization recommendation frameworks \u2691 Foresight: Recommending Visual Insights - Foresight is a system that helps the user rapidly discover visual insights from large high-dimensional datasets. DIVE: A Mixed-Initiative System Supporting Integrated Data Exploration Workflows . The web app is available on MIT website . Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations . Voyager 2: Augmenting Visual Analysis with Partial View Specifications VizML: A Machine Learning Approach to Visualization Recommendation VizDeck: Streamlining Exploratory Visual Analytics of Scientific Data Augmented analytics \u2691 Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication . Conference presentations \u2691 Automating exploratory data analysis tasks with eda - Billy Buchanan","title":"EDA Packages in R"},{"location":"R/EDA%20Packages%20in%20R/#eda-packages-in-r","text":"Exploratory Data Analysis | R for Data Science (had.co.nz) dataMaid (CRAN package) - automated checks of data validity. DataExplorer (CRAN package) - automated data exploration (including univariate and bivariate plots, PCA) and treatment. funModeling (CRAN package) - automated EDA, simple feature engineering and outlier detection. SmartEDA ( CRAN | Github ) - automated generation of descriptive statistics and uni- and bivariate plots, parallel coordinate plots. Details can be found in a dedicated paper . autoEDA (GitHub package) - automated EDA with uni- and bivariate plots. An article with an introduction can be found on LinkedIn . visdat (CRAN package) - 6 exploratory/diagnostic plots for initial data analysis. dlookr (CRAN package) - tools for data quality diagnosis, basic exploration and feature transformations. xray (CRAN package) - first look at the data - distributions and anomalies. More in the blog post . arsenal (CRAN package) - statistical summaries (models and exploration) and quick reporting. RtutoR (CRAN package) - learning material with a automatic reports module. More at R-Bloggers . exploreR (CRAN package) - exploration based on univariate linear regression. summarytools (CRAN package) - table to summarise datasets and perform simple uni- and bivariate analyses. inspectdf (CRAN package) - tools for column-wise exploration and comparison of data frames. Examples are provided in a README of the GitHub repo . explore (CRAN package) - interactive Shiny app for comprehensive dataset exploration (including uni- and bivariate relationships, correlation analysis and simple modeling with decision trees) and stand-alone function for quick exploration. Examples are given in a vignette . skimr (CRAN package) - well formatted summaries of data frames, vector and matrices. Examples are provided in a vignette . janitor (CRAN package) - a tools for fast data cleaning. All functionalities are introduced in the vignette . autoplotly (CRAN package) - a library for fast visualization of statistical results supported by ggfortify. Details can be found in the vignette or JOSS paper brinton (CRAN package) - packages for quick exploration and visualization. Details can be found in the documentation . AEDA (GitHub package) - summary statistics, correlation analysis, cluster analysis, PCA & other projections. automatic-data-explorer (GitHub package) - basic EDA and creating Markdown reports from multiple R scripts. xda (GitHub package) - basic data summaries. modeler (GitHub package) - tools for exploration and pre-processing. IEDA (GitHub package) - EDA simplified through interactive visualization. dfvis (GitHub package) - ggplot2 based implementation of tabplot.","title":"EDA Packages in R"},{"location":"R/EDA%20Packages%20in%20R/#domain-specific-packages","text":"compMS2Miner: An Automatable Metabolite Identification, Visualization, and Data-Sharing R Package for High-Resolution LC-MS Data Sets RBioPlot (GitHub package) - automated data analysis and visualization for molecular biology. Details can be found in the paper at NCBI . ExPanDaR - package for interactive data visualization. Designed for longitudinal data, but can be also used with other types of data after setting an artificial time variable. Shiny apps with examples are provided on the github website of the package . brolgar (GitHub package) - tools to assist in longitudinal data analysis POMA (Bioconductor package) - structured, reproducible and easy-to-use workflow for the visualization, pre-processing, exploratory data analysis and statistical analysis of mass spectrometry data. POMA R/Shiny version available here . featuretoolsR (CRAN package) - R port to Python library for automated feature engineering. vtreat (CRAN package) - data treatment (pre-processing) that includes dealing with missing data and large categorical variables. Details can be found in the paper about vtreat . report - automated modeling report generation. FactoInvestigate (CRAN package) - has an automatic reporting module which selects best plots that summarise different projection techniques. gtsummary (GitHub package) - presentation-ready tables summarizing data sets, regression models, and more. clean (CRAN package) - fast data cleaning. finalfit (CRAN package) - tables and plots to quickly visualize regression results. modelsummary (GitHub package) - summary tables for regression models.","title":"Domain-specific packages"},{"location":"R/EDA%20Packages%20in%20R/#python-libraries","text":"","title":"Python libraries"},{"location":"R/EDA%20Packages%20in%20R/#general-packages","text":"DataPrep (pip library) - data preparation library with an EDA package. Dora (pip library) - data cleaning, featuring engineering and simple modeling tools. statsModels (pip library) - collection of statistical tools, including EDA. TPOT (pip library) - autoML tool with feature engineering module. HoloViews (pip library) - automated visualization based on short data annotations. lens (pip library) - fast calculation of summary statistics and correlations. Presentation about the library . pandas-profiling - popular library for quick data summaries and correlation analysis. speedML (pip library) - large library for ML with module dedicated to fast EDA. edaviz - Python library for fast data exploration that provides functions for dataset overviews, bivariate plots and finding good predictors. (Free version only works for small datasets). AutoViz - Python library for automated visualization. ExploriPy - Python library for various EDA tasks. pandas-summary - simple extension to pandas.describe. sweetviz - visualizations for automated EDA.","title":"General Packages"},{"location":"R/EDA%20Packages%20in%20R/#related-packages","text":"featuretools - library for automated feature engineering. pyvtreat - Python version of the R's vtreat package. autoimpute - easier handling of missing values. Auto_TS - automated time series modeling.","title":"Related packages"},{"location":"R/EDA%20Packages%20in%20R/#stata-packages","text":"eda - a package that produces a pdf report with all permutations of univariate and bivariate visualizations and tables. Notably, three-dimensional displays are also possible.","title":"Stata packages"},{"location":"R/EDA%20Packages%20in%20R/#web-services","text":"DIVE - MIT's tools for data exploration that tries to choose best (most informative) visualizations. Automatic Statistician - tool for automated EDA and modeling. Several Shiny apps by R Squared Computing, including visulizer and descriptr .","title":"Web services"},{"location":"R/EDA%20Packages%20in%20R/#standalone-software","text":"auto-eda - automatic EDA with SQL. elycite - tools for exploration and modelling available (locally) as an web application. Designed for NLP problems.","title":"Standalone software"},{"location":"R/EDA%20Packages%20in%20R/#papers-and-short-articles","text":"","title":"Papers and short articles"},{"location":"R/EDA%20Packages%20in%20R/#methods-and-tools-for-autoeda","text":"Interactive Data Exploration with \u201cBig Data Tukey Plots\u201d - automated visualization of big data. Extracting Top-K Insights from Multi-dimensional Data . Agency plus Automation: Designing Artificial Intelligence into Interactive Systems The Landscape of R Packages for Automated Exploratory Data Analysis Issues in Automating Exploratory Data Analysis Automating anomaly detection for exploratory data analytics Task-Oriented Optimal Sequencing of Visualization Charts A Rank-by-Feature Framework for Interactive Exploration of Multidimensional Data - A paper that describe many measures that can be used to sort 1d and 2d data displays. Towards a benefit-based optimizer for Interactive Data Analysis Spotfire: an information exploration environment AlphaClean: Automatic Generation ofData Cleaning Pipelines Testing MS Excel's autoEDA tool","title":"Methods and tools for autoEDA"},{"location":"R/EDA%20Packages%20in%20R/#visualization-recommendation-frameworks","text":"Foresight: Recommending Visual Insights - Foresight is a system that helps the user rapidly discover visual insights from large high-dimensional datasets. DIVE: A Mixed-Initiative System Supporting Integrated Data Exploration Workflows . The web app is available on MIT website . Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations . Voyager 2: Augmenting Visual Analysis with Partial View Specifications VizML: A Machine Learning Approach to Visualization Recommendation VizDeck: Streamlining Exploratory Visual Analytics of Scientific Data","title":"Visualization recommendation frameworks"},{"location":"R/EDA%20Packages%20in%20R/#augmented-analytics","text":"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication .","title":"Augmented analytics"},{"location":"R/EDA%20Packages%20in%20R/#conference-presentations","text":"Automating exploratory data analysis tasks with eda - Billy Buchanan","title":"Conference presentations"},{"location":"R/Plumber%20Logging/","text":"Plumber Logging \u2691 The plumber R package is used to expose R functions as API endpoints. Due to plumber\u2019s incredible flexibility, most major API design decisions are left up to the developer. One important consideration to be made when developing APIs is how to log information about API requests and responses. This information can be used to determine how plumber APIs are performing and how they are being utilized. An example of logging API requests in plumber is included in the package documentation . That example uses a filter to log information about incoming requests before a response has been generated. This is certainly a valid approach, but it means that the log cannot contain details about the response since the response hasn\u2019t been created yet. In this post we will look at an alternative approach to logging plumber APIs that uses preroute and postroute hooks to log information about each API request and its associated response. Logging \u2691 Logging packages for R: logger Example API \u2691 In this example, I use the logger package to generate the actual log entries. Using this package isn\u2019t required, but it does provide some convenient functionality that we will take advantage of. Since we will be registering hooks for our API, we will need both a plumber.R file and an entrypoint.R file. The plumber.R file contains the following: # plumber.R library ( plumber ) #* @apiTitle Logging Example #* @apiDescription Simple example API for implementing logging with Plumber #* Echo back the input #* @param msg The message to echo #* @get /echo function ( msg = \"\" ) { list ( msg = paste0 ( \"The message is: '\" , msg , \"'\" )) } #* Plot a histogram #* @png #* @get /plot function () { rand <- rnorm ( 100 ) hist ( rand ) } Now that we\u2019ve defined two endpoints ( /echo and /plot ), we can use entrypoint.R to setup logging using preroute and postroute hooks. First, we need to configure the logger package: # entrypoint.R library ( plumber ) # logging library ( logger ) # Specify how logs are written log_dir <- \"logs\" if ( ! fs :: dir_exists ( log_dir )) fs :: dir_create ( log_dir ) log_appender ( appender_tee ( tempfile ( \"plumber_\" , log_dir , \".log\" ))) The log_appender() function is used to specify which appender method is used for logging. Here we use appender_tee() so that logs will be written to stdout and to a specific file path. We create a directory called logs/ in the current working directory to store the resulting logs. Every log file is assigned a unique name using tempfile() . This prevents errors that can occur if concurrent processes try to write to the same file. Now, we need to create a helper function that we will use when creating log entries: convert_empty <- function ( string ) { if ( string == \"\" ) { \"-\" } else { string } } This function takes an empty string and converts it into a dash ( \"-\" ). We will use this to ensure that empty log values still get recorded so that it is easy to read the log files. We\u2019re now ready to create our plumber router and register the hooks necessary for logging: pr <- plumb ( \"plumber.R\" ) pr $ registerHooks ( list ( preroute = function () { # Start timer for log info tictoc :: tic () }, postroute = function ( req , res ) { end <- tictoc :: toc ( quiet = TRUE ) # Log details about the request and the response log_info ( '{convert_empty(req$REMOTE_ADDR)} \"{convert_empty(req$HTTP_USER_AGENT)}\" {convert_empty(req$HTTP_HOST)} {convert_empty(req$REQUEST_METHOD)} {convert_empty(req$PATH_INFO)} {convert_empty(res$status)} {round(end$toc - end$tic, digits = getOption(\"digits\", 5))}' ) } ) ) pr Links: R Development Source: Plumber Logging \u00b7 R Views (rstudio.com)","title":"Plumber Logging"},{"location":"R/Plumber%20Logging/#plumber-logging","text":"The plumber R package is used to expose R functions as API endpoints. Due to plumber\u2019s incredible flexibility, most major API design decisions are left up to the developer. One important consideration to be made when developing APIs is how to log information about API requests and responses. This information can be used to determine how plumber APIs are performing and how they are being utilized. An example of logging API requests in plumber is included in the package documentation . That example uses a filter to log information about incoming requests before a response has been generated. This is certainly a valid approach, but it means that the log cannot contain details about the response since the response hasn\u2019t been created yet. In this post we will look at an alternative approach to logging plumber APIs that uses preroute and postroute hooks to log information about each API request and its associated response.","title":"Plumber Logging"},{"location":"R/Plumber%20Logging/#logging","text":"Logging packages for R: logger","title":"Logging"},{"location":"R/Plumber%20Logging/#example-api","text":"In this example, I use the logger package to generate the actual log entries. Using this package isn\u2019t required, but it does provide some convenient functionality that we will take advantage of. Since we will be registering hooks for our API, we will need both a plumber.R file and an entrypoint.R file. The plumber.R file contains the following: # plumber.R library ( plumber ) #* @apiTitle Logging Example #* @apiDescription Simple example API for implementing logging with Plumber #* Echo back the input #* @param msg The message to echo #* @get /echo function ( msg = \"\" ) { list ( msg = paste0 ( \"The message is: '\" , msg , \"'\" )) } #* Plot a histogram #* @png #* @get /plot function () { rand <- rnorm ( 100 ) hist ( rand ) } Now that we\u2019ve defined two endpoints ( /echo and /plot ), we can use entrypoint.R to setup logging using preroute and postroute hooks. First, we need to configure the logger package: # entrypoint.R library ( plumber ) # logging library ( logger ) # Specify how logs are written log_dir <- \"logs\" if ( ! fs :: dir_exists ( log_dir )) fs :: dir_create ( log_dir ) log_appender ( appender_tee ( tempfile ( \"plumber_\" , log_dir , \".log\" ))) The log_appender() function is used to specify which appender method is used for logging. Here we use appender_tee() so that logs will be written to stdout and to a specific file path. We create a directory called logs/ in the current working directory to store the resulting logs. Every log file is assigned a unique name using tempfile() . This prevents errors that can occur if concurrent processes try to write to the same file. Now, we need to create a helper function that we will use when creating log entries: convert_empty <- function ( string ) { if ( string == \"\" ) { \"-\" } else { string } } This function takes an empty string and converts it into a dash ( \"-\" ). We will use this to ensure that empty log values still get recorded so that it is easy to read the log files. We\u2019re now ready to create our plumber router and register the hooks necessary for logging: pr <- plumb ( \"plumber.R\" ) pr $ registerHooks ( list ( preroute = function () { # Start timer for log info tictoc :: tic () }, postroute = function ( req , res ) { end <- tictoc :: toc ( quiet = TRUE ) # Log details about the request and the response log_info ( '{convert_empty(req$REMOTE_ADDR)} \"{convert_empty(req$HTTP_USER_AGENT)}\" {convert_empty(req$HTTP_HOST)} {convert_empty(req$REQUEST_METHOD)} {convert_empty(req$PATH_INFO)} {convert_empty(res$status)} {round(end$toc - end$tic, digits = getOption(\"digits\", 5))}' ) } ) ) pr Links: R Development Source: Plumber Logging \u00b7 R Views (rstudio.com)","title":"Example API"},{"location":"R/Plumber%20REST%20APIs%20in%20R/","text":"Building REST APIs with R and Plumber \u2691 REST stands for \u201cRepresentational State Transfer\u201d, meaning it represents a set of rules developers follow when creating APIs (i.e. you get a responding piece of data, the response, whenever you make a request to a particular URL). Every request is composed of these four parts: Endpoint - a part of the URL - The endpoint for https://example.com/predict is /predict . Method - a type of request you\u2019re sending; used to perform one of these actions: Create, Read, Update, Delete (CRUD) . Can be one of the following: GET POST PUT PATCH DELETE Headers \u2013 used for providing information (think authentication credentials, for example). They are provided as key-value pairs. Body \u2013 information that is sent to the server. Used only when not making GET requests. Most of the time, the response returned after making a request is in JSON format. The alternative format is XML, but JSON is more common. You can also return other objects, such as images instead. You\u2019ll learn how to do that today. R allows you to develop REST APIs with the plumber package. You can read the official documentation here. It\u2019s easy to repurpose any R script file to an API with plumber, because you only have to decorate your functions with comments. You\u2019ll see all about it in a bit.","title":"Building REST APIs with R and Plumber"},{"location":"R/Plumber%20REST%20APIs%20in%20R/#building-rest-apis-with-r-and-plumber","text":"REST stands for \u201cRepresentational State Transfer\u201d, meaning it represents a set of rules developers follow when creating APIs (i.e. you get a responding piece of data, the response, whenever you make a request to a particular URL). Every request is composed of these four parts: Endpoint - a part of the URL - The endpoint for https://example.com/predict is /predict . Method - a type of request you\u2019re sending; used to perform one of these actions: Create, Read, Update, Delete (CRUD) . Can be one of the following: GET POST PUT PATCH DELETE Headers \u2013 used for providing information (think authentication credentials, for example). They are provided as key-value pairs. Body \u2013 information that is sent to the server. Used only when not making GET requests. Most of the time, the response returned after making a request is in JSON format. The alternative format is XML, but JSON is more common. You can also return other objects, such as images instead. You\u2019ll learn how to do that today. R allows you to develop REST APIs with the plumber package. You can read the official documentation here. It\u2019s easy to repurpose any R script file to an API with plumber, because you only have to decorate your functions with comments. You\u2019ll see all about it in a bit.","title":"Building REST APIs with R and Plumber"},{"location":"R/Plumber%20Resources/","text":"Plumber Resources \u2691 Links: Source:","title":"Plumber Resources"},{"location":"R/Plumber%20Resources/#plumber-resources","text":"Links: Source:","title":"Plumber Resources"},{"location":"R/R%20Books/","text":"R-Books \u2691 A non-exhaustive list of R books. Full list here bookdown.org Contents \u2691 R Core Team Manuals R Core Team Manuals \u2691 Thanks to Colin Fay for re-creating these as bookdowns! View the list here: R manuals - Colin Fay . The original versions by R-Core can be found here: Index of R Manuals . An Introduction to R This manual provides information on data types, programming elements, statistical modeling and graphics. R Data Import/Export This is a guide to importing and exporting data to and from R. R Installation and Administration This is a guide to installation and administration for R. Writing R Extensions Remastered version original CRAN version of Writing R Extensions The R Language Definition This is an introduction to the R language, explaining evaluation, parsing, object oriented programming, computing on the language, and so forth. R Internals This is a guide to the internal structures of R and coding standards for the core team working on R itself. R Programming \u2691 Advanced R This is the website for work-in-progress 2nd edition of \u201cAdvanced R\u201d, a book in Chapman & Hall\u2019s R Series. The book is designed primarily for R users who want to improve their programming skills and understanding of the language. It should also be useful for programmers coming to R from other languages, as it explains some of R\u2019s quirks and shows how some parts that seem horrible do have a positive side. Efficient R programming Drawing on years of experience teaching R courses, authors Colin Gillespie and Robin Lovelace provide practical advice on a range of topics\u2014from optimizing the set-up of RStudio to leveraging C++\u2014that make this book a useful addition to any R user\u2019s bookshelf. Academics, business users, and programmers from a wide range of backgrounds stand to benefit from the guidance in Efficient R Programming. Mastering Software Development in R This book is designed to be used in conjunction with the course sequence Mastering Software Development in R, available on Coursera. The book covers R software development for building data science tools. As the field of data science evolves, it has become clear that software development skills are essential for producing useful data science results and products. You will obtain rigorous training in the R language, including the skills for handling complex data, building R packages and developing custom data visualizations. You will learn modern software development practices to build tools that are highly reusable, modular, and suitable for use in a team-based environment or a community of developers. The tidyverse style guide Good coding style is like correct punctuation: you can manage without it, but it sure makes things easier to read. This site describes the style used throughout the tidyverse. It was originally derived from Google\u2019s R style guide, but has evolved and expanded considerably over the years. R, Not the Best Practices This book will follow the first three months of learning R at my new job. Before that I was just okay at computers. I played games, used the interweb and Microsoft Word and Excel for professional stuff. I did not know anything about programming nor did I know a single programmer. Code for me was just that, code, a random string of matrix letters that meant nothing. Since the memories of not knowing shit are still fresh, I feel quite confident that I should be able to put myself in your shoes and bring you to the level of proficiency that you should be after learning R for three months. Yes, three months. This book will cover my first quarter of dealing with R and programming in general. Modern R with the tidyverse This book can be useful to different audiences. If you have never used R in your life, and want to start, start with Chapter 1 of this book. Chapter 1 to 3 are the very basics, and should be easy to follow up to Chapter 9. Starting with Chapter 9, it gets more technical, and will be harder to follow. But I suggest you keep on going, and do not hesitate to contact me for help if you struggle! Chapter 9 is also where you can start if you are already familiar with R and the {tidyverse} , but not functional programming. If you are familiar with R but not the {tidyverse} (or have no clue what the {tidyverse} is), then you can start with Chapter 4. If you are familiar with R, the {tidyverse} and functional programming, you might still be interested in this book, especially Chapter 9 and 10, which deal with package development and further advanced topics respectively. A Sufficient Introduction to R This book is intended to guide people that are completely new to programming along a path towards a useful skill level using R. I belive that while people can get by with just copying code chunks, that doesn\u2019t give them the background information to modify the code in non-trivial ways. Therefore we will spend more time on foundational details than a \u201ccrash-course\u201d would. R Package Development \u2691 Writing R Extensions Remastered version original CRAN version of Writing R Extensions R Packages Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. In this book you\u2019ll learn how to turn your code into packages that others can easily download and use. rOpenSci Packages Dev Guide: Development, Maintenance, and Peer Review This book is a guide for authors, maintainers, reviewers and editors of rOpenSci. The first section of the book contains our guidelines for creating and testing R packages. The second section is dedicated to rOpenSci\u2019s software peer review process: what it is, our policies, and specific guides for authors, editors and reviewers throughout the process. The third and last section features our best practice for nurturing your package once it has been onboarded: how to collaborate with other developers, how to document releases, how to promote your package and how to leverage GitHub as a development platform. The third section also features a chapter for anyone wishing to start contributing to rOpenSci packages . R Shiny \u2691 JavaScript for R This book aims to remedy that by revealing how much JavaScript can greatly enhance various stages of data science pipelines from the analysis to the communication of results. Mastering Shiny Shiny is a framework for creating web applications using R code. It is designed primarily with data scientists in mind, and to that end, you can create pretty complicated Shiny apps with no knowledge of HTML, CSS, or JavaScript. Outstanding User Interfaces with Shiny This book is not an HTML/Javascript/CSS course! Instead, it provides a survival kit to be able to customize Shiny. I am sure however that readers will want to explore more about these topics. Engineering Production-Grade Shiny Apps This book will not get you started with Shiny, nor talk how to work with Shiny once it is sent to production. What we\u2019ll see is the process of building an application that will later be sent to production. Data Science \u2691 R for Data Science This is the website for \u201cR for Data Science\u201d. This book will teach you how to do data science with R: You\u2019ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you\u2019ll learn how to clean data and draw plots\u2014and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You\u2019ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You\u2019ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data. Functional programming and unit testing for data munging with R This book serves to show how functional programming and unit testing can be useful for the task of data munging. This book is not an in-depth guide to functional programming, nor unit testing with R. If you want to have an in-depth understanding of the concepts presented in these books, I can\u2019t but recommend Wickham (2014a), Wickham (2015) and Wickham and Grolemund (2016) enough. Here, I will only briefly present functional programming, unit testing and building your own R packages. Just enough to get you (hopefully) interested and going. Exploratory Data Analysis with R This book covers the essential exploratory techniques for summarizing data with R. These techniques are typically applied before formal modeling commences and can help inform the development of more complex statistical models. Exploratory techniques are also important for eliminating or sharpening potential hypotheses about the world that can be addressed by the data you have. We will cover in detail the plotting systems in R as well as some of the basic principles of constructing informative data graphics. We will also cover some of the common multivariate statistical techniques used to visualize high-dimensional data. Text Mining with R This book serves as an introduction of text mining using the tidytext package and other tidy tools in R. The functions provided by the tidytext package are relatively simple; what is important are the possible applications. Thus, this book provides compelling examples of real text mining problems. Feature Engineering and Selection: A Practical Approach for Predictive Models The goals of Feature Engineering and Selection are to provide tools for re-representing predictors, to place these tools in the context of a good predictive modeling framework, and to convey our experience of utilizing these tools in practice. Statistical Inference via Data Science This book will help you develop your \u201cdata science toolbox\u201d, including tools such as data visualization, data formatting, data wrangling, and data modeling using regression. With these tools, you\u2019ll be able to perform the entirety of the \u201cdata/science pipeline\u201d while building data communication skills Hands-on Machine Learning with R You will learn how to build and tune these various models with R packages that have been tested and approved due to their ability to scale well. However, our motivation in almost every case is to describe the techniques in a way that helps develop intuition for its strengths and weaknesses. Introduction to Data Exploration and Analysis with R This book is designed as a crash course in coding with R and data analysis, built for people trying to teach themselves the skills needed for most analyst jobs today. You won\u2019t need any past experience with R or data analytics - the aim of the book is to work as a primer for people of all backgrounds. Explanatory Model Analysis Explore, Explain, and Examine Predictive Models. With examples in R and Python Data Skills for Reproducible Science This course provides an overview of skills needed for reproducible research and open science using the statistical programming language R. Students will learn about data visualization, data tidying and wrangling, archiving, iteration and functions, probability and data simulations, general linear models, and reproducible workflows. Learning is reinforced through weekly assignments that involve working with different types of data. Working with Data in R This book will provide readers a few basic steps to begin working with data in R. It is not meant as a comprehensive introduction to using R for all of the different functions that are possible. Rather, it is tailored to help an individual that has quantitative data they would like to work with, but has not worked in R previously. The presentation of material is meant to be accessible to students with little to no background in R or computer programming. R Programming for Data Science This book is about the fundamentals of R programming. You will get started with the basics of the language, learn how to manipulate datasets, how to write functions, and how to debug and optimize code. With the fundamentals provided in this book, you will have a solid foundation on which to build your data science toolbox. Designing and Building Data Science Solutions Data science can be an exciting, invigorating field, and for the business leader, it can bring about revolutionary changes to an organization that can come with huge returns on investment and value added. For the data scientist, designing and delivering successful projects is rewarding, stimulating and tremendously gratifying. We hope this guide gives you the confidence to understand the risks and approach your project in a sensible way. Data Preparation: Essential Steps Before & After Analysis It is routinely noted that the Pareto principle applies to data science\u201480% of one\u2019s time is spent on data collection and preparation, and the remaining 20% on the \u201cfun stuff\u201d like modeling, data visualization, and communication. The caret Package The caret package (short for C lassification A nd RE gression T raining) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for: data splitting pre-processing feature selection model tuning using resampling variable importance estimation as well as other functionality. Data Visualization \u2691 Fundamentals of Data Visualization The book is meant as a guide to making visualizations that accurately reflect the data, tell a story, and look professional. It has grown out of my experience of working with students and postdocs in my laboratory on thousands of data visualizations. Over the years, I have noticed that the same issues arise over and over. I have attempted to collect my accumulated knowledge from these interactions in the form of this book. The book\u2019s source code is hosted on GitHub, at https://github.com/clauswilke/dataviz. R Graphics Cookbook, 2nd edition A practical guide that provides more than 150 recipes to help you generate high-quality graphs quickly, without having to comb through all the details of R\u2019s graphing systems. Data Visualization with R R is an amazing platform for data analysis, capable of creating almost any type of graph. This book helps you create the most popular visualizations - from quick and dirty plots to publication-ready graphs. The text relies heavily on the ggplot2 package for graphics, but other approaches are covered as well. Interactive web-based data visualization with R, plotly, and shiny In this book, you\u2019ll gain insight and practical skills for creating interactive and dynamic web graphics for data analysis from R. It makes heavy use of plotly for rendering graphics, but you\u2019ll also learn about other R packages that augment a data science workflow, such as the tidyverse and shiny. Data Visualization with ggplot2 Data Visualization with ggplot2 Databases \u2691 R, Databases, and Docker This book will help you create a hybrid environment on your machine that can mimic some of the uncharted territory in your organization. It goes far beyond the basic connection issues and covers issues that you face when you are finding your way around or writing queries to your organization\u2019s databases, not just when maintaining inherited scripts. Mongolite User Manual This book provides a high level introduction to using MongoDB with the mongolite client in R. Financial \u2691 Techincal Analysis with R This short book is a short introduction on how to use R and RStudio to do financial data analysis from the beginning. No prior knowledge of R is required. While you will learn various skills to work on R programming but the main goal is to learn how to use R to backtest a trading strategy and evaluate its performance. Principles of Econometrics with R Resource for the \u201cPrinciples of Econometrics\u201d textbook by Carter Hill, William Griffiths and Guay Lim, 4-th edition (Hill, Griffiths, and Lim 2011). Processing and Analyzing Financial Data with R This book introduces the reader to the use of R and RStudio as a platform for processing and analyzing financial data. Forecasting: Principles and Practice This textbook is intended to provide a comprehensive introduction to forecasting methods and to present enough information about each method for readers to be able to use them sensibly. We don\u2019t attempt to give a thorough discussion of the theoretical details behind each method, although the references at the end of each chapter will fill in many of those details. R-Markdown and Friends \u2691 R Markdown: The Definitive Guide The document format \u201cR Markdown\u201d was first introduced in the knitr package (Xie 2015, 2018d) in early 2012. The idea was to embed code chunks (of R or other languages) in Markdown documents. In fact, knitr supported several authoring languages from the beginning in addition to Markdown, including LaTeX, HTML, AsciiDoc, reStructuredText, and Textile. Looking back over the five years, it seems to be fair to say that Markdown has become the most popular document format, which is what we expected. The simplicity of Markdown clearly stands out among these document formats. R Markdown Cookbook This book is designed to provide a range of examples on how to extend the functionality of your R Markdown documents. As a cookbook, this guide is recommended to new and intermediate R Markdown users who desire to enhance the efficiency of using R Markdown and also explore the power of R Markdown. Bookdown bookdown: Authoring Books and Technical Documents with R Markdown. This short book introduces an R package, bookdown, to change your workflow of writing books. It should be technically easy to write a book, visually pleasant to view the book, fun to interact with the book, convenient to navigate through the book, straightforward for readers to contribute or leave feedback to the book author(s), and more importantly, authors should not always be distracted by typesetting details. blogdown: Creating Websites with R Markdown We introduce an R package, blogdown, in this short book, to teach you how to create websites using R Markdown and Hugo. If you have experience with creating websites, you may naturally ask what the benefits of using R Markdown are, and how blogdown is different from existing popular website platforms, such as WordPress. Introduction to Research Methods This book is intended as a practical introduction to research methods in the social sciences. If you pursue research academically or professionally, it will probably not be the last book you need to read on the subject. This is intended as something of a gentle introduction with a focus on the applications of the information and examples. Utility \u2691 Happy Git and GitHub for the useR Happy Git provides opinionated instructions on how to: Install Git and get it working smoothly with GitHub, in the shell and in the RStudio IDE. Develop a few key workflows that cover your most common tasks. Integrate Git and GitHub into your daily work with R and R Markdown. What They Forgot to Teach You About R We focus on building holistic and project-oriented workflows that address the most common sources of friction in data analysis, outside of doing the statistical analysis itself . Data Science at the Command Line, 1e Discover why the command line is an agile, scalable, and extensible technology. Even if you\u2019re already comfortable processing data with, say, Python or R, you\u2019ll greatly improve your data science workflow by also leveraging the power of the command line. Github actions with R GitHub actions allow us to trigger automated steps after we launch GitHub interactions such as when we push, pull, submit a pull request, or write an issue. For example, there are actions that will automatically trigger: continuous integration (CI) messages in response to issues or pull requests rendering/compiling e.g. of rmarkdown, bookdown, blogdowns etc GitHub actions follow the steps designated in a yaml file, which we place in the .github/workflows folder of the repo. We can add these yaml files to our repo either by clicking on a series of steps on GitHub.com, or using wrapper functions provided by the usethis package, depending on which actions you wish to include. We describe both ways here. Geocomputation with R The book is designed for intermediate-to-advanced R users interested in geocomputation and R beginners who have prior experience with geographic data. If you are new to both R and geographic data, do not be discouraged: we provide links to further materials and describe the nature of spatial data from a beginner\u2019s perspective in Chapter 2 and in links provided below. Business Analytics with R The notes for this course were compiled from years of work in industry using R. These notes are intended to provide the non-programmer and programmer alike a hands-on approach to learning R for use in Business Analysis. The notes are ideal for the Accounting, Business, and Data Science majors desiring a better understanding of how the programming language R may be used to retrieve and organize data, perform analysis, create visualizations and automate business processes to reduce error and improve efficiency in the workplace. Misc books here","title":"R-Books"},{"location":"R/R%20Books/#r-books","text":"A non-exhaustive list of R books. Full list here bookdown.org","title":"R-Books"},{"location":"R/R%20Books/#contents","text":"R Core Team Manuals","title":"Contents"},{"location":"R/R%20Books/#r-core-team-manuals","text":"Thanks to Colin Fay for re-creating these as bookdowns! View the list here: R manuals - Colin Fay . The original versions by R-Core can be found here: Index of R Manuals . An Introduction to R This manual provides information on data types, programming elements, statistical modeling and graphics. R Data Import/Export This is a guide to importing and exporting data to and from R. R Installation and Administration This is a guide to installation and administration for R. Writing R Extensions Remastered version original CRAN version of Writing R Extensions The R Language Definition This is an introduction to the R language, explaining evaluation, parsing, object oriented programming, computing on the language, and so forth. R Internals This is a guide to the internal structures of R and coding standards for the core team working on R itself.","title":"R Core Team Manuals"},{"location":"R/R%20Books/#r-programming","text":"Advanced R This is the website for work-in-progress 2nd edition of \u201cAdvanced R\u201d, a book in Chapman & Hall\u2019s R Series. The book is designed primarily for R users who want to improve their programming skills and understanding of the language. It should also be useful for programmers coming to R from other languages, as it explains some of R\u2019s quirks and shows how some parts that seem horrible do have a positive side. Efficient R programming Drawing on years of experience teaching R courses, authors Colin Gillespie and Robin Lovelace provide practical advice on a range of topics\u2014from optimizing the set-up of RStudio to leveraging C++\u2014that make this book a useful addition to any R user\u2019s bookshelf. Academics, business users, and programmers from a wide range of backgrounds stand to benefit from the guidance in Efficient R Programming. Mastering Software Development in R This book is designed to be used in conjunction with the course sequence Mastering Software Development in R, available on Coursera. The book covers R software development for building data science tools. As the field of data science evolves, it has become clear that software development skills are essential for producing useful data science results and products. You will obtain rigorous training in the R language, including the skills for handling complex data, building R packages and developing custom data visualizations. You will learn modern software development practices to build tools that are highly reusable, modular, and suitable for use in a team-based environment or a community of developers. The tidyverse style guide Good coding style is like correct punctuation: you can manage without it, but it sure makes things easier to read. This site describes the style used throughout the tidyverse. It was originally derived from Google\u2019s R style guide, but has evolved and expanded considerably over the years. R, Not the Best Practices This book will follow the first three months of learning R at my new job. Before that I was just okay at computers. I played games, used the interweb and Microsoft Word and Excel for professional stuff. I did not know anything about programming nor did I know a single programmer. Code for me was just that, code, a random string of matrix letters that meant nothing. Since the memories of not knowing shit are still fresh, I feel quite confident that I should be able to put myself in your shoes and bring you to the level of proficiency that you should be after learning R for three months. Yes, three months. This book will cover my first quarter of dealing with R and programming in general. Modern R with the tidyverse This book can be useful to different audiences. If you have never used R in your life, and want to start, start with Chapter 1 of this book. Chapter 1 to 3 are the very basics, and should be easy to follow up to Chapter 9. Starting with Chapter 9, it gets more technical, and will be harder to follow. But I suggest you keep on going, and do not hesitate to contact me for help if you struggle! Chapter 9 is also where you can start if you are already familiar with R and the {tidyverse} , but not functional programming. If you are familiar with R but not the {tidyverse} (or have no clue what the {tidyverse} is), then you can start with Chapter 4. If you are familiar with R, the {tidyverse} and functional programming, you might still be interested in this book, especially Chapter 9 and 10, which deal with package development and further advanced topics respectively. A Sufficient Introduction to R This book is intended to guide people that are completely new to programming along a path towards a useful skill level using R. I belive that while people can get by with just copying code chunks, that doesn\u2019t give them the background information to modify the code in non-trivial ways. Therefore we will spend more time on foundational details than a \u201ccrash-course\u201d would.","title":"R Programming"},{"location":"R/R%20Books/#r-package-development","text":"Writing R Extensions Remastered version original CRAN version of Writing R Extensions R Packages Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. In this book you\u2019ll learn how to turn your code into packages that others can easily download and use. rOpenSci Packages Dev Guide: Development, Maintenance, and Peer Review This book is a guide for authors, maintainers, reviewers and editors of rOpenSci. The first section of the book contains our guidelines for creating and testing R packages. The second section is dedicated to rOpenSci\u2019s software peer review process: what it is, our policies, and specific guides for authors, editors and reviewers throughout the process. The third and last section features our best practice for nurturing your package once it has been onboarded: how to collaborate with other developers, how to document releases, how to promote your package and how to leverage GitHub as a development platform. The third section also features a chapter for anyone wishing to start contributing to rOpenSci packages .","title":"R Package Development"},{"location":"R/R%20Books/#r-shiny","text":"JavaScript for R This book aims to remedy that by revealing how much JavaScript can greatly enhance various stages of data science pipelines from the analysis to the communication of results. Mastering Shiny Shiny is a framework for creating web applications using R code. It is designed primarily with data scientists in mind, and to that end, you can create pretty complicated Shiny apps with no knowledge of HTML, CSS, or JavaScript. Outstanding User Interfaces with Shiny This book is not an HTML/Javascript/CSS course! Instead, it provides a survival kit to be able to customize Shiny. I am sure however that readers will want to explore more about these topics. Engineering Production-Grade Shiny Apps This book will not get you started with Shiny, nor talk how to work with Shiny once it is sent to production. What we\u2019ll see is the process of building an application that will later be sent to production.","title":"R Shiny"},{"location":"R/R%20Books/#data-science","text":"R for Data Science This is the website for \u201cR for Data Science\u201d. This book will teach you how to do data science with R: You\u2019ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you\u2019ll learn how to clean data and draw plots\u2014and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You\u2019ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You\u2019ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data. Functional programming and unit testing for data munging with R This book serves to show how functional programming and unit testing can be useful for the task of data munging. This book is not an in-depth guide to functional programming, nor unit testing with R. If you want to have an in-depth understanding of the concepts presented in these books, I can\u2019t but recommend Wickham (2014a), Wickham (2015) and Wickham and Grolemund (2016) enough. Here, I will only briefly present functional programming, unit testing and building your own R packages. Just enough to get you (hopefully) interested and going. Exploratory Data Analysis with R This book covers the essential exploratory techniques for summarizing data with R. These techniques are typically applied before formal modeling commences and can help inform the development of more complex statistical models. Exploratory techniques are also important for eliminating or sharpening potential hypotheses about the world that can be addressed by the data you have. We will cover in detail the plotting systems in R as well as some of the basic principles of constructing informative data graphics. We will also cover some of the common multivariate statistical techniques used to visualize high-dimensional data. Text Mining with R This book serves as an introduction of text mining using the tidytext package and other tidy tools in R. The functions provided by the tidytext package are relatively simple; what is important are the possible applications. Thus, this book provides compelling examples of real text mining problems. Feature Engineering and Selection: A Practical Approach for Predictive Models The goals of Feature Engineering and Selection are to provide tools for re-representing predictors, to place these tools in the context of a good predictive modeling framework, and to convey our experience of utilizing these tools in practice. Statistical Inference via Data Science This book will help you develop your \u201cdata science toolbox\u201d, including tools such as data visualization, data formatting, data wrangling, and data modeling using regression. With these tools, you\u2019ll be able to perform the entirety of the \u201cdata/science pipeline\u201d while building data communication skills Hands-on Machine Learning with R You will learn how to build and tune these various models with R packages that have been tested and approved due to their ability to scale well. However, our motivation in almost every case is to describe the techniques in a way that helps develop intuition for its strengths and weaknesses. Introduction to Data Exploration and Analysis with R This book is designed as a crash course in coding with R and data analysis, built for people trying to teach themselves the skills needed for most analyst jobs today. You won\u2019t need any past experience with R or data analytics - the aim of the book is to work as a primer for people of all backgrounds. Explanatory Model Analysis Explore, Explain, and Examine Predictive Models. With examples in R and Python Data Skills for Reproducible Science This course provides an overview of skills needed for reproducible research and open science using the statistical programming language R. Students will learn about data visualization, data tidying and wrangling, archiving, iteration and functions, probability and data simulations, general linear models, and reproducible workflows. Learning is reinforced through weekly assignments that involve working with different types of data. Working with Data in R This book will provide readers a few basic steps to begin working with data in R. It is not meant as a comprehensive introduction to using R for all of the different functions that are possible. Rather, it is tailored to help an individual that has quantitative data they would like to work with, but has not worked in R previously. The presentation of material is meant to be accessible to students with little to no background in R or computer programming. R Programming for Data Science This book is about the fundamentals of R programming. You will get started with the basics of the language, learn how to manipulate datasets, how to write functions, and how to debug and optimize code. With the fundamentals provided in this book, you will have a solid foundation on which to build your data science toolbox. Designing and Building Data Science Solutions Data science can be an exciting, invigorating field, and for the business leader, it can bring about revolutionary changes to an organization that can come with huge returns on investment and value added. For the data scientist, designing and delivering successful projects is rewarding, stimulating and tremendously gratifying. We hope this guide gives you the confidence to understand the risks and approach your project in a sensible way. Data Preparation: Essential Steps Before & After Analysis It is routinely noted that the Pareto principle applies to data science\u201480% of one\u2019s time is spent on data collection and preparation, and the remaining 20% on the \u201cfun stuff\u201d like modeling, data visualization, and communication. The caret Package The caret package (short for C lassification A nd RE gression T raining) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for: data splitting pre-processing feature selection model tuning using resampling variable importance estimation as well as other functionality.","title":"Data Science"},{"location":"R/R%20Books/#data-visualization","text":"Fundamentals of Data Visualization The book is meant as a guide to making visualizations that accurately reflect the data, tell a story, and look professional. It has grown out of my experience of working with students and postdocs in my laboratory on thousands of data visualizations. Over the years, I have noticed that the same issues arise over and over. I have attempted to collect my accumulated knowledge from these interactions in the form of this book. The book\u2019s source code is hosted on GitHub, at https://github.com/clauswilke/dataviz. R Graphics Cookbook, 2nd edition A practical guide that provides more than 150 recipes to help you generate high-quality graphs quickly, without having to comb through all the details of R\u2019s graphing systems. Data Visualization with R R is an amazing platform for data analysis, capable of creating almost any type of graph. This book helps you create the most popular visualizations - from quick and dirty plots to publication-ready graphs. The text relies heavily on the ggplot2 package for graphics, but other approaches are covered as well. Interactive web-based data visualization with R, plotly, and shiny In this book, you\u2019ll gain insight and practical skills for creating interactive and dynamic web graphics for data analysis from R. It makes heavy use of plotly for rendering graphics, but you\u2019ll also learn about other R packages that augment a data science workflow, such as the tidyverse and shiny. Data Visualization with ggplot2 Data Visualization with ggplot2","title":"Data Visualization"},{"location":"R/R%20Books/#databases","text":"R, Databases, and Docker This book will help you create a hybrid environment on your machine that can mimic some of the uncharted territory in your organization. It goes far beyond the basic connection issues and covers issues that you face when you are finding your way around or writing queries to your organization\u2019s databases, not just when maintaining inherited scripts. Mongolite User Manual This book provides a high level introduction to using MongoDB with the mongolite client in R.","title":"Databases"},{"location":"R/R%20Books/#financial","text":"Techincal Analysis with R This short book is a short introduction on how to use R and RStudio to do financial data analysis from the beginning. No prior knowledge of R is required. While you will learn various skills to work on R programming but the main goal is to learn how to use R to backtest a trading strategy and evaluate its performance. Principles of Econometrics with R Resource for the \u201cPrinciples of Econometrics\u201d textbook by Carter Hill, William Griffiths and Guay Lim, 4-th edition (Hill, Griffiths, and Lim 2011). Processing and Analyzing Financial Data with R This book introduces the reader to the use of R and RStudio as a platform for processing and analyzing financial data. Forecasting: Principles and Practice This textbook is intended to provide a comprehensive introduction to forecasting methods and to present enough information about each method for readers to be able to use them sensibly. We don\u2019t attempt to give a thorough discussion of the theoretical details behind each method, although the references at the end of each chapter will fill in many of those details.","title":"Financial"},{"location":"R/R%20Books/#r-markdown-and-friends","text":"R Markdown: The Definitive Guide The document format \u201cR Markdown\u201d was first introduced in the knitr package (Xie 2015, 2018d) in early 2012. The idea was to embed code chunks (of R or other languages) in Markdown documents. In fact, knitr supported several authoring languages from the beginning in addition to Markdown, including LaTeX, HTML, AsciiDoc, reStructuredText, and Textile. Looking back over the five years, it seems to be fair to say that Markdown has become the most popular document format, which is what we expected. The simplicity of Markdown clearly stands out among these document formats. R Markdown Cookbook This book is designed to provide a range of examples on how to extend the functionality of your R Markdown documents. As a cookbook, this guide is recommended to new and intermediate R Markdown users who desire to enhance the efficiency of using R Markdown and also explore the power of R Markdown. Bookdown bookdown: Authoring Books and Technical Documents with R Markdown. This short book introduces an R package, bookdown, to change your workflow of writing books. It should be technically easy to write a book, visually pleasant to view the book, fun to interact with the book, convenient to navigate through the book, straightforward for readers to contribute or leave feedback to the book author(s), and more importantly, authors should not always be distracted by typesetting details. blogdown: Creating Websites with R Markdown We introduce an R package, blogdown, in this short book, to teach you how to create websites using R Markdown and Hugo. If you have experience with creating websites, you may naturally ask what the benefits of using R Markdown are, and how blogdown is different from existing popular website platforms, such as WordPress. Introduction to Research Methods This book is intended as a practical introduction to research methods in the social sciences. If you pursue research academically or professionally, it will probably not be the last book you need to read on the subject. This is intended as something of a gentle introduction with a focus on the applications of the information and examples.","title":"R-Markdown and Friends"},{"location":"R/R%20Books/#utility","text":"Happy Git and GitHub for the useR Happy Git provides opinionated instructions on how to: Install Git and get it working smoothly with GitHub, in the shell and in the RStudio IDE. Develop a few key workflows that cover your most common tasks. Integrate Git and GitHub into your daily work with R and R Markdown. What They Forgot to Teach You About R We focus on building holistic and project-oriented workflows that address the most common sources of friction in data analysis, outside of doing the statistical analysis itself . Data Science at the Command Line, 1e Discover why the command line is an agile, scalable, and extensible technology. Even if you\u2019re already comfortable processing data with, say, Python or R, you\u2019ll greatly improve your data science workflow by also leveraging the power of the command line. Github actions with R GitHub actions allow us to trigger automated steps after we launch GitHub interactions such as when we push, pull, submit a pull request, or write an issue. For example, there are actions that will automatically trigger: continuous integration (CI) messages in response to issues or pull requests rendering/compiling e.g. of rmarkdown, bookdown, blogdowns etc GitHub actions follow the steps designated in a yaml file, which we place in the .github/workflows folder of the repo. We can add these yaml files to our repo either by clicking on a series of steps on GitHub.com, or using wrapper functions provided by the usethis package, depending on which actions you wish to include. We describe both ways here. Geocomputation with R The book is designed for intermediate-to-advanced R users interested in geocomputation and R beginners who have prior experience with geographic data. If you are new to both R and geographic data, do not be discouraged: we provide links to further materials and describe the nature of spatial data from a beginner\u2019s perspective in Chapter 2 and in links provided below. Business Analytics with R The notes for this course were compiled from years of work in industry using R. These notes are intended to provide the non-programmer and programmer alike a hands-on approach to learning R for use in Business Analysis. The notes are ideal for the Accounting, Business, and Data Science majors desiring a better understanding of how the programming language R may be used to retrieve and organize data, perform analysis, create visualizations and automate business processes to reduce error and improve efficiency in the workplace. Misc books here","title":"Utility"},{"location":"R/R%20Development/","text":"R Development \u2691 Plumber Logging","title":"R Development"},{"location":"R/R%20Development/#r-development","text":"Plumber Logging","title":"R Development"},{"location":"R/R%20Miscellaneous%20Notes/","text":"R Miscellaneous Notes \u2691 ### Package Development Guidelines The script should be used to document all changes and additions to package code. In addition, git-flow branches should be used for all added features and bug fixes. The **development** branch serves as the \u201ctest\u201d / \u201cwork-in-progress\u201d branch while the **master** branch is the \u201cproduction\u201d branch and should only be updated when creating a new **release**. For example, say I wanted to add a new feature / function - the following steps would be used: Using *Git-Flow*, a new branch should be created off the current **development** branch with a name corresponding to the new feature. Once in the new branch, add functions via the **devhist.R** script via . Edit the new .R file and commit changes and push to feature branch. Add tests for new function and document additions via , and document this in the **devhist.R** script. Add example usage of the new feature via , and document this in the **devhist.R** script. Document, Test, Check, Build, and Install. Finally, merge feature branch with development branch using git-flow and if desired create a new release and merge with master branch updating the package version. Ideas \u2691 Functions \u2691 Workflow Functions \u2691 Project Configuration Version Control and Git Directory Structure Project Variables External Data Sources Caching Pipelines Databases and Query\u2019s GoogleDrive Files Metadata Codebooks, Data Dictionaries, Name Tables Documentation CI Testing Code Review Actuarial Functions \u2691 Compare to Prior Retentions Policy Periods Grouping by Occurrence Exposure Lagging Experience Mod Factors Industry Loss Costs Data Validation Checks Manual Adjustments Triangle Formation LDF Derivation LDF Interpolation Discount Factors Data Summaries Loss Costs Severity Frequency Credibiilty Allocation Projections Development Methods BF Methods Parrallelogram Methods Evaluation Dates / Year-Month Time Series Data Persistency NCCI Credibility Simulations and Confidence Intervals / Levels Benchmarking Contigency Tables Survival Analysis Utility Functions \u2691 Parsing Dates Excel Data Structures \u2691 Lossruns Triangles Exposures Industry Loss Costs RStudio Addins \u2691 Project Init Dependencies Styling Create Script with Header Git Setup Name Table Addin Excel Integration Addin(s) Remote / Raw Data Directories (Persistent Data Storage) Shiny App Init Find and Replace Data Manipulation (MiniUI with a Pivot Table) Markdown Addins Prefixer / @importFrom Clipboard Templates \u2691 Project Directory Package Directory Shiny App Directory Markdown HTML Template README Template Package DESCRIPTION template Excel Output Template Shiny Apps HTML Templates HTML Widget Templates PDF Acturial Report Template Latex Tables Shiny Apps \u2691 Modules Highcharter DT CSS, Bootstrap, Java, HTML Options \u2691 Global options .Rprofile .Renviron Git BB & GH Conflicts DT Knitr Pre-Commit Hooks Shiny Highcharter Data \u2691 CRM - People, Emails, Phones, Expertise, Clients, Budgets, etc. Historical Archived Data (Versioning) Git LFS Example Lossruns, Exposures, Industry Data Triangles, LDFs, Discount Factors Simulation Outputs Transactional Data Liberty, Sedgwick, etc extraction. Resources and Learning \u2691 https://rtask.thinkr.fr/when-development-starts-with-documentation/ https://emilyriederer.netlify.com/post/rmarkdown-driven-development/ https://r-pkgs.org/index.html https://github.com/ThinkR-open/golem https://thinkr-open.github.io/building-shiny-apps-workflow/ https://github.com/ThinkR-open/chameleon https://github.com/rorynolan/exampletestr https://github.com/ThinkR-open/attachment/blob/master/devstuff_history.R Links: Source:","title":"R Miscellaneous Notes"},{"location":"R/R%20Miscellaneous%20Notes/#r-miscellaneous-notes","text":"### Package Development Guidelines The script should be used to document all changes and additions to package code. In addition, git-flow branches should be used for all added features and bug fixes. The **development** branch serves as the \u201ctest\u201d / \u201cwork-in-progress\u201d branch while the **master** branch is the \u201cproduction\u201d branch and should only be updated when creating a new **release**. For example, say I wanted to add a new feature / function - the following steps would be used: Using *Git-Flow*, a new branch should be created off the current **development** branch with a name corresponding to the new feature. Once in the new branch, add functions via the **devhist.R** script via . Edit the new .R file and commit changes and push to feature branch. Add tests for new function and document additions via , and document this in the **devhist.R** script. Add example usage of the new feature via , and document this in the **devhist.R** script. Document, Test, Check, Build, and Install. Finally, merge feature branch with development branch using git-flow and if desired create a new release and merge with master branch updating the package version.","title":"R Miscellaneous Notes"},{"location":"R/R%20Miscellaneous%20Notes/#ideas","text":"","title":"Ideas"},{"location":"R/R%20Miscellaneous%20Notes/#functions","text":"","title":"Functions"},{"location":"R/R%20Miscellaneous%20Notes/#workflow-functions","text":"Project Configuration Version Control and Git Directory Structure Project Variables External Data Sources Caching Pipelines Databases and Query\u2019s GoogleDrive Files Metadata Codebooks, Data Dictionaries, Name Tables Documentation CI Testing Code Review","title":"Workflow Functions"},{"location":"R/R%20Miscellaneous%20Notes/#actuarial-functions","text":"Compare to Prior Retentions Policy Periods Grouping by Occurrence Exposure Lagging Experience Mod Factors Industry Loss Costs Data Validation Checks Manual Adjustments Triangle Formation LDF Derivation LDF Interpolation Discount Factors Data Summaries Loss Costs Severity Frequency Credibiilty Allocation Projections Development Methods BF Methods Parrallelogram Methods Evaluation Dates / Year-Month Time Series Data Persistency NCCI Credibility Simulations and Confidence Intervals / Levels Benchmarking Contigency Tables Survival Analysis","title":"Actuarial Functions"},{"location":"R/R%20Miscellaneous%20Notes/#utility-functions","text":"Parsing Dates Excel","title":"Utility Functions"},{"location":"R/R%20Miscellaneous%20Notes/#data-structures","text":"Lossruns Triangles Exposures Industry Loss Costs","title":"Data Structures"},{"location":"R/R%20Miscellaneous%20Notes/#rstudio-addins","text":"Project Init Dependencies Styling Create Script with Header Git Setup Name Table Addin Excel Integration Addin(s) Remote / Raw Data Directories (Persistent Data Storage) Shiny App Init Find and Replace Data Manipulation (MiniUI with a Pivot Table) Markdown Addins Prefixer / @importFrom Clipboard","title":"RStudio Addins"},{"location":"R/R%20Miscellaneous%20Notes/#templates","text":"Project Directory Package Directory Shiny App Directory Markdown HTML Template README Template Package DESCRIPTION template Excel Output Template Shiny Apps HTML Templates HTML Widget Templates PDF Acturial Report Template Latex Tables","title":"Templates"},{"location":"R/R%20Miscellaneous%20Notes/#shiny-apps","text":"Modules Highcharter DT CSS, Bootstrap, Java, HTML","title":"Shiny Apps"},{"location":"R/R%20Miscellaneous%20Notes/#options","text":"Global options .Rprofile .Renviron Git BB & GH Conflicts DT Knitr Pre-Commit Hooks Shiny Highcharter","title":"Options"},{"location":"R/R%20Miscellaneous%20Notes/#data","text":"CRM - People, Emails, Phones, Expertise, Clients, Budgets, etc. Historical Archived Data (Versioning) Git LFS Example Lossruns, Exposures, Industry Data Triangles, LDFs, Discount Factors Simulation Outputs Transactional Data Liberty, Sedgwick, etc extraction.","title":"Data"},{"location":"R/R%20Miscellaneous%20Notes/#resources-and-learning","text":"https://rtask.thinkr.fr/when-development-starts-with-documentation/ https://emilyriederer.netlify.com/post/rmarkdown-driven-development/ https://r-pkgs.org/index.html https://github.com/ThinkR-open/golem https://thinkr-open.github.io/building-shiny-apps-workflow/ https://github.com/ThinkR-open/chameleon https://github.com/rorynolan/exampletestr https://github.com/ThinkR-open/attachment/blob/master/devstuff_history.R Links: Source:","title":"Resources and Learning"},{"location":"R/R%20Shiny%20Packages/","text":"R Shiny Resources \u2691 Theming Generic Theming Dashboard Theming Mobile Theming Theme Customization UI Components Bootstrap File Input Special Input Loader Feedback / Alert / Notification Walkthrough / Tooltip / Help Clipboard Color Picker Editor Table Drawers Drag and Drop Text Image / Audio / Video PDF Icon Font Image Comparison Code Diff Calendar Notebooks Animation Effects i18n React Vue.js Advanced Interactivity Visualization General-Purpose Scatterplot Parallel Coordinates Time Series Tree and Hierarchical Data Network and Graph Data Diagrams Heatmap Maps and Spatial Data Sparkline Word Cloud Biological Data Chemical Data WebGL Augmented and Virtual Reality Backend Database API Frameworks URL Routing Authentication Job Scheduling Web APIs Integration Notification Integration Cloud Integration G Suite Integration Deploy Cloud Deploy Desktop Deploy Developer Tools Prototyping Modularization Debugging Testing Profiling Scaling Miscellaneous UI Customization Dependency Resolution Books Videos / Screencasts Theming \u2691 An awesome Shiny app often looks different from the default Bootstrap theme. Generic Theming \u2691 shinythemes - Bootswatch themes (Bootstrap 3) for Shiny. shiny.semantic - Semantic UI for Shiny. shinymaterial - Material Design for Shiny with Materialize.css. shinyUIkit - UIkit API for Shiny. fullPage - Single page styles for Shiny apps. shinybulma - Bulma.io for Shiny. shinyMetroUi - Metro 4 UI for Shiny. yonder - A reactive web framework built on Shiny with Bootstrap 4. Dashboard Theming \u2691 shinydashboard - Shiny dashboarding framework based on AdminLTE 2. shinydashboardPlus - Additional AdminLTE 2 components for shinydashboard. gentelellaShiny - Bootstrap 3 Gentelella theme for Shiny dashboards. semantic.dashboard - Semantic UI for Shiny dashboards. bs4Dash - Bootstrap 4 Shiny dashboards using AdminLTE 3. argonDash - Bootstrap 4 Argon template for Shiny dashboards. tablerDash - Tabler dashboard template for Shiny with Bootstrap 4. Mobile Theming \u2691 miniUI - Widgets and layouts for Shiny apps working on small screens. Designed for creating Shiny Gadgets. shinyMobile - Theming Shiny apps with Framework7, a full featured HTML framework for building iOS & Android apps. Theme Customization \u2691 bslib - Tools for theming Shiny and R Markdown from R via Bootstrap (3 or 4) Sass. fresh - Create fresh themes for use in shiny & shinydashboard applications and flexdashboard documents. Rnightly - An R wrapper of the JavaScript library Nightly. UI Components \u2691 Frontend UI components for special input/output types. Bootstrap \u2691 ShinyWidgets - Bootstrap 3 custom widgets for Shiny (switches, checkboxes, sweet alerts, slider text, knob inputs, select pickers, search bar, dropdown buttons). bsplus - Bootstrap 3 addons for Shiny and R Markdown (collapsible elements, accordion panels, accordion-sidebar sets, tooltips, popovers, modals, carousels). shinyBS - Bootstrap 3 components for Shiny (alerts, tooltips, popovers, modals, collapsible panels, button upgrades). slickR - Carousels for Shiny apps using slick.js. shinyLP - Bootstrap 3 landing pages for Shiny apps. shinypanels - Shiny layout with collapsible panels. spsComps - Additional Bootstrap 3 custom UI components (gallery, panels, buttons, animation and more) and additional Shiny server components (exception catch, validation, etc.). File Input \u2691 shinyFiles - A server-side file system viewer for Shiny. directoryInput - Shiny input widget for selecting directories. Special Input \u2691 shinyTime - A timeInput widget for Shiny. shinyDatetimePickers - Datetime pickers for Shiny shinyCleave - Customized text inputs (phone number, ZIP code, currency, credit card) based on Cleave.js. regexSelect - Enable regular expression searches within a Shiny selectize object. ShinyRatingInput - Star rating inputs for Shiny based on bootstrap-rating. algo - Implements the Algolia Places address search auto completion menu on shiny text inputs. shinyChakraSlider - Combined slider and number input for Shiny. shinyMultiActionButton - A multi-action button for Shiny. NestedMenu - Multi-level dropdown menu selection input. Loader \u2691 shinycssloaders - CSS loader animations for Shiny outputs. shinycustomloader - Custom css/html or gif/image loaders for Shiny outputs. shinybusy - Minimal busy indicator for Shiny apps. shinydisconnect - Show a nice message when a Shiny app disconnects or errors. waiter - Splash loading screens for Shiny. sever - Customize Shiny's disconnected screen. Feedback / Alert / Notification \u2691 shinyFeedback - Display user feedback next to Shiny inputs. shinyalert - Create pretty popup messages (modals) in Shiny apps. shinytoastr - Notifications for Shiny apps, via toastr. shinypop - Collection of notifications, confirm dialogs, and alerts for Shiny apps based on noty, notie, push.js, and notiflix. shinyvalidate - Add input validation capabilities to Shiny. Walkthrough / Tooltip / Help \u2691 rintrojs - Wrapper for Intro.js to create step-by-step introductions and clickable hints. tippy - Wrapper for Tippy.js to add tooltips to R Markdown documents or Shiny apps. cicerone - Create guided tours for Shiny apps using driver.js. shinyhelper - Add Markdown help files to Shiny app elements. scrollrevealR - Animate shiny elements when they scroll into view using the scrollrevealjs library. faq - Accordion-based FAQ component with expand/collapse control. flashCard - htmlwidget for creating flippable flash cards. Clipboard \u2691 rclipboard - Wrapper for clipboard.js to create copy-to-clipboard buttons for Shiny apps. Color Picker \u2691 colourpicker - A colour picker tool for Shiny. gradientPickerD3 - Interactive color gradient picker based on jquery-gradient-picker. gradientInput - Another approach at gradient colour picker, implemented using the colourpicker package. Editor \u2691 shinyAce - Ace code editor bindings for Shiny. shinyMonacoEditor - The Monaco Editor in Shiny. shinyMCE - TinyMCE WYSIWYG editor bindings for Shiny. sqlquery - htmlwidget for writing SQL queries with autocompletion for SQL keywords and table/field names. Table \u2691 DT - R interface to the DataTables library. reactable - Interactive data tables for R, based on the React Table library and made with reactR. reactablefmtr - Simplify the styling, formatting, and customization of tables made with reactable. kableExtra - Construct complex table with knitr::kable() and pipes. formattable - Table elements formatting and styling for R Markdown documents and Shiny apps. flextable - Create tables for reporting with format and layout control. gt - Generate information-rich, publication-quality tables. rhandsontable - Create Excel-like editable tables in Shiny apps. DTedit - Editable DataTables for Shiny apps. texPreview - Preview and save images of rendered snippets of LaTeX in RStudio viewer, R Markdown and Shiny. basictabler - Construct rich tables for output to HTML/Excel. pivottabler - Create pivot tables in R. pivta - R wrapper for WebDataRocks, an interactive pivot table component for data analysis. excelR - R interface to the jExcel.js library. RXSpreadsheet - R wrapper for the JavaScript canvas spreadsheet library x-spreadsheet. Drawers \u2691 pushbar - pushbar.js for Shiny. Create off-canvas sliding drawers for inputs, outputs, or any other content. Drag and Drop \u2691 shinyDND - Create drag and drop elements in Shiny. sortable - htmlwidget for SortableJS that enables drag-and-drop behavior and reorderable elements. dragulaR - R interface for the dragula JavaScript library for moving around elements in Shiny apps. dndselectr - Drag-and-drop Shiny select input. esquisse - Drag and drop inputs and visual builder for ggplot2. Text \u2691 marker - Highlight text in Shiny with markjs. Image / Audio / Video \u2691 shinysense - A series of shiny modules to help Shiny sense the world around it (draw, swipe cards, record images from a webcam, record audio, capture accelerometer data). pixels - htmlwidget and Shiny Gadget to render and draw pixels. fabricerin - Create HTML5 canvas in Shiny and R Markdown documents based on Fabric.js. heyshiny - Speech to text input. vembedr - Embed videos in R Markdown documents and Shiny apps. webcamR - htmlwidget wrapper around the react-webcam library. pianobar - Create histograms with audible features. drawer - A front-end only image editor for both Shiny and R Markdown. PDF \u2691 rpdf - Mozilla pdf.js htmlwidget for R. Icon Font \u2691 fontawesome - Insert FontAwesome icons into R Markdown documents and Shiny apps. icongram - Interface to Icongram, easily fetch svg icons with a single function. Image Comparison \u2691 vdiffr - Visual regression testing and graphical diffing, with toggle, slide, and diff widgets for comparing two images. Code Diff \u2691 diffr - Create code diff widgets based on codediff.js. diffRgit - Create an HTML git diff widget using the diff2html library. jsondiff - R interface to jsondiffpatch for comparing R objects as JSONs. Calendar \u2691 tuicalendr - htmlwidget to create interactive calendars with JavaScript library tui-calendar. Notebooks \u2691 robservable - Observable notebooks as R htmlwidgets. Animation Effects \u2691 typed - R htmlwidget for animated typing effect with typed.js. countup - R htmlwidget that animates a numerical value by counting to it with CountUp.js. textillate - CSS3 text animations with textillate.js. shinyglide - Create carousel-like or assistant-like (wizard) UI components with Glide.js. d3rain - htmlwidget bringing D3 drip to R. hover - Add animations to Shiny button elements using Hover.css. bubblyr - Add animated bubbles to Shiny and R Markdown backgrounds. i18n \u2691 shiny.i18n - Easy internationalization of Shiny apps. shi18ny - Tools for shiny apps internationalization. React \u2691 reactR - Use React in R with htmlwidget constructor templates and local JavaScript dependencies. shinyReactWidgets - React widgets for Shiny apps. Vue.js \u2691 vuer - Use Vue components and build Vue apps in R. vueR - Use Vue.js in R with htmlwidget constructor templates and local JavaScript dependencies. Advanced Interactivity \u2691 htmlwidgets - A framework for creating R bindings to JavaScript libraries. crosstalk - Inter-widget interactivity (for example, linked brushing and filtering) for htmlwidgets. shinyjs - Perform common JavaScript operations in Shiny apps. shinyjqui - Add jQuery UI interactions and effects (e.g. draggable, resizable, sortable elements) to Shiny apps. shiny.collections - Google Docs-like live collaboration in Shiny with RethinkDB. shinyCanvas - Create and customize an interactive canvas using the D3 JavaScript library and the htmlwidgets package. shinymeta - Record and expose Shiny app logic using metaprogramming. shinyscroll - Scroll to an element in Shiny. pagemapR - Create a mini map for Shiny apps and R Markdown documents. keys - Assign and listen to keyboard shortcuts in Shiny using the Mousetrap Javascript library. Visualization \u2691 Frontend components for interactive visualization of specific data types. General-Purpose \u2691 plotly - Interactive web graphics via plotly.js. Has special support for linking/highlighting/filtering views. highcharter - R wrapper for the highcharts JavaScript charting library. rbokeh - R interface for Bokeh. echarts4r - Interactive graphs with Echarts v4. r2d3 - R interface to D3 visualizations. vegalite - R ggplot2 bindings for Vega-Lite. vegawidget - htmlwidget renderer for Vega and Vega-Lite. ggiraph - htmlwidget that makes ggplot2 graphics interactive. Select graphical elements, add tooltips, animations, and JavaScript actions to the graphics. rfrappe - htmlwidget for the Frappe Charts JavaScript library. tuichartr - htmlwidget for tui-chart. billboarder - htmlwidget for billboard.js. apexcharter - htmlwidget for ApexCharts.js. taucharts - htmlwidget for Taucharts. googleVis - R interface to Google Charts. rroughviz - R warpper for roughViz.js, a JavaScript library for creating sketchy/hand-drawn styled charts. rAmCharts4 - R interface to amCharts 4. Scatterplot \u2691 scatterD3 - R scatterplot htmlwidget based on D3.js. pairsD3 - D3 scatterplot matrices. rthreejs - Interactive 3D scatterplots, networks, and globes based on three.js. graph3d - R wrapper of the JavaScript library vis-graph3d. hpackedbubble - Split packed bubble charts with highcharts. Parallel Coordinates \u2691 parcoords - htmlwidget for D3 parallel coordinates chart. Time Series \u2691 dygraphs - R interface to the dygraphs JavaScript charting library. metricsgraphics - An htmlwidget interface to the MetricsGraphics.js D3-based charting library. timevis - Interactive timeline visualizations based on vis.js. timelineschart - R interface to timelines-chart. streamgraph - htmlwidget for creating streamgraph visualizations in R. eventdropR - htmlwidget for EventDrops, time based and event series interactive visualization using D3. Tree and Hierarchical Data \u2691 D3partitionR - D3-based interactive visualization of nested and hierarchical data (sunburst, treemap, circle treemap, icicle, and partition chart) for Shiny. d3Tree - D3-based collapsible trees for Shiny. collapsibleTree - D3-based interactive collapsible tree diagrams. jsTree - R htmlwidget for inspecting heirachal structures with the jQuery jsTree plugin. shinyTree - jsTree bindings for creating interactive trees in Shiny. jsTreeR - A wrapper of the JavaScript library jsTree. shinyCheckboxTree - Checkbox tree widget for Shiny. Wrapper of the JavaScript library react-checkbox-tree. listviewer - View and modify lists interactively, based on jsoneditor and react-json-view. trelliscopejs - Create interactive Trelliscope displays based on trelliscopejs-lib. gwordtree - Render a word tree with Google Charts. Rmarkmap - Create interactive mind maps with the markmap JavaScript library. Network and Graph Data \u2691 networkD3 - Create D3 network, tree, dendrogram, and Sankey diagram from R. visNetwork - Network visualization using vis.js. sigmajs - Interface to the sigma.js graph visualization library, including animations, plugins, and Shiny widgets. sigmaNet - Render igraph networks using sigma.js. chorddiag - R interface to D3 interactive chord diagrams. chordViz - Create interactive chord diagrams in R. edgebundleR - Circular layout plots with bundled edges based on D3. hiveD3 - D3-based hive plots. Tutorial for recreating the package. arcDiagramR - Create arc diagrams with htmlwidgets. grapher - An R integration of ngraph to create 3D and 2D interactive graphs. Diagrams \u2691 DiagrammeR - Diagram, graph, and network visualization based on D3.js, viz.js, and mermaid.js. nomnoml - R interface to nomnoml, a tool for drawing sassy UML diagrams based on syntax with customizable styling. bpmn - R interface to the bpmn-js library. Heatmap \u2691 d3heatmap - D3-based interactive heatmaps (highlight rows/columns, zoom in/out, clustering, dendrograms). heatmaply - Interactive heatmaps using plotly. rChartsCalmap - An htmlwidgets binding for calendar heatmaps using D3. calheatmapR - R interface for the cal-heatmap JavaScript charting library to create calendar heatmaps. nivocal - htmlwidget for drawing calendar heatmaps based on nivo. nivowaffle - htmlwidget for drawing waffle diagrams based on nivo. supercaliheatmapwidget - Supercalifragilistic HTML calendar heatmaps. Maps and Spatial Data \u2691 leaflet - R interface to the Leaflet JavaScript library to create interactive maps. leaflet.extras - Extra functionality for the leaflet package. leaflet.minicharts - Add and modify small charts on the interactive map created with the leaflet package. leaflet.esri - ESRI bindings for the leaflet package. leaflet.opacity - Opacity controls for Leaflet maps. leaftime - Leaflet-timeline plugin for Leaflet to show changing geospatial data over time. leafletCN - China and geojson choropleth maps for Leaflet. leafletGeocoderRshiny - Leaflet + Pelias geocoding for Shiny. leafdown - Provides drilldown functionality for leaflet choropleths. mapdeck - Interactive maps using Mapbox GL and Deck.gl. deckgl - R Interface to Deck.gl. r2deck - R interface to Deck.gl and Mapbox GL visualizations. h3r - Uber's H3 geographical indexing library bindings for R. googleway - Access Google Maps API to retrieve data and draw maps. mapview - Interactive viewing of spatial data. mapedit - Interactive editing of spatial data. tmap - Create thematic maps, such as choropleths and bubble maps. datamaps - Create interactive choropleth maps with the JavaScript library Datamaps, add arcs and bubbles, change choropleth values, and change labels. topogram - Cartogram htmlwidget for visualizing geographical data by distorting a TopoJSON topology using cartogram-chart. rsquaire - R interface for squaire.js, a JavaScript library for making responsive equal-area square maps using D3. hchinamap - Mapping China and its provinces with highcharts. mapbrew - Mapping China with amCharts. planetary - htmlwidget for the planetary.js library for creating interactive globes. gior - htmlwidget for gio.js for declarative 3D globe data visualization. quickglobe - View country data via a 3D D3 globe. Sparkline \u2691 sparkline - jQuery Sparkline (tiny inline charts) HTML Widget for R. Use sparklines in DT . reactrend - Simple, elegant spark lines and trend graphs based on react-trend. peity - Peity htmlwidget for R. dataui - Interactive visualizations of data-ui based on vx. Word Cloud \u2691 wordcloud2 - Word cloud visualization based on wordcloud2.js. hwordcloud - Render word clouds with highcharts. d3wordcloud - htmlwidget for D3.js word cloud layout. Biological Data \u2691 igvR - R package providing interactive connections to igv.js running in a web browser. igvShiny - htmlwidget for igv.js, a JavaScript library for embeddable genomic visualization. cyjShiny - htmlwidget for Cytoscape.js, a JavaScript library for graph/network visualization. The API is based on RCyjs (and thus RCy3). nglShiny - NGL Viewer as an htmlwidget for molecular visualization. msaR - BioJS-based MSA (multiple sequence alignment) viewer. TnT - Track-based visulizations based on the TnT JavaScript libraries. Useful for displaying genomic features as a simple genome browser. mutsneedle - Interactive mutation lollipop diagrams. g3viz - D3-based interactive lollipop plots. BioCircos.R - Interactive circular visualization of genomic data using htmlwidgets and BioCircos.js. chromoMap - Interactive visualization and mapping of human chromosomes. ideogRam - htmlwidget for chromosome visualization with ideogram.js. flowDashboard - Shiny Modules for visualizing flow cytometry data. qtlcharts - Interactive graphics for QTL experiments. phylocanvas - Interactive phylogenetic trees using the Phylocanvas JavaScript library. phylowidget - Interactive phylogenetic trees based on phylotree.js. JBrowseR - R interface to the JBrowse 2 linear genome view. Chemical Data \u2691 r3dmol - Visualizing molecular data in 3D, based on 3Dmol.js. chemdoodle - htmlwidget for visualizing and drawing molecules. WebGL \u2691 rgl - Render WebGL scenes created with the rgl package ( vignette ). rayshader - Create and visualize hillshaded maps from elevation matrices. rayrender - Build and raytrace 3D scenes. rayfocus - Render depth of field for images. Augmented and Virtual Reality \u2691 shinyaframe - WebVR data visualizations with Shiny and Mozilla A-Frame. arframer - Augmented Reality in R based on AR.js. Backend \u2691 Backend components and service integrations for Shiny apps. Database \u2691 db.rstudio.com - Packages and tutorials for connecting R and Shiny apps to databases. pool - Database connection pooling in R. elastic - R client for the Elasticsearch HTTP API. sergeant - Transform and query data with Apache Drill. API Frameworks \u2691 RestRserve - R web API framework for building high-performance and robust microservices and app backends. plumber - Create web APIs by decorating R code with special comments. opencpu - A system for embedded scientific computing and reproducible research with R. URL Routing \u2691 shiny.router - Minimalistic URL router for Shiny apps. shinyURL - Save and restore the state of a Shiny app by encoding the values of user inputs and active tab panels in the app's URL query string. Authentication \u2691 shinymanager - Simple and secure authentification mechanism for single Shiny apps. googleAuthR - Shiny compatible Google API client for authentication with OAuth2. auth0 - Authentication in Shiny apps using Auth0. shinyauthr - Server-side authentication using shiny modules. firebase - Authenticate Shiny users with Google Firebase. otp - One-Time Password generation and verification. backendlessr - R wrapper for Backendless API to manage users. Job Scheduling \u2691 cronR - R package for managing cron jobs. Web APIs Integration \u2691 glouton - Handle browser cookies in shiny, built on top of js-cookie. geoloc - Use the Geolocation API to get the location of the user (with user's permission). shinyStore - Use the Web Storage API to store persistent, synchronized data in the user's browser. Notification Integration \u2691 slackr - Send messages, images, R objects, and files to Slack channels/users. sendgridr - Send emails with SendGrid mail API (v3). twilio - R interface to the Twilio API. blastula - Easily send HTML email messages from R. mjml - Create responsive emails with R and MJML. mailtoR - Creates a friendly user interface for emails sending in Shiny. Cloud Integration \u2691 cloudyr - R packages for integrating with AWS, Azure, and Google Cloud. G Suite Integration \u2691 googlesheets4 - R interface to Google Sheets via the Sheets API v4. googlesheets - R interface to Google Spreadsheets API (no longer under active development). googledrive - R API client for Google Drive. gmailr - Access the Gmail RESTful API from R. Deploy \u2691 Deploy Shiny apps to the cloud, hosted infrastructure, or desktop. Cloud Deploy \u2691 rsconnect - Deploy Shiny apps to shinyapps.io, or RStudio Connect. Desktop Deploy \u2691 RInno - Deploy Shiny apps to Windows by interfacing Inno Setup and Electron. electricShine - Create distributable Shiny Electron apps. photon - RStudio Add-in to build Shiny apps utilizing the Electron framework. DesktopDeployR - A framework for deploying self-contained R-based applications to the desktop. Shiny Meets Electron - Talk at useR! 2018 on turning Shiny app into standalone desktop apps. Talk video . r-shiny-electron - Template for R Shiny and Electron integration. nativefier - Create Electron wrappers for any websites (including remotely deployed Shiny apps). Developer Tools \u2691 Debug, test, and optimize Shiny apps. Prototyping \u2691 golem - Opinionated framework for building production-grade Shiny apps. shinipsum - Lorem-Ipsum-like helpers for fast Shiny prototyping. fakir - Create fake data in R for tutorials. shinysnippets - A series of Shiny related RStudio snippets. Modularization \u2691 tidymodules - An object-oriented approach to Shiny modules. supreme - Structure Shiny applications developed with modules. Debugging \u2691 shinyreactlog - Visual debugger for Shiny reactivity. reactlog - Easier debugging with the Shiny reactive log. Testing \u2691 shinytest - Automated testing for Shiny apps. shinyloadtest - Load testing for Shiny apps. reactor - Unit testing for Shiny reactivity. Profiling \u2691 profvis - Interactive visualizations for profiling R code. Profiling Shiny apps . Scaling \u2691 promises - Promise-based asynchronous programming for R. Using promises with Shiny . shinyParallel - Run Shiny applications in a multi-session mode. Miscellaneous \u2691 Not necessarily an R package, but it helps. UI Customization \u2691 Bootstrap Live Customizer - Customize Bootswatch themes (Bootstrap 3) to create your own Bootstrap themes. google-webfonts-helper - A hassle-free way to self-hosted Google Fonts, useful for air-gapped environments. Dependency Resolution \u2691 packrat - Parse R package dependencies of Shiny apps with packrat::appDependencies . sysreqsdb - SystemRequirements mappings for R packages. shinyapps-package-dependencies - A collection of bash scripts that install system dependencies for R packages. Books \u2691 Mastering Shiny: Build Interactive Apps, Reports, and Dashboards Powered by R Engineering Production-Grade Shiny Apps JavaScript for R Outstanding User Interfaces with Shiny Videos / Screencasts \u2691 Shiny Developer Series - Interviews with practitioners & developers of Shiny and the broader ecosystem of Shiny packages, plus occasional live streams of Shiny app development in action. Links: Source:","title":"R Shiny Resources"},{"location":"R/R%20Shiny%20Packages/#r-shiny-resources","text":"Theming Generic Theming Dashboard Theming Mobile Theming Theme Customization UI Components Bootstrap File Input Special Input Loader Feedback / Alert / Notification Walkthrough / Tooltip / Help Clipboard Color Picker Editor Table Drawers Drag and Drop Text Image / Audio / Video PDF Icon Font Image Comparison Code Diff Calendar Notebooks Animation Effects i18n React Vue.js Advanced Interactivity Visualization General-Purpose Scatterplot Parallel Coordinates Time Series Tree and Hierarchical Data Network and Graph Data Diagrams Heatmap Maps and Spatial Data Sparkline Word Cloud Biological Data Chemical Data WebGL Augmented and Virtual Reality Backend Database API Frameworks URL Routing Authentication Job Scheduling Web APIs Integration Notification Integration Cloud Integration G Suite Integration Deploy Cloud Deploy Desktop Deploy Developer Tools Prototyping Modularization Debugging Testing Profiling Scaling Miscellaneous UI Customization Dependency Resolution Books Videos / Screencasts","title":"R Shiny Resources"},{"location":"R/R%20Shiny%20Packages/#theming","text":"An awesome Shiny app often looks different from the default Bootstrap theme.","title":"Theming"},{"location":"R/R%20Shiny%20Packages/#generic-theming","text":"shinythemes - Bootswatch themes (Bootstrap 3) for Shiny. shiny.semantic - Semantic UI for Shiny. shinymaterial - Material Design for Shiny with Materialize.css. shinyUIkit - UIkit API for Shiny. fullPage - Single page styles for Shiny apps. shinybulma - Bulma.io for Shiny. shinyMetroUi - Metro 4 UI for Shiny. yonder - A reactive web framework built on Shiny with Bootstrap 4.","title":"Generic Theming"},{"location":"R/R%20Shiny%20Packages/#dashboard-theming","text":"shinydashboard - Shiny dashboarding framework based on AdminLTE 2. shinydashboardPlus - Additional AdminLTE 2 components for shinydashboard. gentelellaShiny - Bootstrap 3 Gentelella theme for Shiny dashboards. semantic.dashboard - Semantic UI for Shiny dashboards. bs4Dash - Bootstrap 4 Shiny dashboards using AdminLTE 3. argonDash - Bootstrap 4 Argon template for Shiny dashboards. tablerDash - Tabler dashboard template for Shiny with Bootstrap 4.","title":"Dashboard Theming"},{"location":"R/R%20Shiny%20Packages/#mobile-theming","text":"miniUI - Widgets and layouts for Shiny apps working on small screens. Designed for creating Shiny Gadgets. shinyMobile - Theming Shiny apps with Framework7, a full featured HTML framework for building iOS & Android apps.","title":"Mobile Theming"},{"location":"R/R%20Shiny%20Packages/#theme-customization","text":"bslib - Tools for theming Shiny and R Markdown from R via Bootstrap (3 or 4) Sass. fresh - Create fresh themes for use in shiny & shinydashboard applications and flexdashboard documents. Rnightly - An R wrapper of the JavaScript library Nightly.","title":"Theme Customization"},{"location":"R/R%20Shiny%20Packages/#ui-components","text":"Frontend UI components for special input/output types.","title":"UI Components"},{"location":"R/R%20Shiny%20Packages/#bootstrap","text":"ShinyWidgets - Bootstrap 3 custom widgets for Shiny (switches, checkboxes, sweet alerts, slider text, knob inputs, select pickers, search bar, dropdown buttons). bsplus - Bootstrap 3 addons for Shiny and R Markdown (collapsible elements, accordion panels, accordion-sidebar sets, tooltips, popovers, modals, carousels). shinyBS - Bootstrap 3 components for Shiny (alerts, tooltips, popovers, modals, collapsible panels, button upgrades). slickR - Carousels for Shiny apps using slick.js. shinyLP - Bootstrap 3 landing pages for Shiny apps. shinypanels - Shiny layout with collapsible panels. spsComps - Additional Bootstrap 3 custom UI components (gallery, panels, buttons, animation and more) and additional Shiny server components (exception catch, validation, etc.).","title":"Bootstrap"},{"location":"R/R%20Shiny%20Packages/#file-input","text":"shinyFiles - A server-side file system viewer for Shiny. directoryInput - Shiny input widget for selecting directories.","title":"File Input"},{"location":"R/R%20Shiny%20Packages/#special-input","text":"shinyTime - A timeInput widget for Shiny. shinyDatetimePickers - Datetime pickers for Shiny shinyCleave - Customized text inputs (phone number, ZIP code, currency, credit card) based on Cleave.js. regexSelect - Enable regular expression searches within a Shiny selectize object. ShinyRatingInput - Star rating inputs for Shiny based on bootstrap-rating. algo - Implements the Algolia Places address search auto completion menu on shiny text inputs. shinyChakraSlider - Combined slider and number input for Shiny. shinyMultiActionButton - A multi-action button for Shiny. NestedMenu - Multi-level dropdown menu selection input.","title":"Special Input"},{"location":"R/R%20Shiny%20Packages/#loader","text":"shinycssloaders - CSS loader animations for Shiny outputs. shinycustomloader - Custom css/html or gif/image loaders for Shiny outputs. shinybusy - Minimal busy indicator for Shiny apps. shinydisconnect - Show a nice message when a Shiny app disconnects or errors. waiter - Splash loading screens for Shiny. sever - Customize Shiny's disconnected screen.","title":"Loader"},{"location":"R/R%20Shiny%20Packages/#feedback-alert-notification","text":"shinyFeedback - Display user feedback next to Shiny inputs. shinyalert - Create pretty popup messages (modals) in Shiny apps. shinytoastr - Notifications for Shiny apps, via toastr. shinypop - Collection of notifications, confirm dialogs, and alerts for Shiny apps based on noty, notie, push.js, and notiflix. shinyvalidate - Add input validation capabilities to Shiny.","title":"Feedback / Alert / Notification"},{"location":"R/R%20Shiny%20Packages/#walkthrough-tooltip-help","text":"rintrojs - Wrapper for Intro.js to create step-by-step introductions and clickable hints. tippy - Wrapper for Tippy.js to add tooltips to R Markdown documents or Shiny apps. cicerone - Create guided tours for Shiny apps using driver.js. shinyhelper - Add Markdown help files to Shiny app elements. scrollrevealR - Animate shiny elements when they scroll into view using the scrollrevealjs library. faq - Accordion-based FAQ component with expand/collapse control. flashCard - htmlwidget for creating flippable flash cards.","title":"Walkthrough / Tooltip / Help"},{"location":"R/R%20Shiny%20Packages/#clipboard","text":"rclipboard - Wrapper for clipboard.js to create copy-to-clipboard buttons for Shiny apps.","title":"Clipboard"},{"location":"R/R%20Shiny%20Packages/#color-picker","text":"colourpicker - A colour picker tool for Shiny. gradientPickerD3 - Interactive color gradient picker based on jquery-gradient-picker. gradientInput - Another approach at gradient colour picker, implemented using the colourpicker package.","title":"Color Picker"},{"location":"R/R%20Shiny%20Packages/#editor","text":"shinyAce - Ace code editor bindings for Shiny. shinyMonacoEditor - The Monaco Editor in Shiny. shinyMCE - TinyMCE WYSIWYG editor bindings for Shiny. sqlquery - htmlwidget for writing SQL queries with autocompletion for SQL keywords and table/field names.","title":"Editor"},{"location":"R/R%20Shiny%20Packages/#table","text":"DT - R interface to the DataTables library. reactable - Interactive data tables for R, based on the React Table library and made with reactR. reactablefmtr - Simplify the styling, formatting, and customization of tables made with reactable. kableExtra - Construct complex table with knitr::kable() and pipes. formattable - Table elements formatting and styling for R Markdown documents and Shiny apps. flextable - Create tables for reporting with format and layout control. gt - Generate information-rich, publication-quality tables. rhandsontable - Create Excel-like editable tables in Shiny apps. DTedit - Editable DataTables for Shiny apps. texPreview - Preview and save images of rendered snippets of LaTeX in RStudio viewer, R Markdown and Shiny. basictabler - Construct rich tables for output to HTML/Excel. pivottabler - Create pivot tables in R. pivta - R wrapper for WebDataRocks, an interactive pivot table component for data analysis. excelR - R interface to the jExcel.js library. RXSpreadsheet - R wrapper for the JavaScript canvas spreadsheet library x-spreadsheet.","title":"Table"},{"location":"R/R%20Shiny%20Packages/#drawers","text":"pushbar - pushbar.js for Shiny. Create off-canvas sliding drawers for inputs, outputs, or any other content.","title":"Drawers"},{"location":"R/R%20Shiny%20Packages/#drag-and-drop","text":"shinyDND - Create drag and drop elements in Shiny. sortable - htmlwidget for SortableJS that enables drag-and-drop behavior and reorderable elements. dragulaR - R interface for the dragula JavaScript library for moving around elements in Shiny apps. dndselectr - Drag-and-drop Shiny select input. esquisse - Drag and drop inputs and visual builder for ggplot2.","title":"Drag and Drop"},{"location":"R/R%20Shiny%20Packages/#text","text":"marker - Highlight text in Shiny with markjs.","title":"Text"},{"location":"R/R%20Shiny%20Packages/#image-audio-video","text":"shinysense - A series of shiny modules to help Shiny sense the world around it (draw, swipe cards, record images from a webcam, record audio, capture accelerometer data). pixels - htmlwidget and Shiny Gadget to render and draw pixels. fabricerin - Create HTML5 canvas in Shiny and R Markdown documents based on Fabric.js. heyshiny - Speech to text input. vembedr - Embed videos in R Markdown documents and Shiny apps. webcamR - htmlwidget wrapper around the react-webcam library. pianobar - Create histograms with audible features. drawer - A front-end only image editor for both Shiny and R Markdown.","title":"Image / Audio / Video"},{"location":"R/R%20Shiny%20Packages/#pdf","text":"rpdf - Mozilla pdf.js htmlwidget for R.","title":"PDF"},{"location":"R/R%20Shiny%20Packages/#icon-font","text":"fontawesome - Insert FontAwesome icons into R Markdown documents and Shiny apps. icongram - Interface to Icongram, easily fetch svg icons with a single function.","title":"Icon Font"},{"location":"R/R%20Shiny%20Packages/#image-comparison","text":"vdiffr - Visual regression testing and graphical diffing, with toggle, slide, and diff widgets for comparing two images.","title":"Image Comparison"},{"location":"R/R%20Shiny%20Packages/#code-diff","text":"diffr - Create code diff widgets based on codediff.js. diffRgit - Create an HTML git diff widget using the diff2html library. jsondiff - R interface to jsondiffpatch for comparing R objects as JSONs.","title":"Code Diff"},{"location":"R/R%20Shiny%20Packages/#calendar","text":"tuicalendr - htmlwidget to create interactive calendars with JavaScript library tui-calendar.","title":"Calendar"},{"location":"R/R%20Shiny%20Packages/#notebooks","text":"robservable - Observable notebooks as R htmlwidgets.","title":"Notebooks"},{"location":"R/R%20Shiny%20Packages/#animation-effects","text":"typed - R htmlwidget for animated typing effect with typed.js. countup - R htmlwidget that animates a numerical value by counting to it with CountUp.js. textillate - CSS3 text animations with textillate.js. shinyglide - Create carousel-like or assistant-like (wizard) UI components with Glide.js. d3rain - htmlwidget bringing D3 drip to R. hover - Add animations to Shiny button elements using Hover.css. bubblyr - Add animated bubbles to Shiny and R Markdown backgrounds.","title":"Animation Effects"},{"location":"R/R%20Shiny%20Packages/#i18n","text":"shiny.i18n - Easy internationalization of Shiny apps. shi18ny - Tools for shiny apps internationalization.","title":"i18n"},{"location":"R/R%20Shiny%20Packages/#react","text":"reactR - Use React in R with htmlwidget constructor templates and local JavaScript dependencies. shinyReactWidgets - React widgets for Shiny apps.","title":"React"},{"location":"R/R%20Shiny%20Packages/#vuejs","text":"vuer - Use Vue components and build Vue apps in R. vueR - Use Vue.js in R with htmlwidget constructor templates and local JavaScript dependencies.","title":"Vue.js"},{"location":"R/R%20Shiny%20Packages/#advanced-interactivity","text":"htmlwidgets - A framework for creating R bindings to JavaScript libraries. crosstalk - Inter-widget interactivity (for example, linked brushing and filtering) for htmlwidgets. shinyjs - Perform common JavaScript operations in Shiny apps. shinyjqui - Add jQuery UI interactions and effects (e.g. draggable, resizable, sortable elements) to Shiny apps. shiny.collections - Google Docs-like live collaboration in Shiny with RethinkDB. shinyCanvas - Create and customize an interactive canvas using the D3 JavaScript library and the htmlwidgets package. shinymeta - Record and expose Shiny app logic using metaprogramming. shinyscroll - Scroll to an element in Shiny. pagemapR - Create a mini map for Shiny apps and R Markdown documents. keys - Assign and listen to keyboard shortcuts in Shiny using the Mousetrap Javascript library.","title":"Advanced Interactivity"},{"location":"R/R%20Shiny%20Packages/#visualization","text":"Frontend components for interactive visualization of specific data types.","title":"Visualization"},{"location":"R/R%20Shiny%20Packages/#general-purpose","text":"plotly - Interactive web graphics via plotly.js. Has special support for linking/highlighting/filtering views. highcharter - R wrapper for the highcharts JavaScript charting library. rbokeh - R interface for Bokeh. echarts4r - Interactive graphs with Echarts v4. r2d3 - R interface to D3 visualizations. vegalite - R ggplot2 bindings for Vega-Lite. vegawidget - htmlwidget renderer for Vega and Vega-Lite. ggiraph - htmlwidget that makes ggplot2 graphics interactive. Select graphical elements, add tooltips, animations, and JavaScript actions to the graphics. rfrappe - htmlwidget for the Frappe Charts JavaScript library. tuichartr - htmlwidget for tui-chart. billboarder - htmlwidget for billboard.js. apexcharter - htmlwidget for ApexCharts.js. taucharts - htmlwidget for Taucharts. googleVis - R interface to Google Charts. rroughviz - R warpper for roughViz.js, a JavaScript library for creating sketchy/hand-drawn styled charts. rAmCharts4 - R interface to amCharts 4.","title":"General-Purpose"},{"location":"R/R%20Shiny%20Packages/#scatterplot","text":"scatterD3 - R scatterplot htmlwidget based on D3.js. pairsD3 - D3 scatterplot matrices. rthreejs - Interactive 3D scatterplots, networks, and globes based on three.js. graph3d - R wrapper of the JavaScript library vis-graph3d. hpackedbubble - Split packed bubble charts with highcharts.","title":"Scatterplot"},{"location":"R/R%20Shiny%20Packages/#parallel-coordinates","text":"parcoords - htmlwidget for D3 parallel coordinates chart.","title":"Parallel Coordinates"},{"location":"R/R%20Shiny%20Packages/#time-series","text":"dygraphs - R interface to the dygraphs JavaScript charting library. metricsgraphics - An htmlwidget interface to the MetricsGraphics.js D3-based charting library. timevis - Interactive timeline visualizations based on vis.js. timelineschart - R interface to timelines-chart. streamgraph - htmlwidget for creating streamgraph visualizations in R. eventdropR - htmlwidget for EventDrops, time based and event series interactive visualization using D3.","title":"Time Series"},{"location":"R/R%20Shiny%20Packages/#tree-and-hierarchical-data","text":"D3partitionR - D3-based interactive visualization of nested and hierarchical data (sunburst, treemap, circle treemap, icicle, and partition chart) for Shiny. d3Tree - D3-based collapsible trees for Shiny. collapsibleTree - D3-based interactive collapsible tree diagrams. jsTree - R htmlwidget for inspecting heirachal structures with the jQuery jsTree plugin. shinyTree - jsTree bindings for creating interactive trees in Shiny. jsTreeR - A wrapper of the JavaScript library jsTree. shinyCheckboxTree - Checkbox tree widget for Shiny. Wrapper of the JavaScript library react-checkbox-tree. listviewer - View and modify lists interactively, based on jsoneditor and react-json-view. trelliscopejs - Create interactive Trelliscope displays based on trelliscopejs-lib. gwordtree - Render a word tree with Google Charts. Rmarkmap - Create interactive mind maps with the markmap JavaScript library.","title":"Tree and Hierarchical Data"},{"location":"R/R%20Shiny%20Packages/#network-and-graph-data","text":"networkD3 - Create D3 network, tree, dendrogram, and Sankey diagram from R. visNetwork - Network visualization using vis.js. sigmajs - Interface to the sigma.js graph visualization library, including animations, plugins, and Shiny widgets. sigmaNet - Render igraph networks using sigma.js. chorddiag - R interface to D3 interactive chord diagrams. chordViz - Create interactive chord diagrams in R. edgebundleR - Circular layout plots with bundled edges based on D3. hiveD3 - D3-based hive plots. Tutorial for recreating the package. arcDiagramR - Create arc diagrams with htmlwidgets. grapher - An R integration of ngraph to create 3D and 2D interactive graphs.","title":"Network and Graph Data"},{"location":"R/R%20Shiny%20Packages/#diagrams","text":"DiagrammeR - Diagram, graph, and network visualization based on D3.js, viz.js, and mermaid.js. nomnoml - R interface to nomnoml, a tool for drawing sassy UML diagrams based on syntax with customizable styling. bpmn - R interface to the bpmn-js library.","title":"Diagrams"},{"location":"R/R%20Shiny%20Packages/#heatmap","text":"d3heatmap - D3-based interactive heatmaps (highlight rows/columns, zoom in/out, clustering, dendrograms). heatmaply - Interactive heatmaps using plotly. rChartsCalmap - An htmlwidgets binding for calendar heatmaps using D3. calheatmapR - R interface for the cal-heatmap JavaScript charting library to create calendar heatmaps. nivocal - htmlwidget for drawing calendar heatmaps based on nivo. nivowaffle - htmlwidget for drawing waffle diagrams based on nivo. supercaliheatmapwidget - Supercalifragilistic HTML calendar heatmaps.","title":"Heatmap"},{"location":"R/R%20Shiny%20Packages/#maps-and-spatial-data","text":"leaflet - R interface to the Leaflet JavaScript library to create interactive maps. leaflet.extras - Extra functionality for the leaflet package. leaflet.minicharts - Add and modify small charts on the interactive map created with the leaflet package. leaflet.esri - ESRI bindings for the leaflet package. leaflet.opacity - Opacity controls for Leaflet maps. leaftime - Leaflet-timeline plugin for Leaflet to show changing geospatial data over time. leafletCN - China and geojson choropleth maps for Leaflet. leafletGeocoderRshiny - Leaflet + Pelias geocoding for Shiny. leafdown - Provides drilldown functionality for leaflet choropleths. mapdeck - Interactive maps using Mapbox GL and Deck.gl. deckgl - R Interface to Deck.gl. r2deck - R interface to Deck.gl and Mapbox GL visualizations. h3r - Uber's H3 geographical indexing library bindings for R. googleway - Access Google Maps API to retrieve data and draw maps. mapview - Interactive viewing of spatial data. mapedit - Interactive editing of spatial data. tmap - Create thematic maps, such as choropleths and bubble maps. datamaps - Create interactive choropleth maps with the JavaScript library Datamaps, add arcs and bubbles, change choropleth values, and change labels. topogram - Cartogram htmlwidget for visualizing geographical data by distorting a TopoJSON topology using cartogram-chart. rsquaire - R interface for squaire.js, a JavaScript library for making responsive equal-area square maps using D3. hchinamap - Mapping China and its provinces with highcharts. mapbrew - Mapping China with amCharts. planetary - htmlwidget for the planetary.js library for creating interactive globes. gior - htmlwidget for gio.js for declarative 3D globe data visualization. quickglobe - View country data via a 3D D3 globe.","title":"Maps and Spatial Data"},{"location":"R/R%20Shiny%20Packages/#sparkline","text":"sparkline - jQuery Sparkline (tiny inline charts) HTML Widget for R. Use sparklines in DT . reactrend - Simple, elegant spark lines and trend graphs based on react-trend. peity - Peity htmlwidget for R. dataui - Interactive visualizations of data-ui based on vx.","title":"Sparkline"},{"location":"R/R%20Shiny%20Packages/#word-cloud","text":"wordcloud2 - Word cloud visualization based on wordcloud2.js. hwordcloud - Render word clouds with highcharts. d3wordcloud - htmlwidget for D3.js word cloud layout.","title":"Word Cloud"},{"location":"R/R%20Shiny%20Packages/#biological-data","text":"igvR - R package providing interactive connections to igv.js running in a web browser. igvShiny - htmlwidget for igv.js, a JavaScript library for embeddable genomic visualization. cyjShiny - htmlwidget for Cytoscape.js, a JavaScript library for graph/network visualization. The API is based on RCyjs (and thus RCy3). nglShiny - NGL Viewer as an htmlwidget for molecular visualization. msaR - BioJS-based MSA (multiple sequence alignment) viewer. TnT - Track-based visulizations based on the TnT JavaScript libraries. Useful for displaying genomic features as a simple genome browser. mutsneedle - Interactive mutation lollipop diagrams. g3viz - D3-based interactive lollipop plots. BioCircos.R - Interactive circular visualization of genomic data using htmlwidgets and BioCircos.js. chromoMap - Interactive visualization and mapping of human chromosomes. ideogRam - htmlwidget for chromosome visualization with ideogram.js. flowDashboard - Shiny Modules for visualizing flow cytometry data. qtlcharts - Interactive graphics for QTL experiments. phylocanvas - Interactive phylogenetic trees using the Phylocanvas JavaScript library. phylowidget - Interactive phylogenetic trees based on phylotree.js. JBrowseR - R interface to the JBrowse 2 linear genome view.","title":"Biological Data"},{"location":"R/R%20Shiny%20Packages/#chemical-data","text":"r3dmol - Visualizing molecular data in 3D, based on 3Dmol.js. chemdoodle - htmlwidget for visualizing and drawing molecules.","title":"Chemical Data"},{"location":"R/R%20Shiny%20Packages/#webgl","text":"rgl - Render WebGL scenes created with the rgl package ( vignette ). rayshader - Create and visualize hillshaded maps from elevation matrices. rayrender - Build and raytrace 3D scenes. rayfocus - Render depth of field for images.","title":"WebGL"},{"location":"R/R%20Shiny%20Packages/#augmented-and-virtual-reality","text":"shinyaframe - WebVR data visualizations with Shiny and Mozilla A-Frame. arframer - Augmented Reality in R based on AR.js.","title":"Augmented and Virtual Reality"},{"location":"R/R%20Shiny%20Packages/#backend","text":"Backend components and service integrations for Shiny apps.","title":"Backend"},{"location":"R/R%20Shiny%20Packages/#database","text":"db.rstudio.com - Packages and tutorials for connecting R and Shiny apps to databases. pool - Database connection pooling in R. elastic - R client for the Elasticsearch HTTP API. sergeant - Transform and query data with Apache Drill.","title":"Database"},{"location":"R/R%20Shiny%20Packages/#api-frameworks","text":"RestRserve - R web API framework for building high-performance and robust microservices and app backends. plumber - Create web APIs by decorating R code with special comments. opencpu - A system for embedded scientific computing and reproducible research with R.","title":"API Frameworks"},{"location":"R/R%20Shiny%20Packages/#url-routing","text":"shiny.router - Minimalistic URL router for Shiny apps. shinyURL - Save and restore the state of a Shiny app by encoding the values of user inputs and active tab panels in the app's URL query string.","title":"URL Routing"},{"location":"R/R%20Shiny%20Packages/#authentication","text":"shinymanager - Simple and secure authentification mechanism for single Shiny apps. googleAuthR - Shiny compatible Google API client for authentication with OAuth2. auth0 - Authentication in Shiny apps using Auth0. shinyauthr - Server-side authentication using shiny modules. firebase - Authenticate Shiny users with Google Firebase. otp - One-Time Password generation and verification. backendlessr - R wrapper for Backendless API to manage users.","title":"Authentication"},{"location":"R/R%20Shiny%20Packages/#job-scheduling","text":"cronR - R package for managing cron jobs.","title":"Job Scheduling"},{"location":"R/R%20Shiny%20Packages/#web-apis-integration","text":"glouton - Handle browser cookies in shiny, built on top of js-cookie. geoloc - Use the Geolocation API to get the location of the user (with user's permission). shinyStore - Use the Web Storage API to store persistent, synchronized data in the user's browser.","title":"Web APIs Integration"},{"location":"R/R%20Shiny%20Packages/#notification-integration","text":"slackr - Send messages, images, R objects, and files to Slack channels/users. sendgridr - Send emails with SendGrid mail API (v3). twilio - R interface to the Twilio API. blastula - Easily send HTML email messages from R. mjml - Create responsive emails with R and MJML. mailtoR - Creates a friendly user interface for emails sending in Shiny.","title":"Notification Integration"},{"location":"R/R%20Shiny%20Packages/#cloud-integration","text":"cloudyr - R packages for integrating with AWS, Azure, and Google Cloud.","title":"Cloud Integration"},{"location":"R/R%20Shiny%20Packages/#g-suite-integration","text":"googlesheets4 - R interface to Google Sheets via the Sheets API v4. googlesheets - R interface to Google Spreadsheets API (no longer under active development). googledrive - R API client for Google Drive. gmailr - Access the Gmail RESTful API from R.","title":"G Suite Integration"},{"location":"R/R%20Shiny%20Packages/#deploy","text":"Deploy Shiny apps to the cloud, hosted infrastructure, or desktop.","title":"Deploy"},{"location":"R/R%20Shiny%20Packages/#cloud-deploy","text":"rsconnect - Deploy Shiny apps to shinyapps.io, or RStudio Connect.","title":"Cloud Deploy"},{"location":"R/R%20Shiny%20Packages/#desktop-deploy","text":"RInno - Deploy Shiny apps to Windows by interfacing Inno Setup and Electron. electricShine - Create distributable Shiny Electron apps. photon - RStudio Add-in to build Shiny apps utilizing the Electron framework. DesktopDeployR - A framework for deploying self-contained R-based applications to the desktop. Shiny Meets Electron - Talk at useR! 2018 on turning Shiny app into standalone desktop apps. Talk video . r-shiny-electron - Template for R Shiny and Electron integration. nativefier - Create Electron wrappers for any websites (including remotely deployed Shiny apps).","title":"Desktop Deploy"},{"location":"R/R%20Shiny%20Packages/#developer-tools","text":"Debug, test, and optimize Shiny apps.","title":"Developer Tools"},{"location":"R/R%20Shiny%20Packages/#prototyping","text":"golem - Opinionated framework for building production-grade Shiny apps. shinipsum - Lorem-Ipsum-like helpers for fast Shiny prototyping. fakir - Create fake data in R for tutorials. shinysnippets - A series of Shiny related RStudio snippets.","title":"Prototyping"},{"location":"R/R%20Shiny%20Packages/#modularization","text":"tidymodules - An object-oriented approach to Shiny modules. supreme - Structure Shiny applications developed with modules.","title":"Modularization"},{"location":"R/R%20Shiny%20Packages/#debugging","text":"shinyreactlog - Visual debugger for Shiny reactivity. reactlog - Easier debugging with the Shiny reactive log.","title":"Debugging"},{"location":"R/R%20Shiny%20Packages/#testing","text":"shinytest - Automated testing for Shiny apps. shinyloadtest - Load testing for Shiny apps. reactor - Unit testing for Shiny reactivity.","title":"Testing"},{"location":"R/R%20Shiny%20Packages/#profiling","text":"profvis - Interactive visualizations for profiling R code. Profiling Shiny apps .","title":"Profiling"},{"location":"R/R%20Shiny%20Packages/#scaling","text":"promises - Promise-based asynchronous programming for R. Using promises with Shiny . shinyParallel - Run Shiny applications in a multi-session mode.","title":"Scaling"},{"location":"R/R%20Shiny%20Packages/#miscellaneous","text":"Not necessarily an R package, but it helps.","title":"Miscellaneous"},{"location":"R/R%20Shiny%20Packages/#ui-customization","text":"Bootstrap Live Customizer - Customize Bootswatch themes (Bootstrap 3) to create your own Bootstrap themes. google-webfonts-helper - A hassle-free way to self-hosted Google Fonts, useful for air-gapped environments.","title":"UI Customization"},{"location":"R/R%20Shiny%20Packages/#dependency-resolution","text":"packrat - Parse R package dependencies of Shiny apps with packrat::appDependencies . sysreqsdb - SystemRequirements mappings for R packages. shinyapps-package-dependencies - A collection of bash scripts that install system dependencies for R packages.","title":"Dependency Resolution"},{"location":"R/R%20Shiny%20Packages/#books","text":"Mastering Shiny: Build Interactive Apps, Reports, and Dashboards Powered by R Engineering Production-Grade Shiny Apps JavaScript for R Outstanding User Interfaces with Shiny","title":"Books"},{"location":"R/R%20Shiny%20Packages/#videos-screencasts","text":"Shiny Developer Series - Interviews with practitioners & developers of Shiny and the broader ecosystem of Shiny packages, plus occasional live streams of Shiny app development in action. Links: Source:","title":"Videos / Screencasts"},{"location":"R/RStudio%20Configuration%20Notes/","text":"RStudio Configuration \u2691 Directories \u2691 %localappdata%\\RStudio-Desktop - RStudio Desktop Internal State %appdata%\\RStudio - RStudio Configuration Directory (Preferences)","title":"RStudio Configuration"},{"location":"R/RStudio%20Configuration%20Notes/#rstudio-configuration","text":"","title":"RStudio Configuration"},{"location":"R/RStudio%20Configuration%20Notes/#directories","text":"%localappdata%\\RStudio-Desktop - RStudio Desktop Internal State %appdata%\\RStudio - RStudio Configuration Directory (Preferences)","title":"Directories"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/","text":"Shiny Apps as Packages in R \u2691 Books \u2691 Engineering Shiny Book Chapter on Structuring App as Package \u2b50 Mastering Shiny Package Chapter \u2b50 Blog Posts \u2691 Workflow: Part 1 Workflow: Part 2 Articles \u2691 Building Shiny App as a Package \u2b50 Packaging Shiny Applications: A Deep Dive \u2b50 Best Practices for Developing Robust Shiny Dashboards as R Packages \u2b50 Dean Attali: Packaging Shiny Apps Further Reading \u2691 R Package Development \u2691 These resources are for general package development within the R ecosystem. R-Core & CRAN \u2691 Official R Manuals Writing R Extensions \u2b50 CRAN Task Views Homepage \u2b50 Robust Research Web Technologies \u2b50 rOpenSci \u2691 rOpenSci Package DevGuide \u2b50 BioConductor \u2691 BioConductor Package Guidelines \u2b50 Building Packages for BioConductor Unit Testing Creating Workflow Packages \u2b50 RStudio \u2691 Developing Packages with RStudio Building, Testing and Distributing Packages Writing Package Documentation RStudio Video: Auto-Magic Package Development Books \u2691 See R-Project's Book Listing for more resources on all topics R related. R Packages (Hadley Wickham) \u2b50 Advanced R (Hadley Wickam) Advanced R Course (Florian Priv\u00e9) R Package Workshop Bookdown \u2b50 Best Practices: R Development Workshop \u2b50 Advanced Topics: R Development Workshop Package Development in R Package Development Chapter in \"Modern R with Tidyverse\" \u2b50 Tutorials \u2691 Writing an R package from scratch (Hilary Parker) Writing an R package from scratch (Updated) (Thomas Westlake) usethis workflow for package development (Emil Hvitfeldt) R package primer (Karl Broman) \u2b50 R Package Development Pictorial (Matthew J Denny) Building R Packages with Devtools (Jiddu Alexander) Developing R packages (Jeff Leek) R Package Tutorial (Colautti Lab) Instructions for creating your own R package (MIT) How to Create and Distribute an R Package (Shian Su) Workshops \u2691 R Forwards Package Workshop (Chicago, February 23, 2019) Write your own R package (UBC STAT 545) Blogs \u2691 How to develop good R packages (for open science) (Malle Salmon) R-Task: RMD First Development \u2b50 Style \u2691 Tidyverse Google Jean Fan A Computational Analysis of the Dynamics of R Style Based on 94 Million Lines of Code from All CRAN Packages in the Past 20 Years. (Yen, C.Y., Chang, M.H.W., Chan, C.H.) Shiny Modules \u2691 https://shiny.rstudio.com/articles/modules.html Desktop Applications \u2691 https://www.travishinkelman.com/deploy-shiny-electron/ Links: Source:","title":"Shiny Apps as Packages in R"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#shiny-apps-as-packages-in-r","text":"","title":"Shiny Apps as Packages in R"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#books","text":"Engineering Shiny Book Chapter on Structuring App as Package \u2b50 Mastering Shiny Package Chapter \u2b50","title":"Books"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#blog-posts","text":"Workflow: Part 1 Workflow: Part 2","title":"Blog Posts"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#articles","text":"Building Shiny App as a Package \u2b50 Packaging Shiny Applications: A Deep Dive \u2b50 Best Practices for Developing Robust Shiny Dashboards as R Packages \u2b50 Dean Attali: Packaging Shiny Apps","title":"Articles"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#further-reading","text":"","title":"Further Reading"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#r-package-development","text":"These resources are for general package development within the R ecosystem.","title":"R Package Development"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#r-core-cran","text":"Official R Manuals Writing R Extensions \u2b50 CRAN Task Views Homepage \u2b50 Robust Research Web Technologies \u2b50","title":"R-Core &amp; CRAN"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#ropensci","text":"rOpenSci Package DevGuide \u2b50","title":"rOpenSci"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#bioconductor","text":"BioConductor Package Guidelines \u2b50 Building Packages for BioConductor Unit Testing Creating Workflow Packages \u2b50","title":"BioConductor"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#rstudio","text":"Developing Packages with RStudio Building, Testing and Distributing Packages Writing Package Documentation RStudio Video: Auto-Magic Package Development","title":"RStudio"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#books_1","text":"See R-Project's Book Listing for more resources on all topics R related. R Packages (Hadley Wickham) \u2b50 Advanced R (Hadley Wickam) Advanced R Course (Florian Priv\u00e9) R Package Workshop Bookdown \u2b50 Best Practices: R Development Workshop \u2b50 Advanced Topics: R Development Workshop Package Development in R Package Development Chapter in \"Modern R with Tidyverse\" \u2b50","title":"Books"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#tutorials","text":"Writing an R package from scratch (Hilary Parker) Writing an R package from scratch (Updated) (Thomas Westlake) usethis workflow for package development (Emil Hvitfeldt) R package primer (Karl Broman) \u2b50 R Package Development Pictorial (Matthew J Denny) Building R Packages with Devtools (Jiddu Alexander) Developing R packages (Jeff Leek) R Package Tutorial (Colautti Lab) Instructions for creating your own R package (MIT) How to Create and Distribute an R Package (Shian Su)","title":"Tutorials"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#workshops","text":"R Forwards Package Workshop (Chicago, February 23, 2019) Write your own R package (UBC STAT 545)","title":"Workshops"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#blogs","text":"How to develop good R packages (for open science) (Malle Salmon) R-Task: RMD First Development \u2b50","title":"Blogs"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#style","text":"Tidyverse Google Jean Fan A Computational Analysis of the Dynamics of R Style Based on 94 Million Lines of Code from All CRAN Packages in the Past 20 Years. (Yen, C.Y., Chang, M.H.W., Chan, C.H.)","title":"Style"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#shiny-modules","text":"https://shiny.rstudio.com/articles/modules.html","title":"Shiny Modules"},{"location":"R/Shiny%20Apps%20as%20Packages%20in%20R/#desktop-applications","text":"https://www.travishinkelman.com/deploy-shiny-electron/ Links: Source:","title":"Desktop Applications"},{"location":"R/Tools%20Package%20Hidden%20Gems%20in%20R/","text":"Check Unstated Dependencies in Tests: tools:::check_packages_used_in_tests() Source: r-devel/r-svn) tools ::: check_packages_used_in_tests ( dir = \".\" , testdir = \"tests/testthat\" )","title":"Tools Package Hidden Gems in R"},{"location":"R/Useful%20Packages%20in%20R%20List/","text":"Useful R Packages to Look Into \u2691 Awesome Lists \u2691 qinwf/awesome-R: A curated list of awesome R packages, frameworks and software grabear/awesome-rshiny: An awesome R-shiny list! R Universe \u2691 R-Universe provides monorepos for R packages and renders them into nice websites. Source Github Repository Website data-cleaning r-universe/data-cleaning data-cleaning.r-universe.dev rOpenSci r-universe/ropensci ropensci.r-universe.dev RStudio r-universe/rstudio rstudio.r-universe.dev Appsilon r-universe/appsilon appsilon.r-universe.dev r-dbi r-universe/r-dbi r-dbi.r-universe.dev JohnCoene r-universe/johncoene johncoene.r-universe.dev HenrikBengtsson r-universe/henrikbengtsson henrikbengtsson.r-universe.dev daattali r-universe/daattali daattali.r-universe.dev eddelbuettel r-universe/eddelbuettel eddelbuettel.r-universe.dev r-forge r-universe/r-forge r-forge.r-universe.dev dreamRs r-universe/dreamrs dreamrs.r-universe.dev MarkEdmondson1234 r-universe/markedmondson1234 markedmondson1234.r-universe.dev jimhester r-universe/jimhester jimhester.r-universe.dev RinteRface r-universe/rinterface rinterface.r-universe.dev ColinFay r-universe/colinfay colinfay.r-universe.dev tidyverse r-universe/tidyverse tidyverse.r-universe.dev r-spatial r-universe/r-spatial r-spatial.r-universe.dev cboettig r-universe/cboettig cboettig.r-universe.dev yihui (Yihui Xie) r-universe/yihui: yihui yihui.r-universe.dev (Various) r-universe/test: test test.r-universe.dev the cloudyr project r-universe/cloudyr cloudyr.r-universe.dev Microsoft Azure r-universe/azure azure.r-universe.dev R Shiny Related \u2691 ThinkR-open/golem: A Framework for Building Robust Shiny Apps colinfay/golemexamples tyronehunt/GolemShinyTemplate ColinFay/glouton: 'JS-cookies' in Shiny ColinFay/cordes: Boilerplate for Wrapping Node Modules in R packages r4fun/keys: Keyboard Shortcuts for shiny ColinFay/crrry: 'crrri' recipes for shiny ColinFay/dockerfiler: Easy Dockerfile Creation from R yonicd/whereami: Reliably return location where command is called from in R ColinFay/minifyr: Wrapper around node-minify NodeJS module grabear/shiny-pathfinder-loot: An R shiny app used to keep up with loot tables on Google Sheets during Pathfinder/RPG sessions","title":"Useful R Packages to Look Into"},{"location":"R/Useful%20Packages%20in%20R%20List/#useful-r-packages-to-look-into","text":"","title":"Useful R Packages to Look Into"},{"location":"R/Useful%20Packages%20in%20R%20List/#awesome-lists","text":"qinwf/awesome-R: A curated list of awesome R packages, frameworks and software grabear/awesome-rshiny: An awesome R-shiny list!","title":"Awesome Lists"},{"location":"R/Useful%20Packages%20in%20R%20List/#r-universe","text":"R-Universe provides monorepos for R packages and renders them into nice websites. Source Github Repository Website data-cleaning r-universe/data-cleaning data-cleaning.r-universe.dev rOpenSci r-universe/ropensci ropensci.r-universe.dev RStudio r-universe/rstudio rstudio.r-universe.dev Appsilon r-universe/appsilon appsilon.r-universe.dev r-dbi r-universe/r-dbi r-dbi.r-universe.dev JohnCoene r-universe/johncoene johncoene.r-universe.dev HenrikBengtsson r-universe/henrikbengtsson henrikbengtsson.r-universe.dev daattali r-universe/daattali daattali.r-universe.dev eddelbuettel r-universe/eddelbuettel eddelbuettel.r-universe.dev r-forge r-universe/r-forge r-forge.r-universe.dev dreamRs r-universe/dreamrs dreamrs.r-universe.dev MarkEdmondson1234 r-universe/markedmondson1234 markedmondson1234.r-universe.dev jimhester r-universe/jimhester jimhester.r-universe.dev RinteRface r-universe/rinterface rinterface.r-universe.dev ColinFay r-universe/colinfay colinfay.r-universe.dev tidyverse r-universe/tidyverse tidyverse.r-universe.dev r-spatial r-universe/r-spatial r-spatial.r-universe.dev cboettig r-universe/cboettig cboettig.r-universe.dev yihui (Yihui Xie) r-universe/yihui: yihui yihui.r-universe.dev (Various) r-universe/test: test test.r-universe.dev the cloudyr project r-universe/cloudyr cloudyr.r-universe.dev Microsoft Azure r-universe/azure azure.r-universe.dev","title":"R Universe"},{"location":"R/Useful%20Packages%20in%20R%20List/#r-shiny-related","text":"ThinkR-open/golem: A Framework for Building Robust Shiny Apps colinfay/golemexamples tyronehunt/GolemShinyTemplate ColinFay/glouton: 'JS-cookies' in Shiny ColinFay/cordes: Boilerplate for Wrapping Node Modules in R packages r4fun/keys: Keyboard Shortcuts for shiny ColinFay/crrry: 'crrri' recipes for shiny ColinFay/dockerfiler: Easy Dockerfile Creation from R yonicd/whereami: Reliably return location where command is called from in R ColinFay/minifyr: Wrapper around node-minify NodeJS module grabear/shiny-pathfinder-loot: An R shiny app used to keep up with loot tables on Google Sheets during Pathfinder/RPG sessions","title":"R Shiny Related"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/","text":"Utils Package Hidden Gems \u2691 readClipboard and writeClipboard \u2691 One of my favorite duo of functions from utils is readCLipboard and writeClipboard . If you\u2019re doing some manipulation to get a quick answer between R and Excel, these functions can come in handy. readClipboard reads in whatever is currently on the Clipboard. For example, let\u2019s copy a column of cells from Excel. We can now run readClipboard() in R. The result of running this command is a vector containing the column of cells we just copied. Each cell corresponds to an element in the vector. Similarly, if we want to write a vector of elements to the clipboard, we can the writeClipboard command: test <- c ( \"write\" , \"to\" , \"clipboard\" ) writeClipboard ( test ) Now, the vector test has been copied to the clipboard. If you paste the result in Excel, you\u2019ll see a column of cells corresponding to the vector you just copied. combn \u2691 The combn function is useful for getting the possible combinations of an input vector. For instance, let\u2019s say we want to get all of the possible 2-element combinations of a vector, we could do this: food <- c ( \"apple\" , \"grape\" , \"orange\" , \"pear\" , \"peach\" , \"banana\" ) combn ( food , 2 ) In general, the first parameter of combn is the vector of elements you want to get possible combinations from. The second parameter is the number of elements you want in each combination. So if you need to get all possible 3-element or 4-element combinations, you would just need to change this number to three or four. combn(food, 3) combn(food, 4) We can also add a parameter called simplify to make the function return a list of each combination, rather than giving back a matrix output like above. combn(food, 3, simplify = FALSE) fileSnapshot \u2691 The fileSnapshot function is one R\u2019s collection of file manipulation functions. To learn more about file manipulation and getting information on files in R, check out this post . fileSnapshot will list and provide details about the files in a directory. This function returns a list of objects. # get file snapshot of current directory snapshot <- fileSnapshot () # or file snapshot of another directory snapshot <- fileSnapshot ( \"C:/some/other/directory\" ) fileSnapshot returns a list, which here we will just call \u201csnapshot\u201d. The most useful piece of information can be garnered from this by referencing \u201cinfo\u201d: snapshot$info Here, snapshot$info is a data frame showing information about the files in the input folder parameter. Its headers include: size ==> size of file isdir ==> is file a directory? ==> TRUE or FALSE mode ==> the file permissions in octal mtime ==> last modified time stamp ctime ==> time stamp created atime ==> time stamp last accessed exe ==> type of executable (or \u201cno\u201d if not an executable) download.file \u2691 download.file does just what it sounds like \u2013 downloads a file from the internet to the destination provided in the function\u2019s input. The first parameter is the URL of the file you wish to download. The second parameter is the name you want to give to the downloaded file. Below, we download a file and call it \u201ccensus_data.csv\u201d. download . file ( \"https://www2.census.gov/programs-surveys/popest/datasets/2010/2010-eval-estimates/cc-est2010-alldata.csv\" , \"census_data.csv\" ) fix - modify an object on the fly \u2691 The utils package also has the ability to modify objects on the fly with the fix function. For instance, let\u2019s say you define a function interactively, and you want to make some modification. some_func <- function ( num ) { 3 * num + 1 } Now, let\u2019s modify the function with fix : fix(some_func) : When you call fix , it comes up with an editor allowing you to modify the definition of the function. You can also call fix to modify a vector or data frame. fix(iris)","title":"Utils Package Hidden Gems"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/#utils-package-hidden-gems","text":"","title":"Utils Package Hidden Gems"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/#readclipboard-andwriteclipboard","text":"One of my favorite duo of functions from utils is readCLipboard and writeClipboard . If you\u2019re doing some manipulation to get a quick answer between R and Excel, these functions can come in handy. readClipboard reads in whatever is currently on the Clipboard. For example, let\u2019s copy a column of cells from Excel. We can now run readClipboard() in R. The result of running this command is a vector containing the column of cells we just copied. Each cell corresponds to an element in the vector. Similarly, if we want to write a vector of elements to the clipboard, we can the writeClipboard command: test <- c ( \"write\" , \"to\" , \"clipboard\" ) writeClipboard ( test ) Now, the vector test has been copied to the clipboard. If you paste the result in Excel, you\u2019ll see a column of cells corresponding to the vector you just copied.","title":"readClipboard andwriteClipboard"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/#combn","text":"The combn function is useful for getting the possible combinations of an input vector. For instance, let\u2019s say we want to get all of the possible 2-element combinations of a vector, we could do this: food <- c ( \"apple\" , \"grape\" , \"orange\" , \"pear\" , \"peach\" , \"banana\" ) combn ( food , 2 ) In general, the first parameter of combn is the vector of elements you want to get possible combinations from. The second parameter is the number of elements you want in each combination. So if you need to get all possible 3-element or 4-element combinations, you would just need to change this number to three or four. combn(food, 3) combn(food, 4) We can also add a parameter called simplify to make the function return a list of each combination, rather than giving back a matrix output like above. combn(food, 3, simplify = FALSE)","title":"combn"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/#filesnapshot","text":"The fileSnapshot function is one R\u2019s collection of file manipulation functions. To learn more about file manipulation and getting information on files in R, check out this post . fileSnapshot will list and provide details about the files in a directory. This function returns a list of objects. # get file snapshot of current directory snapshot <- fileSnapshot () # or file snapshot of another directory snapshot <- fileSnapshot ( \"C:/some/other/directory\" ) fileSnapshot returns a list, which here we will just call \u201csnapshot\u201d. The most useful piece of information can be garnered from this by referencing \u201cinfo\u201d: snapshot$info Here, snapshot$info is a data frame showing information about the files in the input folder parameter. Its headers include: size ==> size of file isdir ==> is file a directory? ==> TRUE or FALSE mode ==> the file permissions in octal mtime ==> last modified time stamp ctime ==> time stamp created atime ==> time stamp last accessed exe ==> type of executable (or \u201cno\u201d if not an executable)","title":"fileSnapshot"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/#downloadfile","text":"download.file does just what it sounds like \u2013 downloads a file from the internet to the destination provided in the function\u2019s input. The first parameter is the URL of the file you wish to download. The second parameter is the name you want to give to the downloaded file. Below, we download a file and call it \u201ccensus_data.csv\u201d. download . file ( \"https://www2.census.gov/programs-surveys/popest/datasets/2010/2010-eval-estimates/cc-est2010-alldata.csv\" , \"census_data.csv\" )","title":"download.file"},{"location":"R/Utils%20Package%20Hidden%20Gems%20in%20R/#fix-modify-an-object-on-the-fly","text":"The utils package also has the ability to modify objects on the fly with the fix function. For instance, let\u2019s say you define a function interactively, and you want to make some modification. some_func <- function ( num ) { 3 * num + 1 } Now, let\u2019s modify the function with fix : fix(some_func) : When you call fix , it comes up with an editor allowing you to modify the definition of the function. You can also call fix to modify a vector or data frame. fix(iris)","title":"fix - modify an object on the fly"},{"location":"R/Databases%20with%20R/Using%20Pool/","text":"Using Pool, DBI, and Other Drivers \u2691 The R PAckage pool enables the creation of object pools, which make it less computationally expensive to fetch a new object. Currently the only supported pooled objects are 'DBI' connections. Pool \u2691 Connection Methods: pool::poolCreate( ... ) pool::dbPool( ... ) DBI Connection Methods: \u2691 As a convenience, Pool implements DBIConnection methods; calling any implemented DBI method directly on a Pool object will result in a connection being checked out (with poolCheckout() ), the operation being performed on that connection, and the connection being returned to the pool (with poolReturn() ). Pool cannot implement the DBI::dbSendQuery() and DBI::dbSendStatement() methods because they both return live ResultSet objects. This is incompatible with the Pool model, because once a connection is returned to the pool, using an existing ResultSet object could give erroneous results, throw an error, or even crash the entire R process. In most cases, DBI::dbGetQuery() and DBI::dbExecute() can be used instead. If you really need the control that dbSendQuery gives you (for example, to process a large table in chunks) then use poolCheckout() to get a real connection object (and don't forget to return it to the pool using poolReturn() afterwards). Usage: ## S4 method for signature 'Pool' dbSendQuery ( conn , statement , ... ) ## S4 method for signature 'Pool,ANY' dbSendStatement ( conn , statement , ... ) ## S4 method for signature 'Pool,character' dbGetQuery ( conn , statement , ... ) ## S4 method for signature 'Pool,character' dbExecute ( conn , statement , ... ) ## S4 method for signature 'Pool' dbListResults ( conn , ... ) ## S4 method for signature 'Pool,character' dbListFields ( conn , name , ... ) ## S4 method for signature 'Pool' dbListTables ( conn , ... ) ## S4 method for signature 'Pool' dbListObjects ( conn , prefix = NULL , ... ) ## S4 method for signature 'Pool,character' dbReadTable ( conn , name , ... ) ## S4 method for signature 'Pool,ANY' dbWriteTable ( conn , name , value , ... ) ## S4 method for signature 'Pool' dbCreateTable ( conn , name , fields , ... , row.names = NULL , temporary = FALSE ) ## S4 method for signature 'Pool' dbAppendTable ( conn , name , value , ... , row.names = NULL ) ## S4 method for signature 'Pool,ANY' dbExistsTable ( conn , name , ... ) ## S4 method for signature 'Pool,ANY' dbRemoveTable ( conn , name , ... ) ## S4 method for signature 'Pool' dbIsReadOnly ( dbObj , ... ) Where conn and dbObj are Pool objects, as returned by dbPool() . See DBI Documentation for remaining parameters/arguments. poolCreate \u2691 poolCreate is an S4 class for compatibility with the DBI methods. The main difference to consider here is that poolCreate builds off an existing factory function responsible for the generation of the objects that the pool will hold (ex: for DBI database connections, this function is dbConnect ). It must take no arguments. Example Usage \u2691 Will provide two example factory functions to pass to pool::poolCreate , one using DBI and the other using dbx : DBI: library ( DBI ) # DBI factory function factory_fn_dbi <- function () { # note: no args in the factory function DBI :: dbConnect ( Postgres (), host = \"<host>\" , dbname = \"<dbname>\" , user = \"postgres\" , password = \"<password>\" , port = \"5432\" ) } pool <- poolCreate ( factory = factory_fn_dbi ) pool :: poolClose ( pool ) dbx: library ( dbx ) library ( RPostgres ) library ( pool ) factory_fn_dbx <- function () dbx :: dbxConnect ( < database - URI > ) pool <- poolCreate ( factory = factory_fn_dbx ) pool :: poolClose ( pool ) dbPool \u2691 dbPool Creates a DBI Database Connection Pool serving as a wrapper around poolCreate to simplify the creation of a DBI database connection pool. Check the documentation of poolCreate() for a generic overview of the parent function and the Pool object. The main thing to point out is that, for dbPool , you always need to provide a DBI driver (i.e. of class DBI::DBIDriver-class()), and it should always be accompanied by the required authorization arguments (see the example below). dbPool ( drv , ... , validateQuery = NULL ) Example: pool <- pool :: dbPool ( drv = RPostres :: Postgres (), < database connection arguments > ) The pool Object: \u2691 pool <- poolCreate ( factory , minSize = 1 , maxSize = Inf , idleTimeout = 60 , validationInterval = 600 , state = NULL ) factory_fn <- function () DBI :: dbConnect ( < connection_params > ) pool <- poolCreate ( factory_fn ) > class ( pool ) [ 1 ] \"Pool\" \"R6\" > str ( pool ) Classes 'Pool' , 'R6' < Pool > Public : clone : function ( deep = FALSE ) close : function () counters : environment fetch : function () idleTimeout : 60 initialize : function ( factory , minSize , maxSize , idleTimeout , validationInterval , maxSize : Inf minSize : 1 release : function ( object ) state : NULL valid : TRUE validationInterval : 600 Private : cancelScheduledTask : function ( object , task ) changeObjectStatus : function ( object , to ) checkValid : function ( object ) checkValidTemplate : function ( object , errorFun ) createObject : function () destroyObject : function ( object ) factory : function () freeObjects : environment idCounter : 2 validate : function ( object ) As you can see above a pool object in R is classified with two classes: R6 and Pool which can be useful in many ways: - To determine a connections type for reference/informational purposes - Dispatch generic methods onto the connection objects by utilizing their inherited classes (i.e. through if (inherits(conn, \"R6\" ) corresponding methods connections bases off their inherited classes. - As opposed to a direct DBI connection which would inherit the class of the database driver's specifications: i.e. conn <- poolCheckout ( pool ) > class ( conn ) [ 1 ] \"PqConnection\" attr (, \"package\" ) [ 1 ] \"RPostgres\" > str ( conn ) Formal class 'PqConnection' [ package \"RPostgres\" ] with 5 slots .. @ ptr :< externalptr > .. @ bigint : chr \"integer64\" .. @ timezone : chr \"UTC\" .. @ timezone_out : chr \"UTC\" .. @ typnames : 'data.frame' : 487 obs. of 2 variables : .. .. $ oid : int [ 1 : 487 ] 16 17 18 19 20 21 22 23 24 25 ... .. .. $ typname : chr [ 1 : 487 ] \"bool\" \"bytea\" \"char\" \"name\" ... Notice that by checking out the pool object using conn <- pool::poolCheckout(pool) the newly created conn connection argument now resembles the default DBI::dbConnect(...) returning object's attributes, classes, etc. Links: Source:","title":"Using Pool, DBI, and Other Drivers"},{"location":"R/Databases%20with%20R/Using%20Pool/#using-pool-dbi-and-other-drivers","text":"The R PAckage pool enables the creation of object pools, which make it less computationally expensive to fetch a new object. Currently the only supported pooled objects are 'DBI' connections.","title":"Using Pool, DBI, and Other Drivers"},{"location":"R/Databases%20with%20R/Using%20Pool/#pool","text":"Connection Methods: pool::poolCreate( ... ) pool::dbPool( ... )","title":"Pool"},{"location":"R/Databases%20with%20R/Using%20Pool/#dbi-connection-methods","text":"As a convenience, Pool implements DBIConnection methods; calling any implemented DBI method directly on a Pool object will result in a connection being checked out (with poolCheckout() ), the operation being performed on that connection, and the connection being returned to the pool (with poolReturn() ). Pool cannot implement the DBI::dbSendQuery() and DBI::dbSendStatement() methods because they both return live ResultSet objects. This is incompatible with the Pool model, because once a connection is returned to the pool, using an existing ResultSet object could give erroneous results, throw an error, or even crash the entire R process. In most cases, DBI::dbGetQuery() and DBI::dbExecute() can be used instead. If you really need the control that dbSendQuery gives you (for example, to process a large table in chunks) then use poolCheckout() to get a real connection object (and don't forget to return it to the pool using poolReturn() afterwards). Usage: ## S4 method for signature 'Pool' dbSendQuery ( conn , statement , ... ) ## S4 method for signature 'Pool,ANY' dbSendStatement ( conn , statement , ... ) ## S4 method for signature 'Pool,character' dbGetQuery ( conn , statement , ... ) ## S4 method for signature 'Pool,character' dbExecute ( conn , statement , ... ) ## S4 method for signature 'Pool' dbListResults ( conn , ... ) ## S4 method for signature 'Pool,character' dbListFields ( conn , name , ... ) ## S4 method for signature 'Pool' dbListTables ( conn , ... ) ## S4 method for signature 'Pool' dbListObjects ( conn , prefix = NULL , ... ) ## S4 method for signature 'Pool,character' dbReadTable ( conn , name , ... ) ## S4 method for signature 'Pool,ANY' dbWriteTable ( conn , name , value , ... ) ## S4 method for signature 'Pool' dbCreateTable ( conn , name , fields , ... , row.names = NULL , temporary = FALSE ) ## S4 method for signature 'Pool' dbAppendTable ( conn , name , value , ... , row.names = NULL ) ## S4 method for signature 'Pool,ANY' dbExistsTable ( conn , name , ... ) ## S4 method for signature 'Pool,ANY' dbRemoveTable ( conn , name , ... ) ## S4 method for signature 'Pool' dbIsReadOnly ( dbObj , ... ) Where conn and dbObj are Pool objects, as returned by dbPool() . See DBI Documentation for remaining parameters/arguments.","title":"DBI Connection Methods:"},{"location":"R/Databases%20with%20R/Using%20Pool/#poolcreate","text":"poolCreate is an S4 class for compatibility with the DBI methods. The main difference to consider here is that poolCreate builds off an existing factory function responsible for the generation of the objects that the pool will hold (ex: for DBI database connections, this function is dbConnect ). It must take no arguments.","title":"poolCreate"},{"location":"R/Databases%20with%20R/Using%20Pool/#example-usage","text":"Will provide two example factory functions to pass to pool::poolCreate , one using DBI and the other using dbx : DBI: library ( DBI ) # DBI factory function factory_fn_dbi <- function () { # note: no args in the factory function DBI :: dbConnect ( Postgres (), host = \"<host>\" , dbname = \"<dbname>\" , user = \"postgres\" , password = \"<password>\" , port = \"5432\" ) } pool <- poolCreate ( factory = factory_fn_dbi ) pool :: poolClose ( pool ) dbx: library ( dbx ) library ( RPostgres ) library ( pool ) factory_fn_dbx <- function () dbx :: dbxConnect ( < database - URI > ) pool <- poolCreate ( factory = factory_fn_dbx ) pool :: poolClose ( pool )","title":"Example Usage"},{"location":"R/Databases%20with%20R/Using%20Pool/#dbpool","text":"dbPool Creates a DBI Database Connection Pool serving as a wrapper around poolCreate to simplify the creation of a DBI database connection pool. Check the documentation of poolCreate() for a generic overview of the parent function and the Pool object. The main thing to point out is that, for dbPool , you always need to provide a DBI driver (i.e. of class DBI::DBIDriver-class()), and it should always be accompanied by the required authorization arguments (see the example below). dbPool ( drv , ... , validateQuery = NULL ) Example: pool <- pool :: dbPool ( drv = RPostres :: Postgres (), < database connection arguments > )","title":"dbPool"},{"location":"R/Databases%20with%20R/Using%20Pool/#the-pool-object","text":"pool <- poolCreate ( factory , minSize = 1 , maxSize = Inf , idleTimeout = 60 , validationInterval = 600 , state = NULL ) factory_fn <- function () DBI :: dbConnect ( < connection_params > ) pool <- poolCreate ( factory_fn ) > class ( pool ) [ 1 ] \"Pool\" \"R6\" > str ( pool ) Classes 'Pool' , 'R6' < Pool > Public : clone : function ( deep = FALSE ) close : function () counters : environment fetch : function () idleTimeout : 60 initialize : function ( factory , minSize , maxSize , idleTimeout , validationInterval , maxSize : Inf minSize : 1 release : function ( object ) state : NULL valid : TRUE validationInterval : 600 Private : cancelScheduledTask : function ( object , task ) changeObjectStatus : function ( object , to ) checkValid : function ( object ) checkValidTemplate : function ( object , errorFun ) createObject : function () destroyObject : function ( object ) factory : function () freeObjects : environment idCounter : 2 validate : function ( object ) As you can see above a pool object in R is classified with two classes: R6 and Pool which can be useful in many ways: - To determine a connections type for reference/informational purposes - Dispatch generic methods onto the connection objects by utilizing their inherited classes (i.e. through if (inherits(conn, \"R6\" ) corresponding methods connections bases off their inherited classes. - As opposed to a direct DBI connection which would inherit the class of the database driver's specifications: i.e. conn <- poolCheckout ( pool ) > class ( conn ) [ 1 ] \"PqConnection\" attr (, \"package\" ) [ 1 ] \"RPostgres\" > str ( conn ) Formal class 'PqConnection' [ package \"RPostgres\" ] with 5 slots .. @ ptr :< externalptr > .. @ bigint : chr \"integer64\" .. @ timezone : chr \"UTC\" .. @ timezone_out : chr \"UTC\" .. @ typnames : 'data.frame' : 487 obs. of 2 variables : .. .. $ oid : int [ 1 : 487 ] 16 17 18 19 20 21 22 23 24 25 ... .. .. $ typname : chr [ 1 : 487 ] \"bool\" \"bytea\" \"char\" \"name\" ... Notice that by checking out the pool object using conn <- pool::poolCheckout(pool) the newly created conn connection argument now resembles the default DBI::dbConnect(...) returning object's attributes, classes, etc. Links: Source:","title":"The pool Object:"},{"location":"System%20Design/","text":"System Design \u2691 Categories \u2691 Documents \u2691 System Design Primer System Design","title":"System Design"},{"location":"System%20Design/#system-design","text":"","title":"System Design"},{"location":"System%20Design/#categories","text":"","title":"Categories"},{"location":"System%20Design/#documents","text":"System Design Primer System Design","title":"Documents"},{"location":"System%20Design/System%20Design%20Primer/","text":"System Design Primer \u2691 Performance vs scalability \u2691 A service is scalable if it results in increased performance in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow. 1 Another way to look at performance vs scalability: If you have a performance problem, your system is slow for a single user. If you have a scalability problem, your system is fast for a single user but slow under heavy load. Source(s) and further reading \u2691 A word on scalability Scalability, availability, stability, patterns Latency vs throughput \u2691 Latency is the time to perform some action or to produce some result. Throughput is the number of such actions or results per unit of time. Generally, you should aim for maximal throughput with acceptable latency . Source(s) and further reading \u2691 Availability vs consistency \u2691 CAP theorem \u2691 Source: CAP theorem revisited In a distributed computer system, you can only support two of the following guarantees: Consistency - Every read receives the most recent write or an error Availability - Every request receives a response, without guarantee that it contains the most recent version of the information Partition Tolerance - The system continues to operate despite arbitrary partitioning due to network failures Networks aren't reliable, so you'll need to support partition tolerance. You'll need to make a software tradeoff between consistency and availability. CP - consistency and partition tolerance \u2691 Waiting for a response from the partitioned node might result in a timeout error. CP is a good choice if your business needs require atomic reads and writes. AP - availability and partition tolerance \u2691 Responses return the most readily available version of the data available on any node, which might not be the latest. Writes might take some time to propagate when the partition is resolved. AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors. Source(s) and further reading \u2691 CAP theorem revisited A plain english introduction to CAP theorem CAP FAQ The CAP theorem Understanding latency vs throughput Links: System Design | [[Web Development]] | Databases Source: donnemartin/system-design-primer","title":"System Design Primer"},{"location":"System%20Design/System%20Design%20Primer/#system-design-primer","text":"","title":"System Design Primer"},{"location":"System%20Design/System%20Design%20Primer/#performance-vs-scalability","text":"A service is scalable if it results in increased performance in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow. 1 Another way to look at performance vs scalability: If you have a performance problem, your system is slow for a single user. If you have a scalability problem, your system is fast for a single user but slow under heavy load.","title":"Performance vs scalability"},{"location":"System%20Design/System%20Design%20Primer/#sources-and-further-reading","text":"A word on scalability Scalability, availability, stability, patterns","title":"Source(s) and further reading"},{"location":"System%20Design/System%20Design%20Primer/#latency-vs-throughput","text":"Latency is the time to perform some action or to produce some result. Throughput is the number of such actions or results per unit of time. Generally, you should aim for maximal throughput with acceptable latency .","title":"Latency vs throughput"},{"location":"System%20Design/System%20Design%20Primer/#sources-and-further-reading_1","text":"","title":"Source(s) and further reading"},{"location":"System%20Design/System%20Design%20Primer/#availability-vs-consistency","text":"","title":"Availability vs consistency"},{"location":"System%20Design/System%20Design%20Primer/#cap-theorem","text":"Source: CAP theorem revisited In a distributed computer system, you can only support two of the following guarantees: Consistency - Every read receives the most recent write or an error Availability - Every request receives a response, without guarantee that it contains the most recent version of the information Partition Tolerance - The system continues to operate despite arbitrary partitioning due to network failures Networks aren't reliable, so you'll need to support partition tolerance. You'll need to make a software tradeoff between consistency and availability.","title":"CAP theorem"},{"location":"System%20Design/System%20Design%20Primer/#cp-consistency-and-partition-tolerance","text":"Waiting for a response from the partitioned node might result in a timeout error. CP is a good choice if your business needs require atomic reads and writes.","title":"CP - consistency and partition tolerance"},{"location":"System%20Design/System%20Design%20Primer/#ap-availability-and-partition-tolerance","text":"Responses return the most readily available version of the data available on any node, which might not be the latest. Writes might take some time to propagate when the partition is resolved. AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors.","title":"AP - availability and partition tolerance"},{"location":"System%20Design/System%20Design%20Primer/#sources-and-further-reading_2","text":"CAP theorem revisited A plain english introduction to CAP theorem CAP FAQ The CAP theorem Understanding latency vs throughput Links: System Design | [[Web Development]] | Databases Source: donnemartin/system-design-primer","title":"Source(s) and further reading"},{"location":"System%20Design/System%20Design/","text":"","title":"System Design"},{"location":"Tools/","text":"Tools \u2691 Categories \u2691 Web Browsers Documents \u2691","title":"Tools"},{"location":"Tools/#tools","text":"","title":"Tools"},{"location":"Tools/#categories","text":"Web Browsers","title":"Categories"},{"location":"Tools/#documents","text":"","title":"Documents"},{"location":"Tools/Web%20Browsers/","text":"Web Browsers \u2691 Categories \u2691 Documents \u2691 Firefox Developer Edition Web Browsers","title":"Web Browsers"},{"location":"Tools/Web%20Browsers/#web-browsers","text":"","title":"Web Browsers"},{"location":"Tools/Web%20Browsers/#categories","text":"","title":"Categories"},{"location":"Tools/Web%20Browsers/#documents","text":"Firefox Developer Edition Web Browsers","title":"Documents"},{"location":"Tools/Web%20Browsers/Firefox%20Developer%20Edition/","text":"Firefox Developer Edition \u2691 Really, just use Firefox for fuck sake. Features \u2691 The key killer feature of Firefox is containers . Settings \u2691 General: Enable Ctrl + Tab to cycle through tabs in recently used order: ![[_assets/Pasted image 20210501120642.png]] - Themes \u2691 Currently using the Matte Black (Red) Theme Extensions \u2691 Keeper Password Manager & Digital Vault Momentum OneTab Raindrop.io Drak Reader Non-Essential Extensions: Evernote Web Clipper Instapaper TamperMonkey Developer Tools \u2691 See 30 Tips Tricks with the Firefox Developer Tools Medium article You can save a snapshot of the network requests in your Network Monitor. It saves them as HAR or HTTP Archive format. You can also import HAR files and have them display in the Network Monitor so you can debug them. Links: Sources: - 30 Tips Tricks with the Firefox Developer Tools - Mozilla Github Organization Account Home Page - Calling all web developers: here\u2019s why you should be using Firefox - Firefox is the best browser for web-developers","title":"Firefox Developer Edition"},{"location":"Tools/Web%20Browsers/Firefox%20Developer%20Edition/#firefox-developer-edition","text":"Really, just use Firefox for fuck sake.","title":"Firefox Developer Edition"},{"location":"Tools/Web%20Browsers/Firefox%20Developer%20Edition/#features","text":"The key killer feature of Firefox is containers .","title":"Features"},{"location":"Tools/Web%20Browsers/Firefox%20Developer%20Edition/#settings","text":"General: Enable Ctrl + Tab to cycle through tabs in recently used order: ![[_assets/Pasted image 20210501120642.png]] -","title":"Settings"},{"location":"Tools/Web%20Browsers/Firefox%20Developer%20Edition/#themes","text":"Currently using the Matte Black (Red) Theme","title":"Themes"},{"location":"Tools/Web%20Browsers/Firefox%20Developer%20Edition/#extensions","text":"Keeper Password Manager & Digital Vault Momentum OneTab Raindrop.io Drak Reader Non-Essential Extensions: Evernote Web Clipper Instapaper TamperMonkey","title":"Extensions"},{"location":"Tools/Web%20Browsers/Firefox%20Developer%20Edition/#developer-tools","text":"See 30 Tips Tricks with the Firefox Developer Tools Medium article You can save a snapshot of the network requests in your Network Monitor. It saves them as HAR or HTTP Archive format. You can also import HAR files and have them display in the Network Monitor so you can debug them. Links: Sources: - 30 Tips Tricks with the Firefox Developer Tools - Mozilla Github Organization Account Home Page - Calling all web developers: here\u2019s why you should be using Firefox - Firefox is the best browser for web-developers","title":"Developer Tools"},{"location":"Tools/Web%20Browsers/Web%20Browsers/","text":"Web Browsers \u2691 ![[_assets/Pasted image 20210501123611.png]] ![[_assets/Pasted image 20210501123655.png]] ![[_assets/Pasted image 20210501123722.png]] Currently my primary web browsers are: - [Microsoft Edge Canary] - [Mozilla Firefox Developer Edition] Links: Source:","title":"Web Browsers"},{"location":"Tools/Web%20Browsers/Web%20Browsers/#web-browsers","text":"![[_assets/Pasted image 20210501123611.png]] ![[_assets/Pasted image 20210501123655.png]] ![[_assets/Pasted image 20210501123722.png]] Currently my primary web browsers are: - [Microsoft Edge Canary] - [Mozilla Firefox Developer Edition] Links: Source:","title":"Web Browsers"},{"location":"WSL/","text":"WSL \u2691 Categories \u2691 Documents \u2691 WSL Commands and Installs WSL","title":"WSL"},{"location":"WSL/#wsl","text":"","title":"WSL"},{"location":"WSL/#categories","text":"","title":"Categories"},{"location":"WSL/#documents","text":"WSL Commands and Installs WSL","title":"Documents"},{"location":"WSL/WSL%20Commands%20and%20Installs/","text":"WSL Commands and Installs \u2691 Initial WSL Installations \u2691 Update, upgrade, and cleanup apt packages library Install build-essential , autoconf , and libtool Install python , gcloud sdk , and github-cli (Python, Git, and Bash should already be included with installation and upgraded in step 1 above) sudp apt update && sudo apt upgrade && sudo apt autoclean sudo apt - get install build - essential autoconf libtool Install gcloud sdk \u2691 Follow the instruction below to install the Cloud SDK: Add the Cloud SDK distribution URI as a package source: echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list sudo apt-get install apt-transport-https ca-certificates curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - Update and install the Cloud SDK: sudo apt-get update && sudo apt-get install google-cloud-sdk Run gcloud init to initialize the SDK: gcloud init Install Python \u2691 Install Node.js \u2691 Install Github-CLI \u2691 Install Git \u2691 New WSLg GUI Applications \u2691 Install Edge with new WSLg version of WSL that supports GUI applications natively. sudo apt update && sudo apt upgrade sudo curl https://packages.microsoft.com/repos/edge/pool/main/m/microsoft-edge-dev/microsoft-edge-dev_91.0.852.0-1_amd64.deb -o /tmp/edge.deb sudo apt install /tmp/edge.deb \\- y Install RStudio: sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 sudo add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/\" sudo apt update sudo apt install r-base sudo apt-get install gdebi-core wget https://download1.rstudio.org/desktop/bionic/amd64/rstudio-1.2.5042-amd64.deb sudo gdebi rstudio-1.2.5042-amd64.deb Links: Source:","title":"WSL Commands and Installs"},{"location":"WSL/WSL%20Commands%20and%20Installs/#wsl-commands-and-installs","text":"","title":"WSL Commands and Installs"},{"location":"WSL/WSL%20Commands%20and%20Installs/#initial-wsl-installations","text":"Update, upgrade, and cleanup apt packages library Install build-essential , autoconf , and libtool Install python , gcloud sdk , and github-cli (Python, Git, and Bash should already be included with installation and upgraded in step 1 above) sudp apt update && sudo apt upgrade && sudo apt autoclean sudo apt - get install build - essential autoconf libtool","title":"Initial WSL Installations"},{"location":"WSL/WSL%20Commands%20and%20Installs/#install-gcloud-sdk","text":"Follow the instruction below to install the Cloud SDK: Add the Cloud SDK distribution URI as a package source: echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list sudo apt-get install apt-transport-https ca-certificates curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - Update and install the Cloud SDK: sudo apt-get update && sudo apt-get install google-cloud-sdk Run gcloud init to initialize the SDK: gcloud init","title":"Install gcloud sdk"},{"location":"WSL/WSL%20Commands%20and%20Installs/#install-python","text":"","title":"Install Python"},{"location":"WSL/WSL%20Commands%20and%20Installs/#install-nodejs","text":"","title":"Install Node.js"},{"location":"WSL/WSL%20Commands%20and%20Installs/#install-github-cli","text":"","title":"Install Github-CLI"},{"location":"WSL/WSL%20Commands%20and%20Installs/#install-git","text":"","title":"Install Git"},{"location":"WSL/WSL%20Commands%20and%20Installs/#new-wslg-gui-applications","text":"Install Edge with new WSLg version of WSL that supports GUI applications natively. sudo apt update && sudo apt upgrade sudo curl https://packages.microsoft.com/repos/edge/pool/main/m/microsoft-edge-dev/microsoft-edge-dev_91.0.852.0-1_amd64.deb -o /tmp/edge.deb sudo apt install /tmp/edge.deb \\- y Install RStudio: sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 sudo add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/\" sudo apt update sudo apt install r-base sudo apt-get install gdebi-core wget https://download1.rstudio.org/desktop/bionic/amd64/rstudio-1.2.5042-amd64.deb sudo gdebi rstudio-1.2.5042-amd64.deb Links: Source:","title":"New WSLg GUI Applications"},{"location":"WSL/WSL%20Ubuntu%20on%20Windows%20Community%20Preview/","text":"WSL Ubuntu on Windows Community Preview \u2691 Can only download from this link to Windows Store currently. Links: MS Store Link | Article by Microsoft Source: Announcing Ubuntu on Windows Community Preview \u2013 WSL 2 | Ubuntu","title":"WSL Ubuntu on Windows Community Preview"},{"location":"WSL/WSL%20Ubuntu%20on%20Windows%20Community%20Preview/#wsl-ubuntu-on-windows-community-preview","text":"Can only download from this link to Windows Store currently. Links: MS Store Link | Article by Microsoft Source: Announcing Ubuntu on Windows Community Preview \u2013 WSL 2 | Ubuntu","title":"WSL Ubuntu on Windows Community Preview"},{"location":"WSL/WSL/","text":"WSL- Contents \u2691 Contents: \u2691 WSL Commands and Installs WSL Ubuntu on Windows Community Preview Backlinks: Sources:","title":"WSL- Contents"},{"location":"WSL/WSL/#wsl-contents","text":"","title":"WSL- Contents"},{"location":"WSL/WSL/#contents","text":"WSL Commands and Installs WSL Ubuntu on Windows Community Preview Backlinks: Sources:","title":"Contents:"},{"location":"WSL/WSLg/","text":"WSLg \u2691 In April 2021, Windows announced the preview for the new version of WSL named WSLg with support got graphical user interface applications (GUIs). Enhance the experience by downloading graphics drivers to support hardware acceleration, etc: GPU in Windows Subsystem for Linux (WSL) | NVIDIA Developer 1 Documentation \u2013 CUDA WSL + DirectML Support NVIDIA Blog \u2013 CUDA on WSL Microsoft Blog \u2013 Windows Subsystem for Linux Microsoft Docs: What is WSL? Medium Article: Run RAPIDS on Microsoft Windows 10 using WSL 2 CUDA WSL Forum Links: WSL | WSLg | WSL Ubuntu on Windows Community Preview Reference: WSLg Architecture | Windows Insider | NVIDIA Developer Docs Source: WSLg Architecture Note: NVIDIA drivers for WSL with CUDA and DirectML support are available as preview for Microsoft Windows Insider Program members who have registered for the NVIDIA Developer Program . \u21a9","title":"WSLg"},{"location":"WSL/WSLg/#wslg","text":"In April 2021, Windows announced the preview for the new version of WSL named WSLg with support got graphical user interface applications (GUIs). Enhance the experience by downloading graphics drivers to support hardware acceleration, etc: GPU in Windows Subsystem for Linux (WSL) | NVIDIA Developer 1 Documentation \u2013 CUDA WSL + DirectML Support NVIDIA Blog \u2013 CUDA on WSL Microsoft Blog \u2013 Windows Subsystem for Linux Microsoft Docs: What is WSL? Medium Article: Run RAPIDS on Microsoft Windows 10 using WSL 2 CUDA WSL Forum Links: WSL | WSLg | WSL Ubuntu on Windows Community Preview Reference: WSLg Architecture | Windows Insider | NVIDIA Developer Docs Source: WSLg Architecture Note: NVIDIA drivers for WSL with CUDA and DirectML support are available as preview for Microsoft Windows Insider Program members who have registered for the NVIDIA Developer Program . \u21a9","title":"WSLg"},{"location":"Web%20Development/","text":"Web Development \u2691 Categories \u2691 Documents \u2691 APIs Backend Web Architecture HTTP Requests Notes Javascript WebDev Resource List","title":"Web Development"},{"location":"Web%20Development/#web-development","text":"","title":"Web Development"},{"location":"Web%20Development/#categories","text":"","title":"Categories"},{"location":"Web%20Development/#documents","text":"APIs Backend Web Architecture HTTP Requests Notes Javascript WebDev Resource List","title":"Documents"},{"location":"Web%20Development/APIs/","text":"APIs \u2691 ![[_assets/zapier-introduction-to-apis.pdf]] Links: System Design Source: An Introduction to APIs | Zapier","title":"APIs"},{"location":"Web%20Development/APIs/#apis","text":"![[_assets/zapier-introduction-to-apis.pdf]] Links: System Design Source: An Introduction to APIs | Zapier","title":"APIs"},{"location":"Web%20Development/Backend%20Web%20Architecture/","text":"Source : https://www.codecademy.com/articles/back-end-architecture Backend Web Architecture \u2691 Software engineers seem to always be discussing the front-end and the back-end of their apps. But what exactly does this mean? The front-end is the code that is executed on the client side. This code (typically HTML, CSS, and JavaScript) runs in the user\u2019s browser and creates the user interface. The back-end is the code that runs on the server, that receives requests from the clients, and contains the logic to send the appropriate data back to the client. The back-end also includes the database, which will persistently store all of the data for the application. This article focuses on the hardware and software on the server-side that make this possible. Review HTTP and REST if you want to refresh your memory on these topics. These are the main conventions that provide structure to the request-response cycle between clients and servers. Let\u2019s start by reviewing the client-server relationship, and then we can start to put the pieces all together! What are the clients? \u2691 The clients are anything that send requests to the back-end. They are often browsers that make requests for the HTML and JavaScript code that they will execute to display websites to the end user. However, there many different kinds of clients: they might be a mobile application, an application running on another server, or even a web enabled smart appliance. What is a back-end? \u2691 The back-end is all of the technology required to process the incoming request and generate and send the response to the client. This typically includes three major parts: The server. This is the computer that receives requests. The app. This is the application running on the server that listens for requests, retrieves information from the database, and sends a response. The database. Databases are used to organize and persist data. What is a server? \u2691 A server is simply a computer that listens for incoming requests. Though there are machines made and optimized for this particular purpose, any computer that is connected to a network can act as a server. In fact, you will often use your very own computer as server when developing apps. What are the core functions of the app? \u2691 The server runs an app that contains logic about how to respond to various requests based on the HTTP verb and the Uniform Resource Identifier (URI) . The pair of an HTTP verb and a URI is called a route and matching them based on a request is called routing . Some of these handler functions will be middleware . In this context, middleware is any code that executes between the server receiving a request and sending a response. These middleware functions might modify the request object, query the database, or otherwise process the incoming request. Middleware functions typically end by passing control to the next middleware function, rather than by sending a response. Eventually, a middleware function will be called that ends the request-response cycle by sending an HTTP response back to the client. Often, programmers will use a framework like Express or Ruby on Rails to simplify the logic of routing. For now, just think that each route can have one or many handler functions that are executed whenever a request to that route (HTTP verb and URI) is matched. What kinds of responses can a server send? \u2691 The data that the server sends back can come in different forms. For example, a server might serve up an HTML file, send data as JSON, or it might send back only an HTTP status code . You\u2019ve probably seen the status code \u201c404 - Not Found\u201d whenever you've tried navigating to a URI that doesn\u2019t exist, but there are many more status codes that indicate what happened when the server received the request. What is a database, and why do we need to use them? \u2691 Databases are commonly used on the back-end of web applications. These databases provide an interface to save data in a persistent way to memory. Storing the data in a database both reduces the load on the main memory of the server CPU and allows the data to be retrieved if the server crashes or loses power. Many requests sent to the server might require a database query. A client might request information that is stored in the database, or a client might submit data with their request to be added to the database. What is a Web API, really? \u2691 An API is a collection of clearly defined methods of communication between different software components. More specifically, a Web API is the interface created by the back-end: the collection of endpoints and the resources these endpoints expose. A Web API is defined by the types of requests that it can handle, which is determined by the routes that it defines, and the types of responses that the clients can expect to receive after hitting those routes. One Web API can be used to provide data for different front-ends. Since a Web API can provide data without really specifying how the data is viewed, multiple different HTML pages or mobile applications can be created to view the data from the Web API. Other principles of the request-response cycle: \u2691 The server typically cannot initiate responses without requests! Every request needs a response, even if it\u2019s just a 404 status code indicating that the content was not found. Otherwise your client will be left hanging (indefinitely waiting). The server should not send more than one response per request. This will throw errors in your code. Mapping out a request \u2691 Let\u2019s make all of this a bit more concrete, by following an example of the main steps that happen when a client makes a request to the server. Alice is shopping on SuperCoolShop.com. She clicks on a picture of a cover for her smartphone, and that click event makes a GET request to http://www.SuperCoolShop.com/products/66432 . Remember, GET describes the kind of request (the client is just asking for data, not changing anything). The URI (uniform resource identifier) /products/66432 specifies that the client is looking for more information about a product, and that product, has an id of 66432. SuperCoolShop has an huge number of products, and many different categories for filtering through them, so the actual URI would be more complicated than this. But this is the general principle for how requests and resource identifiers work. Alice\u2019s request travels across the internet to one of SuperCoolShop\u2019s servers. This is one of the slower steps in the process, because the request cannot go faster than the speed of light, and it might have a long distance to travel. For this reason, major websites with users all over the world will have many different servers, and they will direct users to the server that is closest to them! The server, which is actively listening for requests from all users, receives Alice\u2019s request! Event listeners that match this request (the HTTP verb: GET, and the URI: /products/66432 ) are triggered. The code that runs on the server between the request and the response is called middleware . In processing the request, the server code makes a database query to get more information about this smartphone case. The database contains all of the other information that Alice wants to know about this smartphone case: the name of the product, the price of the product, a few product reviews, and a string that will provide a path to the image of the product. The database query is executed, and the database sends the requested data back to the server. It\u2019s worth noting that database queries are one of the slower steps in this process. Reading and writing from static memory is fairly slow, and the database might be on a different machine than the original server. This query itself might have to go across the internet! The server receives the data that it needs from the database, and it is now ready to construct and send its response back to the client. This response body has all of the information needed by the browser to show Alice more details (price, reviews, size, etc) about the phone case she\u2019s interested in. The response header will contain an HTTP status code 200 to indicate that the request has succeeded. The response travels across the internet, back to Alice\u2019s computer. Alice\u2019s browser receives the response and uses that information to create and render the view that Alice ultimately sees!","title":"Backend Web Architecture"},{"location":"Web%20Development/Backend%20Web%20Architecture/#backend-web-architecture","text":"Software engineers seem to always be discussing the front-end and the back-end of their apps. But what exactly does this mean? The front-end is the code that is executed on the client side. This code (typically HTML, CSS, and JavaScript) runs in the user\u2019s browser and creates the user interface. The back-end is the code that runs on the server, that receives requests from the clients, and contains the logic to send the appropriate data back to the client. The back-end also includes the database, which will persistently store all of the data for the application. This article focuses on the hardware and software on the server-side that make this possible. Review HTTP and REST if you want to refresh your memory on these topics. These are the main conventions that provide structure to the request-response cycle between clients and servers. Let\u2019s start by reviewing the client-server relationship, and then we can start to put the pieces all together!","title":"Backend Web Architecture"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-are-the-clients","text":"The clients are anything that send requests to the back-end. They are often browsers that make requests for the HTML and JavaScript code that they will execute to display websites to the end user. However, there many different kinds of clients: they might be a mobile application, an application running on another server, or even a web enabled smart appliance.","title":"What are the clients?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-is-a-back-end","text":"The back-end is all of the technology required to process the incoming request and generate and send the response to the client. This typically includes three major parts: The server. This is the computer that receives requests. The app. This is the application running on the server that listens for requests, retrieves information from the database, and sends a response. The database. Databases are used to organize and persist data.","title":"What is a back-end?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-is-a-server","text":"A server is simply a computer that listens for incoming requests. Though there are machines made and optimized for this particular purpose, any computer that is connected to a network can act as a server. In fact, you will often use your very own computer as server when developing apps.","title":"What is a server?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-are-the-core-functions-of-the-app","text":"The server runs an app that contains logic about how to respond to various requests based on the HTTP verb and the Uniform Resource Identifier (URI) . The pair of an HTTP verb and a URI is called a route and matching them based on a request is called routing . Some of these handler functions will be middleware . In this context, middleware is any code that executes between the server receiving a request and sending a response. These middleware functions might modify the request object, query the database, or otherwise process the incoming request. Middleware functions typically end by passing control to the next middleware function, rather than by sending a response. Eventually, a middleware function will be called that ends the request-response cycle by sending an HTTP response back to the client. Often, programmers will use a framework like Express or Ruby on Rails to simplify the logic of routing. For now, just think that each route can have one or many handler functions that are executed whenever a request to that route (HTTP verb and URI) is matched.","title":"What are the core functions of the app?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-kinds-of-responses-can-a-server-send","text":"The data that the server sends back can come in different forms. For example, a server might serve up an HTML file, send data as JSON, or it might send back only an HTTP status code . You\u2019ve probably seen the status code \u201c404 - Not Found\u201d whenever you've tried navigating to a URI that doesn\u2019t exist, but there are many more status codes that indicate what happened when the server received the request.","title":"What kinds of responses can a server send?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-is-a-database-and-why-do-we-need-to-use-them","text":"Databases are commonly used on the back-end of web applications. These databases provide an interface to save data in a persistent way to memory. Storing the data in a database both reduces the load on the main memory of the server CPU and allows the data to be retrieved if the server crashes or loses power. Many requests sent to the server might require a database query. A client might request information that is stored in the database, or a client might submit data with their request to be added to the database.","title":"What is a database, and why do we need to use them?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#what-is-a-web-api-really","text":"An API is a collection of clearly defined methods of communication between different software components. More specifically, a Web API is the interface created by the back-end: the collection of endpoints and the resources these endpoints expose. A Web API is defined by the types of requests that it can handle, which is determined by the routes that it defines, and the types of responses that the clients can expect to receive after hitting those routes. One Web API can be used to provide data for different front-ends. Since a Web API can provide data without really specifying how the data is viewed, multiple different HTML pages or mobile applications can be created to view the data from the Web API.","title":"What is a Web API, really?"},{"location":"Web%20Development/Backend%20Web%20Architecture/#other-principles-of-the-request-response-cycle","text":"The server typically cannot initiate responses without requests! Every request needs a response, even if it\u2019s just a 404 status code indicating that the content was not found. Otherwise your client will be left hanging (indefinitely waiting). The server should not send more than one response per request. This will throw errors in your code.","title":"Other principles of the request-response cycle:"},{"location":"Web%20Development/Backend%20Web%20Architecture/#mapping-out-a-request","text":"Let\u2019s make all of this a bit more concrete, by following an example of the main steps that happen when a client makes a request to the server. Alice is shopping on SuperCoolShop.com. She clicks on a picture of a cover for her smartphone, and that click event makes a GET request to http://www.SuperCoolShop.com/products/66432 . Remember, GET describes the kind of request (the client is just asking for data, not changing anything). The URI (uniform resource identifier) /products/66432 specifies that the client is looking for more information about a product, and that product, has an id of 66432. SuperCoolShop has an huge number of products, and many different categories for filtering through them, so the actual URI would be more complicated than this. But this is the general principle for how requests and resource identifiers work. Alice\u2019s request travels across the internet to one of SuperCoolShop\u2019s servers. This is one of the slower steps in the process, because the request cannot go faster than the speed of light, and it might have a long distance to travel. For this reason, major websites with users all over the world will have many different servers, and they will direct users to the server that is closest to them! The server, which is actively listening for requests from all users, receives Alice\u2019s request! Event listeners that match this request (the HTTP verb: GET, and the URI: /products/66432 ) are triggered. The code that runs on the server between the request and the response is called middleware . In processing the request, the server code makes a database query to get more information about this smartphone case. The database contains all of the other information that Alice wants to know about this smartphone case: the name of the product, the price of the product, a few product reviews, and a string that will provide a path to the image of the product. The database query is executed, and the database sends the requested data back to the server. It\u2019s worth noting that database queries are one of the slower steps in this process. Reading and writing from static memory is fairly slow, and the database might be on a different machine than the original server. This query itself might have to go across the internet! The server receives the data that it needs from the database, and it is now ready to construct and send its response back to the client. This response body has all of the information needed by the browser to show Alice more details (price, reviews, size, etc) about the phone case she\u2019s interested in. The response header will contain an HTTP status code 200 to indicate that the request has succeeded. The response travels across the internet, back to Alice\u2019s computer. Alice\u2019s browser receives the response and uses that information to create and render the view that Alice ultimately sees!","title":"Mapping out a request"},{"location":"Web%20Development/HTTP%20Requests%20Notes/","text":"HTTP Requests | Codecademy \u2691 Understand the basics of how your web browser communicates with the internet. Background: \u2691 This page is generated by a web of HTML, CSS, and Javascript, sent to you by Codecademy via the internet. The internet is made up of a bunch of resources hosted on different servers. The term \u201cresource\u201d corresponds to any entity on the web, including HTML files, stylesheets, images, videos, and scripts. To access content on the internet, your browser must ask these servers for the resources it wants, and then display these resources to you. This protocol of requests and responses enables you view this page in your browser. This article focuses on one fundamental part of how the internet functions: HTTP. What is HTTP? \u2691 HTTP stands for Hypertext Transfer Protocol and is used to structure requests and responses over the internet. HTTP requires data to be transferred from one point to another over the network. The transfer of resources happens using TCP (Transmission Control Protocol). In viewing this webpage, TCP manages the channels between your browser and the server (in this case, codecademy.com). TCP is used to manage many types of internet connections in which one computer or device wants to send something to another. HTTP is the command language that the devices on both sides of the connection must follow in order to communicate. HTTP & TCP: How it Works \u2691 When you type an address such as www.codecademy.com into your browser, you are commanding it to open a TCP channel to the server that responds to that URL (or Uniform Resource Locator, which you can read more about on Wikipedia ). A URL is like your home address or phone number because it describes how to reach you. In this situation, your computer, which is making the request, is called the client. The URL you are requesting is the address that belongs to the server. Once the TCP connection is established, the client sends a HTTP GET request to the server to retrieve the webpage it should display. After the server has sent the response, it closes the TCP connection. If you open the website in your browser again, or if your browser automatically requests something from the server, a new connection is opened which follows the same process described above. GET requests are one kind of HTTP method a client can call. You can learn more about the other common ones ( POST , PUT and DELETE ) in this article . Let\u2019s explore an example of how GET requests (the most common type of request) are used to help your computer (the client) access resources on the web. Suppose you want to check out the latest course offerings from http://codecademy.com . After you type the URL into your browser, your browser will extract the http part and recognize that it is the name of the network protocol to use. Then, it takes the domain name from the URL, in this case \u201ccodecademy.com\u201d, and asks the internet Domain Name Server to return an Internet Protocol (IP) address. Now the client knows the destination\u2019s IP address. It then opens a connection to the server at that address, using the http protocol as specified. It will initiate a GET request to the server which contains the IP address of the host and optionally a data payload. The GET request contains the following text: GET / HTTP/1.1Host: www.codecademy.com This identifies the type of request, the path on www.codecademy.com (in this case, \u201c/\u201c) and the protocol \u201cHTTP/1.1.\u201d HTTP/1.1 is a revision of the first HTTP, which is now called HTTP/1.0. In HTTP/1.0, every resource request requires a separate connection to the server. HTTP/1.1 uses one connection more than once, so that additional content (like images or stylesheets) is retrieved even after the page has been retrieved. As a result, requests using HTTP/1.1 have less delay than those using HTTP/1.0. The second line of the request contains the address of the server which is \"www.codecademy.com\" . There may be additional lines as well depending on what data your browser chooses to send. If the server is able to locate the path requested, the server might respond with the header: HTTP/1.1 200 OKContent-Type: text/html This header is followed by the content requested, which in this case is the information needed to render www.codecademy.com . The first line of the header, HTTP/1.1 200 OK , is confirmation that the server understands the protocol that the client wants to communicate with ( HTTP/1.1 ), and an HTTP status code signifying that the resource was found on the server. The third line, Content-Type: text/html , shows the type of content that it will be sending to the client. If the server is not able to locate the path requested by the client, it will respond with the header: HTTP/1.1 404 NOT FOUND In this case, the server identifies that it understands the HTTP protocol, but the 404 NOT FOUND status code signifies that the specific piece of content requested was not found. This might happen if the content was moved or if you typed in the URL path incorrectly or if the page was removed. You can read more about the 404 status code, commonly called a 404 error, here . An Analogy: \u2691 It can be tricky to understand how HTTP functions because it\u2019s difficult to examine what your browser is actually doing. (And perhaps also because we explained it using acronyms that may be new to you.) Let\u2019s review what we learned by using an analogy that could be more familiar to you. Imagine the internet is a town. You are a client and your address determines where you can be reached. Businesses in town, such as Codecademy.com, serve requests that are sent to them. The other houses are filled with other clients like you that are making requests and expecting responses from these businesses in town. This town also has a crazy fast mail service, an army of mail delivery staff that can travel on trains that move at the speed of light. Suppose you want to read the morning newspaper. In order to retrieve it, you write down what you need in a language called HTTP and ask your local mail delivery staff agent to retrieve it from a specific business. The mail delivery person agrees and builds a railroad track (connection) between your house and the business nearly instantly, and rides the train car labeled \u201cTCP\u201d to the address of the business you provided. Upon arriving at the business, she asks the first of several free employees ready to fulfill the request. The employee searches for the page of the newspaper that you requested but cannot find it and communicates that back to the mail delivery person. The mail delivery person returns on the light speed train, ripping up the tracks on the way back, and tells you that there was a problem \u201c404 Not Found.\u201d After you check the spelling of what you had written, you realize that you misspelled the newspaper title. You correct it and provide the corrected title to the mail delivery person. This time the mail delivery person is able to retrieve it from the business. You can now read your newspaper in peace until you decide you want to read the next page, at which point, you would make another request and give it to the mail delivery person. What is HTTPS? \u2691 Since your HTTP request can be read by anyone at certain network junctures, it might not be a good idea to deliver information such as your credit card or password using this protocol. Fortunately, many servers support HTTPS, short for HTTP Secure, which allows you to encrypt data that you send and receive. You can read more about HTTPS on Wikipedia . HTTPS is important to use when passing sensitive or personal information to and from websites. However, it is up to the businesses maintaining the servers to set it up. In order to support HTTPS, the business must apply for a certificate from a Certificate Authority . Source","title":"HTTP Requests | Codecademy"},{"location":"Web%20Development/HTTP%20Requests%20Notes/#http-requests-codecademy","text":"Understand the basics of how your web browser communicates with the internet.","title":"HTTP Requests | Codecademy"},{"location":"Web%20Development/HTTP%20Requests%20Notes/#background","text":"This page is generated by a web of HTML, CSS, and Javascript, sent to you by Codecademy via the internet. The internet is made up of a bunch of resources hosted on different servers. The term \u201cresource\u201d corresponds to any entity on the web, including HTML files, stylesheets, images, videos, and scripts. To access content on the internet, your browser must ask these servers for the resources it wants, and then display these resources to you. This protocol of requests and responses enables you view this page in your browser. This article focuses on one fundamental part of how the internet functions: HTTP.","title":"Background:"},{"location":"Web%20Development/HTTP%20Requests%20Notes/#what-is-http","text":"HTTP stands for Hypertext Transfer Protocol and is used to structure requests and responses over the internet. HTTP requires data to be transferred from one point to another over the network. The transfer of resources happens using TCP (Transmission Control Protocol). In viewing this webpage, TCP manages the channels between your browser and the server (in this case, codecademy.com). TCP is used to manage many types of internet connections in which one computer or device wants to send something to another. HTTP is the command language that the devices on both sides of the connection must follow in order to communicate.","title":"What is HTTP?"},{"location":"Web%20Development/HTTP%20Requests%20Notes/#http-tcp-how-it-works","text":"When you type an address such as www.codecademy.com into your browser, you are commanding it to open a TCP channel to the server that responds to that URL (or Uniform Resource Locator, which you can read more about on Wikipedia ). A URL is like your home address or phone number because it describes how to reach you. In this situation, your computer, which is making the request, is called the client. The URL you are requesting is the address that belongs to the server. Once the TCP connection is established, the client sends a HTTP GET request to the server to retrieve the webpage it should display. After the server has sent the response, it closes the TCP connection. If you open the website in your browser again, or if your browser automatically requests something from the server, a new connection is opened which follows the same process described above. GET requests are one kind of HTTP method a client can call. You can learn more about the other common ones ( POST , PUT and DELETE ) in this article . Let\u2019s explore an example of how GET requests (the most common type of request) are used to help your computer (the client) access resources on the web. Suppose you want to check out the latest course offerings from http://codecademy.com . After you type the URL into your browser, your browser will extract the http part and recognize that it is the name of the network protocol to use. Then, it takes the domain name from the URL, in this case \u201ccodecademy.com\u201d, and asks the internet Domain Name Server to return an Internet Protocol (IP) address. Now the client knows the destination\u2019s IP address. It then opens a connection to the server at that address, using the http protocol as specified. It will initiate a GET request to the server which contains the IP address of the host and optionally a data payload. The GET request contains the following text: GET / HTTP/1.1Host: www.codecademy.com This identifies the type of request, the path on www.codecademy.com (in this case, \u201c/\u201c) and the protocol \u201cHTTP/1.1.\u201d HTTP/1.1 is a revision of the first HTTP, which is now called HTTP/1.0. In HTTP/1.0, every resource request requires a separate connection to the server. HTTP/1.1 uses one connection more than once, so that additional content (like images or stylesheets) is retrieved even after the page has been retrieved. As a result, requests using HTTP/1.1 have less delay than those using HTTP/1.0. The second line of the request contains the address of the server which is \"www.codecademy.com\" . There may be additional lines as well depending on what data your browser chooses to send. If the server is able to locate the path requested, the server might respond with the header: HTTP/1.1 200 OKContent-Type: text/html This header is followed by the content requested, which in this case is the information needed to render www.codecademy.com . The first line of the header, HTTP/1.1 200 OK , is confirmation that the server understands the protocol that the client wants to communicate with ( HTTP/1.1 ), and an HTTP status code signifying that the resource was found on the server. The third line, Content-Type: text/html , shows the type of content that it will be sending to the client. If the server is not able to locate the path requested by the client, it will respond with the header: HTTP/1.1 404 NOT FOUND In this case, the server identifies that it understands the HTTP protocol, but the 404 NOT FOUND status code signifies that the specific piece of content requested was not found. This might happen if the content was moved or if you typed in the URL path incorrectly or if the page was removed. You can read more about the 404 status code, commonly called a 404 error, here .","title":"HTTP &amp; TCP: How it Works"},{"location":"Web%20Development/HTTP%20Requests%20Notes/#an-analogy","text":"It can be tricky to understand how HTTP functions because it\u2019s difficult to examine what your browser is actually doing. (And perhaps also because we explained it using acronyms that may be new to you.) Let\u2019s review what we learned by using an analogy that could be more familiar to you. Imagine the internet is a town. You are a client and your address determines where you can be reached. Businesses in town, such as Codecademy.com, serve requests that are sent to them. The other houses are filled with other clients like you that are making requests and expecting responses from these businesses in town. This town also has a crazy fast mail service, an army of mail delivery staff that can travel on trains that move at the speed of light. Suppose you want to read the morning newspaper. In order to retrieve it, you write down what you need in a language called HTTP and ask your local mail delivery staff agent to retrieve it from a specific business. The mail delivery person agrees and builds a railroad track (connection) between your house and the business nearly instantly, and rides the train car labeled \u201cTCP\u201d to the address of the business you provided. Upon arriving at the business, she asks the first of several free employees ready to fulfill the request. The employee searches for the page of the newspaper that you requested but cannot find it and communicates that back to the mail delivery person. The mail delivery person returns on the light speed train, ripping up the tracks on the way back, and tells you that there was a problem \u201c404 Not Found.\u201d After you check the spelling of what you had written, you realize that you misspelled the newspaper title. You correct it and provide the corrected title to the mail delivery person. This time the mail delivery person is able to retrieve it from the business. You can now read your newspaper in peace until you decide you want to read the next page, at which point, you would make another request and give it to the mail delivery person.","title":"An Analogy:"},{"location":"Web%20Development/HTTP%20Requests%20Notes/#what-is-https","text":"Since your HTTP request can be read by anyone at certain network junctures, it might not be a good idea to deliver information such as your credit card or password using this protocol. Fortunately, many servers support HTTPS, short for HTTP Secure, which allows you to encrypt data that you send and receive. You can read more about HTTPS on Wikipedia . HTTPS is important to use when passing sensitive or personal information to and from websites. However, it is up to the businesses maintaining the servers to set it up. In order to support HTTPS, the business must apply for a certificate from a Certificate Authority . Source","title":"What is HTTPS?"},{"location":"Web%20Development/Javascript/","text":"Javascript \u2691 getElementById < html > < body > < h2 > JavaScript in Body </ h2 > < p id = \"demo\" ></ p > //we can use class selctor also < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; //we can use onClick() funcion also </ script > </ body > </ html > Display properties in javascript JavaScript can \"display\" data in different ways: Writing into an HTML element, using innerHTML. Writing into the HTML output using document.write(). Writing into an alert box, using window.alert(). Writing into the browser console, using console.log(). we can write this code in","title":"Javascript"},{"location":"Web%20Development/Javascript/#javascript","text":"getElementById < html > < body > < h2 > JavaScript in Body </ h2 > < p id = \"demo\" ></ p > //we can use class selctor also < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; //we can use onClick() funcion also </ script > </ body > </ html > Display properties in javascript JavaScript can \"display\" data in different ways: Writing into an HTML element, using innerHTML. Writing into the HTML output using document.write(). Writing into an alert box, using window.alert(). Writing into the browser console, using console.log(). we can write this code in","title":"Javascript"},{"location":"Web%20Development/WebDev%20Resource%20List/","text":"WebDev Resource List \u2691 - MDN Web Docs \u2b50 \u2691 Links: Source:","title":"WebDev Resource List"},{"location":"Web%20Development/WebDev%20Resource%20List/#webdev-resource-list","text":"","title":"WebDev Resource List"},{"location":"Web%20Development/WebDev%20Resource%20List/#-mdn-web-docs","text":"Links: Source:","title":"- MDN Web Docs \u2b50"},{"location":"Windows/","text":"Windows \u2691 Categories \u2691 Documents \u2691 How to Cleanup Windows from Command Line SFC and DISM Commands Using diskusage Command in Windows WinGet CLI Setup and Settings Windows Command Line Commands Overview","title":"Windows"},{"location":"Windows/#windows","text":"","title":"Windows"},{"location":"Windows/#categories","text":"","title":"Categories"},{"location":"Windows/#documents","text":"How to Cleanup Windows from Command Line SFC and DISM Commands Using diskusage Command in Windows WinGet CLI Setup and Settings Windows Command Line Commands Overview","title":"Documents"},{"location":"Windows/How%20to%20Cleanup%20Windows%20from%20Command%20Line/","text":"A clean and tidy computer is hard to maintain over time on windows, and therefore one must be tediously maintaining their system for optimal performance. This means running a scan for malware, cleaning your hard drive using cleanmgr and sfc /scannow , uninstalling programs that you no longer need, checking for autostart programs (using msconfig ) and enabling Windows' Automatic Update . Always remember to perform periodic backups, or at least to set restore points. Should you experience an actual problem, try to recall the last thing you did, or the last thing you installed before the problem appeared for the first time. Use the resmon command to identify the processes that are causing your problem. Even for serious problems, rather than reinstalling Windows, you are better off repairing of your installation or, for Windows 8 and later versions, executing the DISM.exe /Online /Cleanup-image /Restorehealth command. This allows you to repair the operating system without losing data. To help you analyze the duet.exe process on your computer, the following programs have proven to be helpful: Security Task Manager displays all running Windows tasks, including embedded hidden processes, such as keyboard and browser monitoring or autostart entries. A unique security risk rating indicates the likelihood of the process being potential spyware, malware or a Trojan. Malwarebytes Anti-Malware detects and removes sleeping spyware, adware, Trojans, keyloggers, malware and trackers from your hard drive. Reference: \u2691 Utilities: cleanmgr or the Disk Cleanup (run as Admin for full system cleanup) uninstall tool or geek.exe msconfig sysinternals suite of executables Security Task Manager Malwarebytes Commands: wuauclt /ShowWindowsUpdate resmon msconfig cleanmgr sfc /scannow DISM.exe /Online /Cleanup-image /Restorehealth","title":"How to Cleanup Windows from Command Line"},{"location":"Windows/How%20to%20Cleanup%20Windows%20from%20Command%20Line/#reference","text":"Utilities: cleanmgr or the Disk Cleanup (run as Admin for full system cleanup) uninstall tool or geek.exe msconfig sysinternals suite of executables Security Task Manager Malwarebytes Commands: wuauclt /ShowWindowsUpdate resmon msconfig cleanmgr sfc /scannow DISM.exe /Online /Cleanup-image /Restorehealth","title":"Reference:"},{"location":"Windows/SFC%20and%20DISM%20Commands/","text":"SFC Scannow and DISM Commands \u2691 On Windows 10, when you start noticing random errors, problems booting up, or features not working as expected, there's a good chance that one or multiple system files might have gone missing or corrupted for unknown reasons. Usually, problems with system files could occur as a result of an issue installing a system update, driver, or application, or while making changes to the installation manually. If you happen to come across this issue, you can use the Windows 10 System File Checker (SFC), which is a command-line tool designed to scan the integrity and restore missing or corrupted system files with working replacements. Use the System File Checker tool to repair damaged system files automatically or manually if the tool refuses to work. On windows these commands are crucial for keeping a clean system. I run these commands so often that I made a .reg registry entry script to add them to my Desktop\u2019s Context Menu (see below for details or visit my dotfiles repo for the actual .reg scripts to run: The System File Checker - sfc /scannow The Deployment Image Servicing and Management tool - DISM.exe NOTE: these must be ran as an ADMINISTRATOR # powershell or CMD as ADMIN # system file checker sfc / scannow # DISM Dism / Online / Cleanup-Image / ScanHealth Dism / Online / Cleanup-Image / CheckHealth Dism / Online / Cleanup-Image / RestoreHealth SFC \u2691 Visit Microsoft's Docs on SFC - Link The sfc /scannow command is a well known way to do an integrity check of all Windows 10 system files. sfc.exe is the System File Checker tool which can be helpful in many scenarios and fix various issues with Windows 10. You can save you time by adding a special context menu entry to launch it directly with one click. Add SFC /Scannow to Desktop Context Menu: \u2691 Registry Script: Windows Registry Editor Version 5.00 [ HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\SFCScannow] \"Icon\" = \"cmd.exe\" \"MUIVerb\" = \"SFC /Scannow\" \"Position\" = \"Bottom\" \"SubCommands\" = \"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\01Scannow] \"HasLUAShield\" = \"\" \"MUIVerb\" = \"Run SFC /Scannow\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\01Scannow\\command] @ = \"PowerShell -windowstyle hidden -command \\\"Start-Process cmd -ArgumentList '/s,/k, sfc.exe /scannow' -Verb runAs\\\"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\02ViewLog] \"MUIVerb\" = \"View log for SFC\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\02ViewLog\\command] @ = \"PowerShell (Select-String [SR] $env:windir\\\\Logs\\\\CBS\\\\CBS.log -s).Line >\\\"$env:userprofile\\\\Desktop\\\\SFC_LOG.txt\\\"\" DISM \u2691 Visit Microsoft's Official Docs on DISM - Link How to Fix Windows 10 using DISM (winaero.com) Deployment Image Servicing and Management (DISM.exe) is a command-line tool that can be used to service and prepare Windows images, including those used for Windows PE, Windows Recovery Environment (Windows RE) and Windows Setup. DISM can be used to service a Windows image (.wim) or a virtual hard disk (.vhd or.vhdx). The Component store is a core feature of Windows 10 which stores all of the files related to the OS grouped by components and as hardlinks. With new servicing model introduced in Vista, some system files are shared between two components and they are all hardlinked to the system32 folder. When the OS is serviced, the component store is updated. The Component Store is part of the Windows Imaging and Servicing stack. There is a special console tool called DISM which ships with Windows 10 by default. It can be used to fix Windows Component Store corruption. It is especially useful when the usual command \" sfc /scannow \" cannot repair damaged system files. The DISM tool writes the following log files: C:\\Windows\\Logs\\CBS\\CBS.log C:\\Windows\\Logs\\DISM\\dism.log They can be used to analyze errors and see completed operations. Add DISM to Desktop Context Menu: \u2691 Registry Script: Windows Registry Editor Version 5.00 [ HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\DismContextMenu] \"Icon\" = \"WmiPrvSE.exe\" \"MUIVerb\" = \"Repair Windows Image\" \"Position\" = \"Bottom\" \"SubCommands\" = \"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\CheckHealth] \"HasLUAShield\" = \"\" \"MUIVerb\" = \"Check Health of Windows Image\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\CheckHealth\\command] @ = \"PowerShell -windowstyle hidden -command \\\"Start-Process cmd -ArgumentList '/s,/k, Dism /Online /Cleanup-Image /CheckHealth' -Verb runAs\\\"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\RestoreHealth] \"HasLUAShield\" = \"\" \"MUIVerb\" = \"Repair Windows Image\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\RestoreHealth\\command] @ = \"PowerShell -windowstyle hidden -command \\\"Start-Process cmd -ArgumentList '/s,/k, Dism /Online /Cleanup-Image /RestoreHealth' -Verb runAs\\\"\" How it Works \u2691 Check Health of Windows Image. This command executes DISM as follows: Dism /Online /Cleanup-Image /CheckHealth . The key option here is CheckHealth. We use it to check if some process has marked the Component Store as corrupted and whether the corruption is repairable. This command is not supposed to fix any issues. It only reports about problems if they are present and if the CBS store is flagged. This command doesn't create a log file. Repair Windows Image . This command starts DISM with the following arguments: Dism /Online /Cleanup-Image /RestoreHealth . The DISM tool started with the /RestoreHealth option will scan the component store for corruption and perform the required repair operations automatically. It will create a log file. The whole process can take several hours, so be patient. On hard drives, it will take longer compared to an SSD. Both commands start elevated from PowerShell . Removing Registry Edits \u2691 To undo the Context Menu REGEDIT entries here\u2019s the scripts: Remove SFC Context Menu: Windows Registry Editor Version 5.00 [ - HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\SFCScannow] Remove DISM Context Menu: Windows Registry Editor Version 5.00 [ - HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\DismContextMenu] JIMMY BRIGGS - 2021","title":"SFC Scannow and DISM Commands"},{"location":"Windows/SFC%20and%20DISM%20Commands/#sfc-scannow-and-dism-commands","text":"On Windows 10, when you start noticing random errors, problems booting up, or features not working as expected, there's a good chance that one or multiple system files might have gone missing or corrupted for unknown reasons. Usually, problems with system files could occur as a result of an issue installing a system update, driver, or application, or while making changes to the installation manually. If you happen to come across this issue, you can use the Windows 10 System File Checker (SFC), which is a command-line tool designed to scan the integrity and restore missing or corrupted system files with working replacements. Use the System File Checker tool to repair damaged system files automatically or manually if the tool refuses to work. On windows these commands are crucial for keeping a clean system. I run these commands so often that I made a .reg registry entry script to add them to my Desktop\u2019s Context Menu (see below for details or visit my dotfiles repo for the actual .reg scripts to run: The System File Checker - sfc /scannow The Deployment Image Servicing and Management tool - DISM.exe NOTE: these must be ran as an ADMINISTRATOR # powershell or CMD as ADMIN # system file checker sfc / scannow # DISM Dism / Online / Cleanup-Image / ScanHealth Dism / Online / Cleanup-Image / CheckHealth Dism / Online / Cleanup-Image / RestoreHealth","title":"SFC Scannow and DISM Commands"},{"location":"Windows/SFC%20and%20DISM%20Commands/#sfc","text":"Visit Microsoft's Docs on SFC - Link The sfc /scannow command is a well known way to do an integrity check of all Windows 10 system files. sfc.exe is the System File Checker tool which can be helpful in many scenarios and fix various issues with Windows 10. You can save you time by adding a special context menu entry to launch it directly with one click.","title":"SFC"},{"location":"Windows/SFC%20and%20DISM%20Commands/#add-sfc-scannow-to-desktop-context-menu","text":"Registry Script: Windows Registry Editor Version 5.00 [ HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\SFCScannow] \"Icon\" = \"cmd.exe\" \"MUIVerb\" = \"SFC /Scannow\" \"Position\" = \"Bottom\" \"SubCommands\" = \"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\01Scannow] \"HasLUAShield\" = \"\" \"MUIVerb\" = \"Run SFC /Scannow\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\01Scannow\\command] @ = \"PowerShell -windowstyle hidden -command \\\"Start-Process cmd -ArgumentList '/s,/k, sfc.exe /scannow' -Verb runAs\\\"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\02ViewLog] \"MUIVerb\" = \"View log for SFC\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\SFCScannow\\shell\\02ViewLog\\command] @ = \"PowerShell (Select-String [SR] $env:windir\\\\Logs\\\\CBS\\\\CBS.log -s).Line >\\\"$env:userprofile\\\\Desktop\\\\SFC_LOG.txt\\\"\"","title":"Add SFC /Scannow to Desktop Context Menu:"},{"location":"Windows/SFC%20and%20DISM%20Commands/#dism","text":"Visit Microsoft's Official Docs on DISM - Link How to Fix Windows 10 using DISM (winaero.com) Deployment Image Servicing and Management (DISM.exe) is a command-line tool that can be used to service and prepare Windows images, including those used for Windows PE, Windows Recovery Environment (Windows RE) and Windows Setup. DISM can be used to service a Windows image (.wim) or a virtual hard disk (.vhd or.vhdx). The Component store is a core feature of Windows 10 which stores all of the files related to the OS grouped by components and as hardlinks. With new servicing model introduced in Vista, some system files are shared between two components and they are all hardlinked to the system32 folder. When the OS is serviced, the component store is updated. The Component Store is part of the Windows Imaging and Servicing stack. There is a special console tool called DISM which ships with Windows 10 by default. It can be used to fix Windows Component Store corruption. It is especially useful when the usual command \" sfc /scannow \" cannot repair damaged system files. The DISM tool writes the following log files: C:\\Windows\\Logs\\CBS\\CBS.log C:\\Windows\\Logs\\DISM\\dism.log They can be used to analyze errors and see completed operations.","title":"DISM"},{"location":"Windows/SFC%20and%20DISM%20Commands/#add-dism-to-desktop-context-menu","text":"Registry Script: Windows Registry Editor Version 5.00 [ HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\DismContextMenu] \"Icon\" = \"WmiPrvSE.exe\" \"MUIVerb\" = \"Repair Windows Image\" \"Position\" = \"Bottom\" \"SubCommands\" = \"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\CheckHealth] \"HasLUAShield\" = \"\" \"MUIVerb\" = \"Check Health of Windows Image\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\CheckHealth\\command] @ = \"PowerShell -windowstyle hidden -command \\\"Start-Process cmd -ArgumentList '/s,/k, Dism /Online /Cleanup-Image /CheckHealth' -Verb runAs\\\"\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\RestoreHealth] \"HasLUAShield\" = \"\" \"MUIVerb\" = \"Repair Windows Image\" [ HKEY_CLASSES_ROOT \\DesktopBackground\\shell\\DismContextMenu\\shell\\RestoreHealth\\command] @ = \"PowerShell -windowstyle hidden -command \\\"Start-Process cmd -ArgumentList '/s,/k, Dism /Online /Cleanup-Image /RestoreHealth' -Verb runAs\\\"\"","title":"Add DISM to Desktop Context Menu:"},{"location":"Windows/SFC%20and%20DISM%20Commands/#how-it-works","text":"Check Health of Windows Image. This command executes DISM as follows: Dism /Online /Cleanup-Image /CheckHealth . The key option here is CheckHealth. We use it to check if some process has marked the Component Store as corrupted and whether the corruption is repairable. This command is not supposed to fix any issues. It only reports about problems if they are present and if the CBS store is flagged. This command doesn't create a log file. Repair Windows Image . This command starts DISM with the following arguments: Dism /Online /Cleanup-Image /RestoreHealth . The DISM tool started with the /RestoreHealth option will scan the component store for corruption and perform the required repair operations automatically. It will create a log file. The whole process can take several hours, so be patient. On hard drives, it will take longer compared to an SSD. Both commands start elevated from PowerShell .","title":"How it Works"},{"location":"Windows/SFC%20and%20DISM%20Commands/#removing-registry-edits","text":"To undo the Context Menu REGEDIT entries here\u2019s the scripts: Remove SFC Context Menu: Windows Registry Editor Version 5.00 [ - HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\SFCScannow] Remove DISM Context Menu: Windows Registry Editor Version 5.00 [ - HKEY_CLASSES_ROOT \\DesktopBackground\\Shell\\DismContextMenu] JIMMY BRIGGS - 2021","title":"Removing Registry Edits"},{"location":"Windows/Using%20diskusage%20Command%20in%20Windows/","text":"Windows New Command Line Tool - diskusage \u2691 If you are running a Windows Insider Preview build (20277, 21277, or later) you can now utilize a great new Windows CLI feature: diskusage . Note that I am writing this as of December 17, 2020. Remember that this tool is still in development so some features aren't completely stable, but whatever, let's look into it! Using diskusage \u2691 Disk Usage allows you to check out how much space your drives are taking up using the Command Prompt. There are some other handy parameters for analysis, too. Open up your command prompt and run diskusage /? : As of can see the command comes fully loaded with a suite of flags, commands, and arguments. Caveat: Bytes Only \u2691 One caveat of the tool is that it currently only recognizes bytes , so if you wanted to search for file larger than 1.73 gigabytes you would have to input 1,857,573,355.52 bytes! I suggest using a simple tool to convert from GB/KB/MB \\<-> Bytes like a search engine's unit conversion tool , a dedicated website like Byte Converter or better yet a command line utility like humaize-bytes . For those who cannot remember the binary prefix multipliers here's a reference conversion table: When Disk Usage is fully functional, you'll be able to create specific configuration files that contain the search and analysis options you regularly use, saving you additional time when analyzing your data.","title":"Using diskusage Command in Windows"},{"location":"Windows/Using%20diskusage%20Command%20in%20Windows/#windows-new-command-line-tool-diskusage","text":"If you are running a Windows Insider Preview build (20277, 21277, or later) you can now utilize a great new Windows CLI feature: diskusage . Note that I am writing this as of December 17, 2020. Remember that this tool is still in development so some features aren't completely stable, but whatever, let's look into it!","title":"Windows New Command Line Tool - diskusage"},{"location":"Windows/Using%20diskusage%20Command%20in%20Windows/#using-diskusage","text":"Disk Usage allows you to check out how much space your drives are taking up using the Command Prompt. There are some other handy parameters for analysis, too. Open up your command prompt and run diskusage /? : As of can see the command comes fully loaded with a suite of flags, commands, and arguments.","title":"Using diskusage"},{"location":"Windows/Using%20diskusage%20Command%20in%20Windows/#caveat-bytes-only","text":"One caveat of the tool is that it currently only recognizes bytes , so if you wanted to search for file larger than 1.73 gigabytes you would have to input 1,857,573,355.52 bytes! I suggest using a simple tool to convert from GB/KB/MB \\<-> Bytes like a search engine's unit conversion tool , a dedicated website like Byte Converter or better yet a command line utility like humaize-bytes . For those who cannot remember the binary prefix multipliers here's a reference conversion table: When Disk Usage is fully functional, you'll be able to create specific configuration files that contain the search and analysis options you regularly use, saving you additional time when analyzing your data.","title":"Caveat: Bytes Only"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/","text":"WinGet CLI Settings \u2691 You can configure WinGet by editing the settings.json file. Running winget settings will open the file in the default json editor, if no editor is configured, notepad.exe is used. File Location \u2691 Settings file is located in %LOCALAPPDATA%\\Packages\\Microsoft.DesktopAppInstaller_8wekyb3d8bbwe\\LocalState\\settings.json If you are using the non-packaged WinGet version by building it from source code, the file will be located under %LOCALAPPDATA%\\Microsoft\\WinGet\\Settings\\settings.json Source \u2691 The source settings involve configuration to the WinGet source. \"source\": { \"autoUpdateIntervalInMinutes\": 3 }, autoUpdateIntervalInMinutes \u2691 A positive integer represents the update interval in minutes. The check for updates only happens when a source is used. A zero will disable the check for updates to a source. Any other values are invalid. Disable: 0 Default: 5 To manually update the source use winget source update Visual \u2691 The visual settings involve visual elements that are displayed by WinGet \"visual\": { \"progressBar\": \"accent\" }, progressBar \u2691 Color of the progress bar that WinGet displays when not specified by arguments. accent (default) retro rainbow Experimental Features \u2691 To allow work to be done and distributed to early adopters for feedback, settings can be used to enable \"experimental\" features. The experimentalFeatures settings involve the configuration of these \"experimental\" features. Individual features can be enabled under this node. The example below shows sample experimental features. \"experimentalFeatures\": { \"experimentalCmd\": true, \"experimentalArg\": false }, experimentalMSStore \u2691 Microsoft Store App support in WinGet is currently implemented as an experimental feature. It supports a curated list of utility apps from Microsoft Store. You can enable the feature as shown below. \"experimentalFeatures\": { \"experimentalMSStore\": true }, list \u2691 While work is in progress on list, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"list\": true }, upgrade \u2691 While work is in progress on upgrade, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"upgrade\": true }, uninstall \u2691 While work is in progress on uninstall, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"uninstall\": true }, import \u2691 While work is in progress for import, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"import\": true }, restSource \u2691 While work is in progress for rest source support, the feature is hidden behind a feature toggle. Enabling this will not change how client works currently and will allow testing any additional rest sources added. One can enable it as below: \"experimentalFeatures\": { \"restSource\": true },","title":"WinGet CLI Settings"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#winget-cli-settings","text":"You can configure WinGet by editing the settings.json file. Running winget settings will open the file in the default json editor, if no editor is configured, notepad.exe is used.","title":"WinGet CLI Settings"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#file-location","text":"Settings file is located in %LOCALAPPDATA%\\Packages\\Microsoft.DesktopAppInstaller_8wekyb3d8bbwe\\LocalState\\settings.json If you are using the non-packaged WinGet version by building it from source code, the file will be located under %LOCALAPPDATA%\\Microsoft\\WinGet\\Settings\\settings.json","title":"File Location"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#source","text":"The source settings involve configuration to the WinGet source. \"source\": { \"autoUpdateIntervalInMinutes\": 3 },","title":"Source"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#autoupdateintervalinminutes","text":"A positive integer represents the update interval in minutes. The check for updates only happens when a source is used. A zero will disable the check for updates to a source. Any other values are invalid. Disable: 0 Default: 5 To manually update the source use winget source update","title":"autoUpdateIntervalInMinutes"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#visual","text":"The visual settings involve visual elements that are displayed by WinGet \"visual\": { \"progressBar\": \"accent\" },","title":"Visual"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#progressbar","text":"Color of the progress bar that WinGet displays when not specified by arguments. accent (default) retro rainbow","title":"progressBar"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#experimental-features","text":"To allow work to be done and distributed to early adopters for feedback, settings can be used to enable \"experimental\" features. The experimentalFeatures settings involve the configuration of these \"experimental\" features. Individual features can be enabled under this node. The example below shows sample experimental features. \"experimentalFeatures\": { \"experimentalCmd\": true, \"experimentalArg\": false },","title":"Experimental Features"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#experimentalmsstore","text":"Microsoft Store App support in WinGet is currently implemented as an experimental feature. It supports a curated list of utility apps from Microsoft Store. You can enable the feature as shown below. \"experimentalFeatures\": { \"experimentalMSStore\": true },","title":"experimentalMSStore"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#list","text":"While work is in progress on list, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"list\": true },","title":"list"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#upgrade","text":"While work is in progress on upgrade, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"upgrade\": true },","title":"upgrade"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#uninstall","text":"While work is in progress on uninstall, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"uninstall\": true },","title":"uninstall"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#import","text":"While work is in progress for import, the command is hidden behind a feature toggle. One can enable it as below: \"experimentalFeatures\": { \"import\": true },","title":"import"},{"location":"Windows/WinGet%20CLI%20Setup%20and%20Settings/#restsource","text":"While work is in progress for rest source support, the feature is hidden behind a feature toggle. Enabling this will not change how client works currently and will allow testing any additional rest sources added. One can enable it as below: \"experimentalFeatures\": { \"restSource\": true },","title":"restSource"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/","text":"Windows Command Prompt \u2691 Command Line - Getting started with CMD \u2691 cmd documentation: Getting started with cmd This section provides an overview of what cmd is, and why a developer might want to use it. It should also mention any large subjects within cmd, and link out to the related topics. Since the Documentation for cmd is new, you may need to create initial versions of those related topics. Commands in CMD # The available commands will be displayed, including a brief description, in tabular format. In Windows 10 the following commands are listed: Command Description ASSOC Displays or modifies file extension associations. ATTRIB Displays or changes file attributes. BREAK Sets or clears extended CTRL+C checking. BCDEDIT Sets properties in boot database to control boot loading. CACLS Displays or modifies access control lists (ACLs) of files. CALL Calls one batch program from another. CD Displays the name of or changes the current directory. CHCP Displays or sets the active code page number. CHDIR Displays the name of or changes the current directory. CHKDSK Checks a disk and displays a status report. CHKNTFS Displays or modifies the checking of disk at boot time. CLS Clears the screen. CMD Starts a new instance of the Windows command interpreter. COLOR Sets the default console foreground and background colors. COMP Compares the contents of two files or sets of files. COMPACT Displays or alters the compression of files on NTFS partitions. CONVERT Converts FAT volumes to NTFS. You cannot convert the current drive. COPY Copies one or more files to another location. DATE Displays or sets the date. DEL Deletes one or more files. DIR Displays a list of files and subdirectories in a directory. DISKPART Displays or configures Disk Partition properties. DOSKEY Edits command lines, recalls Windows commands, and creates macros. DRIVERQUERY Displays current device driver status and properties. ECHO Displays messages, or turns command echoing on or off. ENDLOCAL Ends localization of environment changes in a batch file. ERASE Deletes one or more files. EXIT Quits the CMD.EXE program (command interpreter). FC Compares two files or sets of files, and displays the differences between them. FIND Searches for a text string in a file or files. FINDSTR Searches for strings in files. FOR Runs a specified command for each file in a set of files. FORMAT Formats a disk for use with Windows. FSUTIL Displays or configures the file system properties. FTYPE Displays or modifies file types used in file extension associations. GOTO Directs the Windows command interpreter to a labeled line in a batch program. GPRESULT Displays Group Policy information for machine or user. GRAFTABL Enables Windows to display an extended character set in graphics mode. HELP Provides Help information for Windows commands. ICACLS Display, modify, backup, or restore ACLs for files and directories. IF Performs conditional processing in batch programs. LABEL Creates, changes, or deletes the volume label of a disk. MD Creates a directory. MKDIR Creates a directory. MKLINK Creates Symbolic Links and Hard Links MODE Configures a system device. MORE Displays output one screen at a time. MOVE Moves one or more files from one directory to another directory. OPENFILES Displays files opened by remote users for a file share. PATH Displays or sets a search path for executable files. PAUSE Suspends processing of a batch file and displays a message. POPD Restores the previous value of the current directory saved by PUSHD. PRINT Prints a text file. PROMPT Changes the Windows command prompt. PUSHD Saves the current directory then changes it. RD Removes a directory. RECOVER Recovers readable information from a bad or defective disk. REM Records comments (remarks) in batch files or CONFIG.SYS. REN Renames a file or files. RENAME Renames a file or files. REPLACE Replaces files. RMDIR Removes a directory. ROBOCOPY Advanced utility to copy files and directory trees SET Displays, sets, or removes Windows environment variables. SETLOCAL Begins localization of environment changes in a batch file. SC Displays or configures services (background processes). SCHTASKS Schedules commands and programs to run on a computer. SHIFT Shifts the position of replaceable parameters in batch files. SHUTDOWN Allows proper local or remote shutdown of machine. SORT Sorts input. START Starts a separate window to run a specified program or command. SUBST Associates a path with a drive letter. SYSTEMINFO Displays machine specific properties and configuration. TASKLIST Displays all currently running tasks including services. TASKKILL Kill or stop a running process or application. TIME Displays or sets the system time. TITLE Sets the window title for a CMD.EXE session. TREE Graphically displays the directory structure of a drive or path. TYPE Displays the contents of a text file. VER Displays the Windows version. VERIFY Tells Windows whether to verify that your files are written correctly to a disk. VOL Displays a disk volume label and serial number. XCOPY Copies files and directory trees. WMIC Displays WMI information inside interactive command shell. To get more insight about a specific command use the /? option, e.g. the tree command gives: tree / ? Graphically displays the folder structure of a drive or path . TREE [ drive: ][ path ] [ /F ] [ /A ] / F Display the names of the files in each folder . / A Use ASCII instead of extended characters . Features # Microsoft Command Prompt is a command-line interpreter (CLI) for the Windows operating systems. A CLI is program intended primarily to read operating system instructions typed on a keyboard by the user. It is therefore addressed also as a command-line interface , to contrast it with graphical interfaces. As these interfaces (whether textual or graphical) shield the user from directly accessing to the operating system kernel, they are also said shells . Given the name of the Command Prompt executable file, cmd.exe , the Command Prompt is friendly named cmd . Given its OS piloting role, it is also said the console . Like other shells, cmd can read batch of instructions from a file. In this case the cmd shell acts as a language interpreter and the file content can be regarded as an actual program. When executing these batch programs, there is no intermediate compilation phase. They are typically read, interpreted and executed line by line. Since there is no compilation, there is no production of a separated executable file. For this reason the programs are denoted batch scripts or shell scripts . Note that the instructions entered interactively might have a slightly different syntax from those submitted as a script, but the general principle is that what can be entered from the command line can be also put in a file for later reuse. Hello World # Command Prompt batch scripts have extension .cmd or .bat , the latter for compatibility reasons. To create a hello-word-script, you first need a place where to type it. For simple scripts, also the Windows Notepad will do. If you are serious about shell scripting, you need more effective tools. There are anyway several free alternatives, such as Notepad++ . In your designated editor type: echo Hello World pause Save it as hello.cmd If you are using \"Notepad\" as an editor, you should pay much attention to the saved name, as Notepad tends to add always a .txt extension to your files, which means that the actual name of your file might be hello.cmd.txt . To avoid this, in the save dialog box: In the File name field enter the name in double quotes, e.g. \"hello.cmd\" In the Save as type field select All Files, instead of the default Text Document option. If the file has been saved properly, its icon should be similar to (Windows Vista): You may also consider to disable the option \"Hide extension for known file types\" in File Explorer folder view options. In this case, file names are always displayed with their extensions. To execute hello.cmd there are two possibilities. If you are using the Windows graphical shell, just double click on its icon. If you want to use the Command Prompt itself, you must first identify the directory where you saved hello.cmd . In this regard, if you open File Explorer with +E. In the windows listing files, you normally read the name of the directory path containing them. You can therefore identify the directory of hello.cmd . Windows directory names tend to be quite long and typing them is error prone. It is better if you select and copy the directory path in the clipboard for later pasting. Start the Command Prompt. You read a line similar to this. Microsoft Windows [Version ...] (c) ... Microsoft Corporation. All rights reserved. C:\\Users\\...> The version/year of Windows of course depends on yours. In the the final line, before > , you read the path of the directory which is current. You should make current the directory where your script is. For this reason enter the change directory command cd , using a line similar to the following: cd <dirpath> Instead of <dirpath> , paste the name of the directory you previously copied. To paste the directory path, in Windows 10, you just need to type Ctrl-C, as you would in an editor. For older systems you should be able to do this by right clicking in the cmd window. After entering the command, note that current path, before > , changes accordingly. You can now run your hello script by simply entering: hello Comments # The script prints an output similar to: C :\\ Users \\... > echo Hello World Hello World C :\\ Users \\... > pause Press any key to continue . . . The lines hosting the symbol > restate the script instructions as if you had entered interactively. This can be disabled writing: @echo off as the first line of your script. This might reduce the clutter, but you have less hints on what is going on, with respect to those script commands that do not give visible outputs. The last command, pause , prompts you to hit any key. When you do, you exit hello . If you run hello from the console, you don't really need it, because, when hello terminates its execution, cmd.exe remains open and you can to read hello output. When double-clicking in Explorer, you start cmd.exe for the time necessary to execute hello . When hello terminates, cmd.exe does the same and you have no possibility to read hello output. pause command prevents hello from exiting until you hit a key, which gives also the possibility to read the output. Finally, despite the name of the script is hello.cmd , it is not necessary to type the whole name, its hello stem is sufficient. This mechanism works for executables too, with extension .exe . What if there is a script hello.cmd and an executable hello.exe in the same directory? The former has priority in the Command Prompt, so hello.cmd will be executed. Navigating in cmd # One of the most common things you'll need to do in the command prompt is navigate your file system. To do this, we'll utilize the cd and dir keywords. Start by opening up a command prompt using one of the methods mentioned here . You most likely see something similar to what's below, where UserName is your user. C:\\Users\\UserName> Regardless of where in your file structure you are, if your system is like most, we can start with this command: cd C:\\ This will change your current directory to the C:\\ drive. Notice how the screen now looks like this C:\\> Next, run a dir so we can see anything in the C:\\ drive dir This will show you a list of files and folders with some information about them, similar to this: There's lots of good info here, but for basic navigation, we just care about the right-most column. Notice how we have a Users folder. That means we can run this cd Users Now if you run dir again, you'll see all the files and folders in your C:\\Users directory. Now, we didn't find what we wanted here, so let's go back to the parent folder. Rather than type the path to it, we can use .. to go up one folder like so cd .. Now we are back in C:\\ . If you want to go up multiple folders at once, you can put a backslash and another set of periods like so: cd ..\\.. , but we only needed one folder. Now we want to look in that Program Files folder. To avoid confusing the system, it's a good idea to put quotes around the directories, especially when there are spaces in the name. So this time, we'll use this command C:\\>cd \"Program Files\" Now you are in C:\\Program Files> and a dir command now will tell you anything that's in here. So, say we get tired of wandering around to find the folder and looked up exactly where we were needing to go. Turns out it's C:\\Windows\\Logs Rather than do a .. to Windows to Logs , we can just put the full path like so: cd \"C:\\Windows\\Logs\" And that's the basics of navigating the command prompt. You can now move through all your folders so you can run your other commands in the proper places. Opening a Command Prompt # The command prompt comes pre-installed on all Windows NT, Windows CE, OS/2 and eComStation operating systems, and exists as cmd.exe , typically located in C:\\Windows\\system32\\cmd.exe On Windows 7 the fastest ways to open the command prompt are: Press , type \"cmd\" and then press Enter. Press +R, type \"cmd\" then then press Enter. It can also be opened by navigating to the executable and double-clicking on it. In some cases you might need to run cmd with elevated permissions, in this case right click and select \"Run as administrator\". This can also be achieved by pressing Control+ Shift+Enter instead of Enter. Source Must Know Commands \u2691 Assoc \u2691 Most files in Windows are associated with a specific program that is assigned to open the file by default. At times, remembering these associations can become confusing. You can remind yourself by entering the command assoc to display a full list of file name extensions and program associations. You can also extend the command to change file associations. For example, assoc .txt= will change the file association for text files to whatever program you enter after the equal sign. The Assoc command itself will reveal both the extension names and program names, which will help you properly use this command. In Windows 10, you can view a more user-friendly interface that also lets you change file type associations on the spot. Head to Settings (Windows + I) > Apps > Default apps > Choose default app by file type . Cipher \u2691 Deleting files on a mechanical hard drive doesn't really delete them at all. Instead, it marks the files as no longer accessible and the space they took up as free. The files remain recoverable until the system overwrites them with new data, which can take some time. The cipher command, however, wipes a directory by writing random data to it. To wipe your C drive, for example, you'd use the cipher /w:d command, which will wipe free space on the drive. The command does not overwrite undeleted data, so you will not wipe out files you need by running this command. You can use a host of other cipher commands, however, they are generally redundant with BitLocker enabled versions of Windows . Driverquery \u2691 Drivers remain among the most important software installed on a PC. Improperly configured or missing drivers can cause all sorts of trouble, so its good to have access to a list of what's on your PC. That's exactly what the driverquery command does. You can extend it to driverquery -v to obtain more information, including the directory in which the driver is installed. For best use, I like to pipe the verbose output ( -v ) into a CSV ( -fo \"csv\" ) via: driverquery -v -fo \"csv\" >> My-Drivers.csv File Compare \u2691 You can use this command to identify differences in text between two files. It's particularly useful for writers and programmers trying to find small changes between two versions of a file. Simply type fc and then the directory path and file name of the two files you want to compare. You can also extend the command in several ways. Typing /b compares only binary output, /c disregards the case of text in the comparison, and /l only compares ASCII text. So, for example, you could use the following: fc /l \"C:\\\\Program Files (x86)\\\\example1.doc\" \"C:\\\\Program Files (x86)\\\\example2.doc\" The above command compares ASCII text in two word documents. ipconfig \u2691 This command relays the IP address that your computer is currently using. However, if you're behind a router (like most computers today), you'll instead receive the local network address of the router. Still, ipconfig is useful because of its extensions. ipconfig /release followed by ipconfig /renew can force your Windows PC into asking for a new IP address, which is useful if your computer claims one isn't available. You can also use ipconfig /flushdns to refresh your DNS address. These commands are great if the Windows network troubleshooter chokes, which does happen on occasion. Netstat \u2691 Entering the command netstat -an will provide you with a list of currently open ports and related IP addresses. This command will also tell you what state the port is in; listening, established, or closed. This is a great command for when you're trying to troubleshoot devices connected to your PC or when you fear a Trojan infected your system and you're trying to locate a malicious connection. Ping \u2691 Sometimes, you need to know whether or not packets are making it to a specific networked device. That's where ping comes in handy. Typing ping followed by an IP address or web domain will send a series of test packets to the specified address. If they arrive and are returned, you know the device is capable of communicating with your PC; if it fails, you know that there's something blocking communication between the device and your computer. This can help you decide if the root of the issue is an improper configuration or a failure of network hardware. PathPing \u2691 This is a more advanced version of ping that's useful if there are multiple routers between your PC and the device you're testing. Like ping, you use this command by typing pathping followed by the IP address, but unlike ping, pathping also relays some information about the route the test packets take. Tracert \u2691 The tracert command is similar to pathping. Once again, type tracert followed by the IP address or domain you'd like to trace. You'll receive information about each step in the route between your PC and the target. Unlike pathping, however, tracert also tracks how much time (in milliseconds) each hop between servers or devices takes. Powercfg \u2691 Powercfg is a very powerful command for managing and tracking how your computer uses energy. You can use the command powercfg hibernate on and powercfg hibernate off to manage hibernation, and you can also use the command powercfg /a to view the power-saving states currently available on your PC. Another useful command is powercfg /devicequery s1_supported , which displays a list of devices on your computer that support connected standby. When enabled, you can use these devices to bring your computer out of standby, even remotely. You can enable this by selecting the device in Device Manager , opening its properties, going to the Power Management tab, and then checking the Allow this device to wake the computer box. Powercfg /lastwake will show you what device last woke your PC from a sleep state. You can use this command to troubleshoot your PC if it seems to wake from sleep at random. You can use the powercfg /energy command to build a detailed power consumption report for your PC. The report saves to the directory indicated after the command finishes. This report will let you know of any system faults that might increase power consumption, like devices blocking certain sleep modes, or poorly configured to respond to your power management settings. Windows 8 added powercfg /batteryreport , which provides a detailed analysis of battery use, if applicable. Normally output to your Windows user directory, the report provides details about the time and length of charge and discharge cycles, lifetime average battery life, and estimated battery capacity. Shutdown \u2691 Windows 8 introduced the shutdown command that, you guessed it, shuts down your computer . This is, of course, redundant with the already easily accessed shutdown button, but what's not redundant is the shutdown /r /o command, which restarts your PC and launches the Advanced Start Options menu, which is where you can access Safe Mode and Windows recovery utilities. This is useful if you want to restart your computer for troubleshooting purposes. Systeminfo \u2691 This command will give you a detailed configuration overview of your computer. The list covers your operating system and hardware. For example, you can look up the original Windows installation date, the last boot time, your BIOS version, total and available memory, installed hotfixes, network card configurations, and more. Use systeminfo /s followed by the host name of a computer on your local network, to remotely grab the information for that system. This may require additional syntax elements for the domain, user name, and password, like this: systeminfo / s [ host_name ] / u [ domain ] \\ [ user_name ] / p [ user_password ] System File Checker \u2691 System File Checker is an automatic scan and repair tool that focuses on Windows system files. You will need to run the command prompt with administrator privileges and enter the command sfc /scannow . If SFC finds any corrupt or missing files, it will automatically replace them using cached copies kept by Windows for this purpose alone. The command can require a half-hour to run on older notebooks. Tasklist \u2691 You can use the tasklist command to provide a current list of all tasks running on your PC. Though somewhat redundant with Task Manager, the command may sometimes find tasks hidden from view in that utility. There's also a wide range of modifiers. Tasklist -svc shows services related to each task, use tasklist -v to obtain more detail on each task, and tasklist -m will locate DLL files associated with active tasks. These commands are useful for advanced troubleshooting. Our reader Eric noted that you can \"get the name of the executable associated with the particular process ID you're interested in.\" The command for that operation is tasklist | find [process id]. Taskkill \u2691 Tasks that appear in the tasklist command will have an executable and process ID (a four- or five-digit number) associated with them. You can force stop a program using taskkill -im followed by the executable's name, or taskkill -pid followed by the process ID. Again, this is a bit redundant with Task Manager, but you can use it to kill otherwise unresponsive or hidden programs. Chkdsk \u2691 Windows automatically marks your drive for a diagnostic chkdsk scan when symptoms indicate that a local drive has bad sectors, lost clusters, or other logical or physical errors. If you suspect your hard drive is failing, you can manually initiate a scan. The most basic command is chkdsk c: , which will immediately scan the C: drive, without a need to restart the computer. If you add parameters like /f, /r, /x, or /b, such as in chkdsk /f /r /x /b c: , chkdsk will also fix errors, recover data, dismount the drive, or clear the list of bad sectors, respectively. These actions require a reboot, as they can only run with Windows powered down. If you see chkdsk run at startup, let it do its thing. If it gets stuck, however, refer to the chkdsk troubleshooting article . schtasks \u2691 Schtasks is your command prompt access to the Task Scheduler, one of many underrated Windows administrative tools. While you can use the GUI to manage your scheduled tasks, the command prompt lets you copy&paste complex commands to set up multiple similar tasks without having to click through various options. Ultimately, it's much easier to use, once you've committed key parameters to memory. For example, you could schedule your computer to reboot at 11pm every Friday: schtasks /create /sc weekly /d FRI /tn \"auto reboot computer weekly\" /st 23:00 /tr \"shutdown -r -f -t 10\" To complement your weekly reboot, you could schedule tasks to launch specific programs on startup: schtasks /create /sc onstart /tn \"launch Chrome on startup\" /tr \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\Chrome.exe\" To duplicate the above command for different programs, just copy, paste, and modify it as needed. Command and Conquer Your Windows PC \u2691 This article can only give you a taste of what's hidden within the Windows command line. When including all variables, there are literally hundreds of commands. Download Microsoft's command line reference guide (in Edge or Internet Explorer) for advanced support and troubleshooting. Tired of the command prompt? Time to try the new Windows Terminal !","title":"Windows Command Prompt"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#windows-command-prompt","text":"","title":"Windows Command Prompt"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#command-line-getting-started-with-cmd","text":"cmd documentation: Getting started with cmd This section provides an overview of what cmd is, and why a developer might want to use it. It should also mention any large subjects within cmd, and link out to the related topics. Since the Documentation for cmd is new, you may need to create initial versions of those related topics. Commands in CMD # The available commands will be displayed, including a brief description, in tabular format. In Windows 10 the following commands are listed: Command Description ASSOC Displays or modifies file extension associations. ATTRIB Displays or changes file attributes. BREAK Sets or clears extended CTRL+C checking. BCDEDIT Sets properties in boot database to control boot loading. CACLS Displays or modifies access control lists (ACLs) of files. CALL Calls one batch program from another. CD Displays the name of or changes the current directory. CHCP Displays or sets the active code page number. CHDIR Displays the name of or changes the current directory. CHKDSK Checks a disk and displays a status report. CHKNTFS Displays or modifies the checking of disk at boot time. CLS Clears the screen. CMD Starts a new instance of the Windows command interpreter. COLOR Sets the default console foreground and background colors. COMP Compares the contents of two files or sets of files. COMPACT Displays or alters the compression of files on NTFS partitions. CONVERT Converts FAT volumes to NTFS. You cannot convert the current drive. COPY Copies one or more files to another location. DATE Displays or sets the date. DEL Deletes one or more files. DIR Displays a list of files and subdirectories in a directory. DISKPART Displays or configures Disk Partition properties. DOSKEY Edits command lines, recalls Windows commands, and creates macros. DRIVERQUERY Displays current device driver status and properties. ECHO Displays messages, or turns command echoing on or off. ENDLOCAL Ends localization of environment changes in a batch file. ERASE Deletes one or more files. EXIT Quits the CMD.EXE program (command interpreter). FC Compares two files or sets of files, and displays the differences between them. FIND Searches for a text string in a file or files. FINDSTR Searches for strings in files. FOR Runs a specified command for each file in a set of files. FORMAT Formats a disk for use with Windows. FSUTIL Displays or configures the file system properties. FTYPE Displays or modifies file types used in file extension associations. GOTO Directs the Windows command interpreter to a labeled line in a batch program. GPRESULT Displays Group Policy information for machine or user. GRAFTABL Enables Windows to display an extended character set in graphics mode. HELP Provides Help information for Windows commands. ICACLS Display, modify, backup, or restore ACLs for files and directories. IF Performs conditional processing in batch programs. LABEL Creates, changes, or deletes the volume label of a disk. MD Creates a directory. MKDIR Creates a directory. MKLINK Creates Symbolic Links and Hard Links MODE Configures a system device. MORE Displays output one screen at a time. MOVE Moves one or more files from one directory to another directory. OPENFILES Displays files opened by remote users for a file share. PATH Displays or sets a search path for executable files. PAUSE Suspends processing of a batch file and displays a message. POPD Restores the previous value of the current directory saved by PUSHD. PRINT Prints a text file. PROMPT Changes the Windows command prompt. PUSHD Saves the current directory then changes it. RD Removes a directory. RECOVER Recovers readable information from a bad or defective disk. REM Records comments (remarks) in batch files or CONFIG.SYS. REN Renames a file or files. RENAME Renames a file or files. REPLACE Replaces files. RMDIR Removes a directory. ROBOCOPY Advanced utility to copy files and directory trees SET Displays, sets, or removes Windows environment variables. SETLOCAL Begins localization of environment changes in a batch file. SC Displays or configures services (background processes). SCHTASKS Schedules commands and programs to run on a computer. SHIFT Shifts the position of replaceable parameters in batch files. SHUTDOWN Allows proper local or remote shutdown of machine. SORT Sorts input. START Starts a separate window to run a specified program or command. SUBST Associates a path with a drive letter. SYSTEMINFO Displays machine specific properties and configuration. TASKLIST Displays all currently running tasks including services. TASKKILL Kill or stop a running process or application. TIME Displays or sets the system time. TITLE Sets the window title for a CMD.EXE session. TREE Graphically displays the directory structure of a drive or path. TYPE Displays the contents of a text file. VER Displays the Windows version. VERIFY Tells Windows whether to verify that your files are written correctly to a disk. VOL Displays a disk volume label and serial number. XCOPY Copies files and directory trees. WMIC Displays WMI information inside interactive command shell. To get more insight about a specific command use the /? option, e.g. the tree command gives: tree / ? Graphically displays the folder structure of a drive or path . TREE [ drive: ][ path ] [ /F ] [ /A ] / F Display the names of the files in each folder . / A Use ASCII instead of extended characters . Features # Microsoft Command Prompt is a command-line interpreter (CLI) for the Windows operating systems. A CLI is program intended primarily to read operating system instructions typed on a keyboard by the user. It is therefore addressed also as a command-line interface , to contrast it with graphical interfaces. As these interfaces (whether textual or graphical) shield the user from directly accessing to the operating system kernel, they are also said shells . Given the name of the Command Prompt executable file, cmd.exe , the Command Prompt is friendly named cmd . Given its OS piloting role, it is also said the console . Like other shells, cmd can read batch of instructions from a file. In this case the cmd shell acts as a language interpreter and the file content can be regarded as an actual program. When executing these batch programs, there is no intermediate compilation phase. They are typically read, interpreted and executed line by line. Since there is no compilation, there is no production of a separated executable file. For this reason the programs are denoted batch scripts or shell scripts . Note that the instructions entered interactively might have a slightly different syntax from those submitted as a script, but the general principle is that what can be entered from the command line can be also put in a file for later reuse. Hello World # Command Prompt batch scripts have extension .cmd or .bat , the latter for compatibility reasons. To create a hello-word-script, you first need a place where to type it. For simple scripts, also the Windows Notepad will do. If you are serious about shell scripting, you need more effective tools. There are anyway several free alternatives, such as Notepad++ . In your designated editor type: echo Hello World pause Save it as hello.cmd If you are using \"Notepad\" as an editor, you should pay much attention to the saved name, as Notepad tends to add always a .txt extension to your files, which means that the actual name of your file might be hello.cmd.txt . To avoid this, in the save dialog box: In the File name field enter the name in double quotes, e.g. \"hello.cmd\" In the Save as type field select All Files, instead of the default Text Document option. If the file has been saved properly, its icon should be similar to (Windows Vista): You may also consider to disable the option \"Hide extension for known file types\" in File Explorer folder view options. In this case, file names are always displayed with their extensions. To execute hello.cmd there are two possibilities. If you are using the Windows graphical shell, just double click on its icon. If you want to use the Command Prompt itself, you must first identify the directory where you saved hello.cmd . In this regard, if you open File Explorer with +E. In the windows listing files, you normally read the name of the directory path containing them. You can therefore identify the directory of hello.cmd . Windows directory names tend to be quite long and typing them is error prone. It is better if you select and copy the directory path in the clipboard for later pasting. Start the Command Prompt. You read a line similar to this. Microsoft Windows [Version ...] (c) ... Microsoft Corporation. All rights reserved. C:\\Users\\...> The version/year of Windows of course depends on yours. In the the final line, before > , you read the path of the directory which is current. You should make current the directory where your script is. For this reason enter the change directory command cd , using a line similar to the following: cd <dirpath> Instead of <dirpath> , paste the name of the directory you previously copied. To paste the directory path, in Windows 10, you just need to type Ctrl-C, as you would in an editor. For older systems you should be able to do this by right clicking in the cmd window. After entering the command, note that current path, before > , changes accordingly. You can now run your hello script by simply entering: hello Comments # The script prints an output similar to: C :\\ Users \\... > echo Hello World Hello World C :\\ Users \\... > pause Press any key to continue . . . The lines hosting the symbol > restate the script instructions as if you had entered interactively. This can be disabled writing: @echo off as the first line of your script. This might reduce the clutter, but you have less hints on what is going on, with respect to those script commands that do not give visible outputs. The last command, pause , prompts you to hit any key. When you do, you exit hello . If you run hello from the console, you don't really need it, because, when hello terminates its execution, cmd.exe remains open and you can to read hello output. When double-clicking in Explorer, you start cmd.exe for the time necessary to execute hello . When hello terminates, cmd.exe does the same and you have no possibility to read hello output. pause command prevents hello from exiting until you hit a key, which gives also the possibility to read the output. Finally, despite the name of the script is hello.cmd , it is not necessary to type the whole name, its hello stem is sufficient. This mechanism works for executables too, with extension .exe . What if there is a script hello.cmd and an executable hello.exe in the same directory? The former has priority in the Command Prompt, so hello.cmd will be executed. Navigating in cmd # One of the most common things you'll need to do in the command prompt is navigate your file system. To do this, we'll utilize the cd and dir keywords. Start by opening up a command prompt using one of the methods mentioned here . You most likely see something similar to what's below, where UserName is your user. C:\\Users\\UserName> Regardless of where in your file structure you are, if your system is like most, we can start with this command: cd C:\\ This will change your current directory to the C:\\ drive. Notice how the screen now looks like this C:\\> Next, run a dir so we can see anything in the C:\\ drive dir This will show you a list of files and folders with some information about them, similar to this: There's lots of good info here, but for basic navigation, we just care about the right-most column. Notice how we have a Users folder. That means we can run this cd Users Now if you run dir again, you'll see all the files and folders in your C:\\Users directory. Now, we didn't find what we wanted here, so let's go back to the parent folder. Rather than type the path to it, we can use .. to go up one folder like so cd .. Now we are back in C:\\ . If you want to go up multiple folders at once, you can put a backslash and another set of periods like so: cd ..\\.. , but we only needed one folder. Now we want to look in that Program Files folder. To avoid confusing the system, it's a good idea to put quotes around the directories, especially when there are spaces in the name. So this time, we'll use this command C:\\>cd \"Program Files\" Now you are in C:\\Program Files> and a dir command now will tell you anything that's in here. So, say we get tired of wandering around to find the folder and looked up exactly where we were needing to go. Turns out it's C:\\Windows\\Logs Rather than do a .. to Windows to Logs , we can just put the full path like so: cd \"C:\\Windows\\Logs\" And that's the basics of navigating the command prompt. You can now move through all your folders so you can run your other commands in the proper places. Opening a Command Prompt # The command prompt comes pre-installed on all Windows NT, Windows CE, OS/2 and eComStation operating systems, and exists as cmd.exe , typically located in C:\\Windows\\system32\\cmd.exe On Windows 7 the fastest ways to open the command prompt are: Press , type \"cmd\" and then press Enter. Press +R, type \"cmd\" then then press Enter. It can also be opened by navigating to the executable and double-clicking on it. In some cases you might need to run cmd with elevated permissions, in this case right click and select \"Run as administrator\". This can also be achieved by pressing Control+ Shift+Enter instead of Enter. Source","title":"Command Line - Getting started with CMD"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#must-know-commands","text":"","title":"Must Know Commands"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#assoc","text":"Most files in Windows are associated with a specific program that is assigned to open the file by default. At times, remembering these associations can become confusing. You can remind yourself by entering the command assoc to display a full list of file name extensions and program associations. You can also extend the command to change file associations. For example, assoc .txt= will change the file association for text files to whatever program you enter after the equal sign. The Assoc command itself will reveal both the extension names and program names, which will help you properly use this command. In Windows 10, you can view a more user-friendly interface that also lets you change file type associations on the spot. Head to Settings (Windows + I) > Apps > Default apps > Choose default app by file type .","title":"Assoc"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#cipher","text":"Deleting files on a mechanical hard drive doesn't really delete them at all. Instead, it marks the files as no longer accessible and the space they took up as free. The files remain recoverable until the system overwrites them with new data, which can take some time. The cipher command, however, wipes a directory by writing random data to it. To wipe your C drive, for example, you'd use the cipher /w:d command, which will wipe free space on the drive. The command does not overwrite undeleted data, so you will not wipe out files you need by running this command. You can use a host of other cipher commands, however, they are generally redundant with BitLocker enabled versions of Windows .","title":"Cipher"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#driverquery","text":"Drivers remain among the most important software installed on a PC. Improperly configured or missing drivers can cause all sorts of trouble, so its good to have access to a list of what's on your PC. That's exactly what the driverquery command does. You can extend it to driverquery -v to obtain more information, including the directory in which the driver is installed. For best use, I like to pipe the verbose output ( -v ) into a CSV ( -fo \"csv\" ) via: driverquery -v -fo \"csv\" >> My-Drivers.csv","title":"Driverquery"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#file-compare","text":"You can use this command to identify differences in text between two files. It's particularly useful for writers and programmers trying to find small changes between two versions of a file. Simply type fc and then the directory path and file name of the two files you want to compare. You can also extend the command in several ways. Typing /b compares only binary output, /c disregards the case of text in the comparison, and /l only compares ASCII text. So, for example, you could use the following: fc /l \"C:\\\\Program Files (x86)\\\\example1.doc\" \"C:\\\\Program Files (x86)\\\\example2.doc\" The above command compares ASCII text in two word documents.","title":"File Compare"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#ipconfig","text":"This command relays the IP address that your computer is currently using. However, if you're behind a router (like most computers today), you'll instead receive the local network address of the router. Still, ipconfig is useful because of its extensions. ipconfig /release followed by ipconfig /renew can force your Windows PC into asking for a new IP address, which is useful if your computer claims one isn't available. You can also use ipconfig /flushdns to refresh your DNS address. These commands are great if the Windows network troubleshooter chokes, which does happen on occasion.","title":"ipconfig"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#netstat","text":"Entering the command netstat -an will provide you with a list of currently open ports and related IP addresses. This command will also tell you what state the port is in; listening, established, or closed. This is a great command for when you're trying to troubleshoot devices connected to your PC or when you fear a Trojan infected your system and you're trying to locate a malicious connection.","title":"Netstat"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#ping","text":"Sometimes, you need to know whether or not packets are making it to a specific networked device. That's where ping comes in handy. Typing ping followed by an IP address or web domain will send a series of test packets to the specified address. If they arrive and are returned, you know the device is capable of communicating with your PC; if it fails, you know that there's something blocking communication between the device and your computer. This can help you decide if the root of the issue is an improper configuration or a failure of network hardware.","title":"Ping"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#pathping","text":"This is a more advanced version of ping that's useful if there are multiple routers between your PC and the device you're testing. Like ping, you use this command by typing pathping followed by the IP address, but unlike ping, pathping also relays some information about the route the test packets take.","title":"PathPing"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#tracert","text":"The tracert command is similar to pathping. Once again, type tracert followed by the IP address or domain you'd like to trace. You'll receive information about each step in the route between your PC and the target. Unlike pathping, however, tracert also tracks how much time (in milliseconds) each hop between servers or devices takes.","title":"Tracert"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#powercfg","text":"Powercfg is a very powerful command for managing and tracking how your computer uses energy. You can use the command powercfg hibernate on and powercfg hibernate off to manage hibernation, and you can also use the command powercfg /a to view the power-saving states currently available on your PC. Another useful command is powercfg /devicequery s1_supported , which displays a list of devices on your computer that support connected standby. When enabled, you can use these devices to bring your computer out of standby, even remotely. You can enable this by selecting the device in Device Manager , opening its properties, going to the Power Management tab, and then checking the Allow this device to wake the computer box. Powercfg /lastwake will show you what device last woke your PC from a sleep state. You can use this command to troubleshoot your PC if it seems to wake from sleep at random. You can use the powercfg /energy command to build a detailed power consumption report for your PC. The report saves to the directory indicated after the command finishes. This report will let you know of any system faults that might increase power consumption, like devices blocking certain sleep modes, or poorly configured to respond to your power management settings. Windows 8 added powercfg /batteryreport , which provides a detailed analysis of battery use, if applicable. Normally output to your Windows user directory, the report provides details about the time and length of charge and discharge cycles, lifetime average battery life, and estimated battery capacity.","title":"Powercfg"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#shutdown","text":"Windows 8 introduced the shutdown command that, you guessed it, shuts down your computer . This is, of course, redundant with the already easily accessed shutdown button, but what's not redundant is the shutdown /r /o command, which restarts your PC and launches the Advanced Start Options menu, which is where you can access Safe Mode and Windows recovery utilities. This is useful if you want to restart your computer for troubleshooting purposes.","title":"Shutdown"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#systeminfo","text":"This command will give you a detailed configuration overview of your computer. The list covers your operating system and hardware. For example, you can look up the original Windows installation date, the last boot time, your BIOS version, total and available memory, installed hotfixes, network card configurations, and more. Use systeminfo /s followed by the host name of a computer on your local network, to remotely grab the information for that system. This may require additional syntax elements for the domain, user name, and password, like this: systeminfo / s [ host_name ] / u [ domain ] \\ [ user_name ] / p [ user_password ]","title":"Systeminfo"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#system-file-checker","text":"System File Checker is an automatic scan and repair tool that focuses on Windows system files. You will need to run the command prompt with administrator privileges and enter the command sfc /scannow . If SFC finds any corrupt or missing files, it will automatically replace them using cached copies kept by Windows for this purpose alone. The command can require a half-hour to run on older notebooks.","title":"System File Checker"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#tasklist","text":"You can use the tasklist command to provide a current list of all tasks running on your PC. Though somewhat redundant with Task Manager, the command may sometimes find tasks hidden from view in that utility. There's also a wide range of modifiers. Tasklist -svc shows services related to each task, use tasklist -v to obtain more detail on each task, and tasklist -m will locate DLL files associated with active tasks. These commands are useful for advanced troubleshooting. Our reader Eric noted that you can \"get the name of the executable associated with the particular process ID you're interested in.\" The command for that operation is tasklist | find [process id].","title":"Tasklist"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#taskkill","text":"Tasks that appear in the tasklist command will have an executable and process ID (a four- or five-digit number) associated with them. You can force stop a program using taskkill -im followed by the executable's name, or taskkill -pid followed by the process ID. Again, this is a bit redundant with Task Manager, but you can use it to kill otherwise unresponsive or hidden programs.","title":"Taskkill"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#chkdsk","text":"Windows automatically marks your drive for a diagnostic chkdsk scan when symptoms indicate that a local drive has bad sectors, lost clusters, or other logical or physical errors. If you suspect your hard drive is failing, you can manually initiate a scan. The most basic command is chkdsk c: , which will immediately scan the C: drive, without a need to restart the computer. If you add parameters like /f, /r, /x, or /b, such as in chkdsk /f /r /x /b c: , chkdsk will also fix errors, recover data, dismount the drive, or clear the list of bad sectors, respectively. These actions require a reboot, as they can only run with Windows powered down. If you see chkdsk run at startup, let it do its thing. If it gets stuck, however, refer to the chkdsk troubleshooting article .","title":"Chkdsk"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#schtasks","text":"Schtasks is your command prompt access to the Task Scheduler, one of many underrated Windows administrative tools. While you can use the GUI to manage your scheduled tasks, the command prompt lets you copy&paste complex commands to set up multiple similar tasks without having to click through various options. Ultimately, it's much easier to use, once you've committed key parameters to memory. For example, you could schedule your computer to reboot at 11pm every Friday: schtasks /create /sc weekly /d FRI /tn \"auto reboot computer weekly\" /st 23:00 /tr \"shutdown -r -f -t 10\" To complement your weekly reboot, you could schedule tasks to launch specific programs on startup: schtasks /create /sc onstart /tn \"launch Chrome on startup\" /tr \"C:\\Program Files (x86)\\Google\\Chrome\\Application\\Chrome.exe\" To duplicate the above command for different programs, just copy, paste, and modify it as needed.","title":"schtasks"},{"location":"Windows/Windows%20Command%20Line%20Commands%20Overview/#command-and-conquer-your-windows-pc","text":"This article can only give you a taste of what's hidden within the Windows command line. When including all variables, there are literally hundreds of commands. Download Microsoft's command line reference guide (in Edge or Internet Explorer) for advanced support and troubleshooting. Tired of the command prompt? Time to try the new Windows Terminal !","title":"Command and Conquer Your Windows PC"},{"location":"_daily/","text":"_daily \u2691 Categories \u2691 Documents \u2691 2020-04-20 2021-04-21 2021-04-22 2021-04-23 2021-04-24 2021-04-25 2021-04-28 2021-04-29 2021-05-01 2021-05-02","title":"_daily"},{"location":"_daily/#_daily","text":"","title":"_daily"},{"location":"_daily/#categories","text":"","title":"Categories"},{"location":"_daily/#documents","text":"2020-04-20 2021-04-21 2021-04-22 2021-04-23 2021-04-24 2021-04-25 2021-04-28 2021-04-29 2021-05-01 2021-05-02","title":"Documents"},{"location":"_daily/2020-04-20/","text":"<< [[2021-04-19]] | 2021-04-21 >> Tuesday, April 2021 \u2691 An aspirational diet will have you dreaming of success; but it's the attachment of expectations and tangible goals that feeds the desire, persistence, and fortitude required to make the win. \u2014 Lorii Myers \u2714\ufe0f Todo: \u2691 [ ] Powwater: Implement new delivery fee pricing table from Ellie (see emails): Link to GitHub Issue [x] Adminportal: Add new data_prep R script, SQL creation file, and CSV lookup for the pricing table [ ] Add a new field to orders table showing the delivery fee tier associated with that order [ ] Add a new field to orders table for order type fees (i.e. Refill orders are X amount more than New orders) [ ] Create new tab in Adminportal that displays delivery fee tiers in a table: Columns: Tier (i.e. interval in Kilometers) New (Base) Fee Refill Fee (should be a surcharge - use shinyFeedback ) Swap Fee (should be a discount - use shinyFeedback ) Rows: Need to discuss this further with client - do we need alternative tables between vendors? If so should add vendor_uid to table. Add buttons to make table and data editable and allow addition of new tiers/removal of old tiers Display historical order route distance and time information as reference for pricing the tiers (see vendor's app) \ud83d\udcdd Notes: \u2691","title":"2020 04 20"},{"location":"_daily/2020-04-20/#tuesday-april-2021","text":"An aspirational diet will have you dreaming of success; but it's the attachment of expectations and tangible goals that feeds the desire, persistence, and fortitude required to make the win. \u2014 Lorii Myers","title":"Tuesday, April 2021"},{"location":"_daily/2020-04-20/#todo","text":"[ ] Powwater: Implement new delivery fee pricing table from Ellie (see emails): Link to GitHub Issue [x] Adminportal: Add new data_prep R script, SQL creation file, and CSV lookup for the pricing table [ ] Add a new field to orders table showing the delivery fee tier associated with that order [ ] Add a new field to orders table for order type fees (i.e. Refill orders are X amount more than New orders) [ ] Create new tab in Adminportal that displays delivery fee tiers in a table: Columns: Tier (i.e. interval in Kilometers) New (Base) Fee Refill Fee (should be a surcharge - use shinyFeedback ) Swap Fee (should be a discount - use shinyFeedback ) Rows: Need to discuss this further with client - do we need alternative tables between vendors? If so should add vendor_uid to table. Add buttons to make table and data editable and allow addition of new tiers/removal of old tiers Display historical order route distance and time information as reference for pricing the tiers (see vendor's app)","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2020-04-20/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-21/","text":"<< [[2021-04-20]] | 2021-04-22 >> Wednesday, April 2021 \u2691 If you're walking down the right path and you're willing to keep walking, eventually you'll make progress. \u2014 Barack Obama \u2714\ufe0f Todo: \u2691 Leftover from Tuesday, April 2021: \u2691 [ ] Finish delivery fee pricing table's implementation [ ] Add to adminportal for \"default\" powwater delivery fee tiers [ ] Add to vendor dashboard (non-editable for all vendors except Dutch Water) [ ] NOTE: Dutch Water - all deliveries are free of charge [ ] \ud83d\udcdd Notes: \u2691","title":"2021 04 21"},{"location":"_daily/2021-04-21/#wednesday-april-2021","text":"If you're walking down the right path and you're willing to keep walking, eventually you'll make progress. \u2014 Barack Obama","title":"Wednesday, April 2021"},{"location":"_daily/2021-04-21/#todo","text":"","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-21/#leftover-from-tuesday-april-2021","text":"[ ] Finish delivery fee pricing table's implementation [ ] Add to adminportal for \"default\" powwater delivery fee tiers [ ] Add to vendor dashboard (non-editable for all vendors except Dutch Water) [ ] NOTE: Dutch Water - all deliveries are free of charge [ ]","title":"Leftover from Tuesday, April 2021:"},{"location":"_daily/2021-04-21/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-22/","text":"<< 2021-04-21 | 2021-04-23 >> Thursday, April 2021 \u2691 Feeling grateful to or appreciative of someone or something in your life actually attracts more of the things that you appreciate and value into your life. \u2014 Christiane Northrup \u2714\ufe0f Todo: \u2691 [ ] \ud83d\udcdd Notes: \u2691","title":"2021 04 22"},{"location":"_daily/2021-04-22/#thursday-april-2021","text":"Feeling grateful to or appreciative of someone or something in your life actually attracts more of the things that you appreciate and value into your life. \u2014 Christiane Northrup","title":"Thursday, April 2021"},{"location":"_daily/2021-04-22/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-22/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-23/","text":"<< 2021-04-22 | 2021-04-24 >> Friday, April 2021 \u2691 If you're not making mistakes, you're not taking risks, and that means you're not going anywhere. The key is to make mistakes faster than the competition, so you have more changes to learn and win. \u2014 John W. Holt, Jr. \u2714\ufe0f Todo: \u2691 [ ] \ud83d\udcdd Notes: \u2691","title":"2021 04 23"},{"location":"_daily/2021-04-23/#friday-april-2021","text":"If you're not making mistakes, you're not taking risks, and that means you're not going anywhere. The key is to make mistakes faster than the competition, so you have more changes to learn and win. \u2014 John W. Holt, Jr.","title":"Friday, April 2021"},{"location":"_daily/2021-04-23/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-23/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-24/","text":"<< 2021-04-23 | 2021-04-25 >> Saturday, April 2021 \u2691 When a man is an upright contender, only real people are bound to be around; but if he is a downright pretender, then fake people surround and always abound to be found. \u2014 Anuj Somany \u2714\ufe0f Todo: \u2691 [ ] \ud83d\udcdd Notes: \u2691","title":"2021 04 24"},{"location":"_daily/2021-04-24/#saturday-april-2021","text":"When a man is an upright contender, only real people are bound to be around; but if he is a downright pretender, then fake people surround and always abound to be found. \u2014 Anuj Somany","title":"Saturday, April 2021"},{"location":"_daily/2021-04-24/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-24/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-25/","text":"<< 2021-04-24 | [[2021-04-26]] >> Sunday, April 2021 \u2691 Quote of the Day #quotes \u2691 Named must your fear be before banish it you can. \u2014 Yoda [[Saved Quotes]] \u2714\ufe0f Todo: \u2691 [[Powwater API]] : \u2691 Tags: #pow #api Tasks: [ ] Review all code in plumber.R and refactor accordingly - think MVP, speed, no unnecessary enhancements just because they seem cool and worth learning. [ ] Replace all [[R-Packages: DBI| DBI]] DBI::dbConnect connection conn objects with [[R-Packages: pool| Pool]] (i.e. pool::poolCreate , pool::poolCheckout , etc.) [ ] Review Transactions for Database ensuring are working properly though unit tests. [ ] Implement full test suite against API endpoints and authentication filters. [ ] Create a /customers/<customer_uid>/locations endpoint to GET (retrieve), POST (add new), and PUT (update existing) REST Methods. [ ] Limit all payloads sent between requests and responses to the minimum amount of data necessary and collaborate with Patrick on how to optimize these responses with additional restructuring \ud83d\udcdd Notes: \u2691","title":"2021 04 25"},{"location":"_daily/2021-04-25/#sunday-april-2021","text":"","title":"Sunday, April 2021"},{"location":"_daily/2021-04-25/#quote-of-the-day-quotes","text":"Named must your fear be before banish it you can. \u2014 Yoda [[Saved Quotes]]","title":"Quote of the Day #quotes"},{"location":"_daily/2021-04-25/#todo","text":"","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-25/#powwater-api","text":"Tags: #pow #api Tasks: [ ] Review all code in plumber.R and refactor accordingly - think MVP, speed, no unnecessary enhancements just because they seem cool and worth learning. [ ] Replace all [[R-Packages: DBI| DBI]] DBI::dbConnect connection conn objects with [[R-Packages: pool| Pool]] (i.e. pool::poolCreate , pool::poolCheckout , etc.) [ ] Review Transactions for Database ensuring are working properly though unit tests. [ ] Implement full test suite against API endpoints and authentication filters. [ ] Create a /customers/<customer_uid>/locations endpoint to GET (retrieve), POST (add new), and PUT (update existing) REST Methods. [ ] Limit all payloads sent between requests and responses to the minimum amount of data necessary and collaborate with Patrick on how to optimize these responses with additional restructuring","title":"[[Powwater API]] :"},{"location":"_daily/2021-04-25/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-28/","text":"<< [[2021-04-27]] | 2021-04-29 >> Wednesday, April 2021 \u2691 The determination to win is the better part of winning. \u2014 Daisaku Ikeda \u2714\ufe0f Todo: \u2691 [ ] \ud83d\udcdd Notes: \u2691","title":"2021 04 28"},{"location":"_daily/2021-04-28/#wednesday-april-2021","text":"The determination to win is the better part of winning. \u2014 Daisaku Ikeda","title":"Wednesday, April 2021"},{"location":"_daily/2021-04-28/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-28/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-04-29/","text":"<< 2021-04-28 | [[2021-04-30]] >> Thursday, April 2021 \u2691 The best way to not feel hopeless is to get up and do something. Don't wait for good things to happen to you. If you go out and make some good things happen, you will fill the world with hope, you will fill yourself with hope. \u2014 Barack Obama \u2714\ufe0f Todo: \u2691 [ ] Review powpolished authentication/registration/login process and database schema with Pat: Questions: \u2691 How does accounts table and API use the polished_key and hashed_polished_key \ud83d\udcdd Notes: \u2691","title":"2021 04 29"},{"location":"_daily/2021-04-29/#thursday-april-2021","text":"The best way to not feel hopeless is to get up and do something. Don't wait for good things to happen to you. If you go out and make some good things happen, you will fill the world with hope, you will fill yourself with hope. \u2014 Barack Obama","title":"Thursday, April 2021"},{"location":"_daily/2021-04-29/#todo","text":"[ ] Review powpolished authentication/registration/login process and database schema with Pat:","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-04-29/#questions","text":"How does accounts table and API use the polished_key and hashed_polished_key","title":"Questions:"},{"location":"_daily/2021-04-29/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-05-01/","text":"<< [[2021-04-30]] | 2021-05-02 >> Saturday, May 2021 \u2691 At that point where you have decided to upgrade from aspiration to expectation and have begun to visualize an outcome, something incredibly important has happened, you have committed to the process of change. \u2014 Lorii Myers \u2714\ufe0f Todo: \u2691 [ ] \ud83d\udcdd Notes: \u2691","title":"2021 05 01"},{"location":"_daily/2021-05-01/#saturday-may-2021","text":"At that point where you have decided to upgrade from aspiration to expectation and have begun to visualize an outcome, something incredibly important has happened, you have committed to the process of change. \u2014 Lorii Myers","title":"Saturday, May 2021"},{"location":"_daily/2021-05-01/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-05-01/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-05-02/","text":"<< 2021-05-01 | 2021-05-03 >> Sunday, May 2021 \u2691 Whether you think you can or think you can't, you're right.. \u2014 Henry Ford \u2714\ufe0f Todo: \u2691 [ ] \ud83d\udcdd Notes: \u2691","title":"2021 05 02"},{"location":"_daily/2021-05-02/#sunday-may-2021","text":"Whether you think you can or think you can't, you're right.. \u2014 Henry Ford","title":"Sunday, May 2021"},{"location":"_daily/2021-05-02/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-05-02/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-05-03/","text":"<< 2021-05-02 | 2021-05-04 >> Monday, May 2021 \u2691 I wanted to achieve something essential in life, something that is not measured by money or position in society. The mountains are not stadiums where I satisfy my ambitions to achieve. They are my cathedrals, the houses of my religion. In the mountains I attempt to understand my life. They are the way I practice my religion. In the mountains I celebrate creation, on each journey I am reborn. \u2014 Anatoli Boukreev \u2714\ufe0f Todo: \u2691 [ ] FD&BR Fantasy Review: [ ] \ud83d\udcdd Notes: \u2691 FD&BR \u2691 Travis D'Arneau hurt for Braves -> Options include: William Contreras - Advice: Contreras kicked off the scoring with a two-out RBI single to left field off Ross Stripling in the second inning. With veteran backstop Travis d'Arnaud sidelined for the next two months due to a sprained left thumb, the 23-year-old catching prospect will get an extended opportunity, alongside fellow youngster Alex Jackson, to show he belongs at the major-league level. He's worthy of a speculative roster spot in deeper mixed leagues and two-catcher formats. ( https://NBCSportsEDGE.com ) Austin Jackson on IL","title":"2021 05 03"},{"location":"_daily/2021-05-03/#monday-may-2021","text":"I wanted to achieve something essential in life, something that is not measured by money or position in society. The mountains are not stadiums where I satisfy my ambitions to achieve. They are my cathedrals, the houses of my religion. In the mountains I attempt to understand my life. They are the way I practice my religion. In the mountains I celebrate creation, on each journey I am reborn. \u2014 Anatoli Boukreev","title":"Monday, May 2021"},{"location":"_daily/2021-05-03/#todo","text":"[ ] FD&BR Fantasy Review: [ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-05-03/#notes","text":"","title":"\ud83d\udcdd Notes:"},{"location":"_daily/2021-05-03/#fdbr","text":"Travis D'Arneau hurt for Braves -> Options include: William Contreras - Advice: Contreras kicked off the scoring with a two-out RBI single to left field off Ross Stripling in the second inning. With veteran backstop Travis d'Arnaud sidelined for the next two months due to a sprained left thumb, the 23-year-old catching prospect will get an extended opportunity, alongside fellow youngster Alex Jackson, to show he belongs at the major-league level. He's worthy of a speculative roster spot in deeper mixed leagues and two-catcher formats. ( https://NBCSportsEDGE.com ) Austin Jackson on IL","title":"FD&amp;BR"},{"location":"_daily/2021-05-04/","text":"<< 2021-05-03 | [[2021-05-05]] >> Tuesday, May 2021 \u2691 The last time doesn't exist. It's only this time. And everything is going to be different this time. There's only now. \u2014 Bill Murray \u2714\ufe0f Todo: \u2691 [ ] \ud83d\udcdd Notes: \u2691","title":"2021 05 04"},{"location":"_daily/2021-05-04/#tuesday-may-2021","text":"The last time doesn't exist. It's only this time. And everything is going to be different this time. There's only now. \u2014 Bill Murray","title":"Tuesday, May 2021"},{"location":"_daily/2021-05-04/#todo","text":"[ ]","title":"\u2714\ufe0f Todo:"},{"location":"_daily/2021-05-04/#notes","text":"","title":"\ud83d\udcdd Notes:"}]}